"""
DuckDBë¥¼ ì‚¬ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

DuckDBëŠ” OLAP(ë¶„ì„) ì›Œí¬ë¡œë“œì— ìµœì í™”ëœ ì„ë² ë””ë“œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ,
ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¿¼ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- CSVë¥¼ DuckDB/Parquetìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ ê³µê°„ ì ˆì•½ ë° ë¡œë”© ì†ë„ í–¥ìƒ
- SQL ì¿¼ë¦¬ë¥¼ í†µí•œ ìœ ì—°í•œ ë°ì´í„° í•„í„°ë§
- Pandasì™€ì˜ ì›í™œí•œ í†µí•©
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- APIë¥¼ í†µí•œ ìµœì‹  ë°ì´í„° ìë™ ì—…ë°ì´íŠ¸
"""

import duckdb
import pandas as pd
from pathlib import Path
from typing import Optional, List
import time
import os
import json
import requests
from dotenv import load_dotenv
import warnings

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# SSL ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class TimeSeriesDB:
    """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ DuckDBë¡œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, db_path: str = "influenza_data.duckdb"):
        """
        Parameters:
        -----------
        db_path : str
            DuckDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        """
        self.db_path = Path(db_path)
        self.conn = None
        
    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        if self.conn is None:
            self.conn = duckdb.connect(str(self.db_path))
            print(f"âœ… DuckDB ì—°ê²°ë¨: {self.db_path}")
        return self.conn
    
    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            self.conn = None
            print("âœ… DuckDB ì—°ê²° ì¢…ë£Œë¨")
    
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.connect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.close()
    
    def import_csv_to_db(
        self, 
        csv_path: str, 
        table_name: str = "influenza_data",
        chunk_size: int = 100000,
        show_progress: bool = True
    ):
        """
        ëŒ€ìš©ëŸ‰ CSVë¥¼ DuckDB í…Œì´ë¸”ë¡œ ì„í¬íŠ¸
        
        Parameters:
        -----------
        csv_path : str
            CSV íŒŒì¼ ê²½ë¡œ
        table_name : str
            ìƒì„±í•  í…Œì´ë¸” ì´ë¦„
        chunk_size : int
            í•œ ë²ˆì— ì½ì„ í–‰ ìˆ˜ (ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)
        show_progress : bool
            ì§„í–‰ ìƒí™© í‘œì‹œ ì—¬ë¶€
        """
        self.connect()
        csv_path = Path(csv_path)
        
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¥ CSV â†’ DuckDB ì„í¬íŠ¸ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ì›ë³¸ íŒŒì¼: {csv_path.name}")
        print(f"í…Œì´ë¸”ëª…: {table_name}")
        
        start_time = time.time()
        
        # DuckDBëŠ” CSVë¥¼ ì§ì ‘ ì½ì–´ì„œ í…Œì´ë¸”ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë§¤ìš° ë¹ ë¦„)
        self.conn.execute(f"""
            CREATE OR REPLACE TABLE {table_name} AS 
            SELECT * FROM read_csv_auto('{csv_path}', 
                header=true,
                sample_size=100000
            )
        """)
        
        # í…Œì´ë¸” ì •ë³´ í™•ì¸
        row_count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        
        elapsed = time.time() - start_time
        
        print(f"\nâœ… ì„í¬íŠ¸ ì™„ë£Œ!")
        print(f"   â€¢ ì´ í–‰ ìˆ˜: {row_count:,}")
        print(f"   â€¢ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"   â€¢ ì´ˆë‹¹ ì²˜ë¦¬: {row_count/elapsed:,.0f} í–‰/ì´ˆ")
        print(f"{'='*60}\n")
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ í¬ê¸° í™•ì¸
        if self.db_path.exists():
            db_size_mb = self.db_path.stat().st_size / (1024 * 1024)
            csv_size_mb = csv_path.stat().st_size / (1024 * 1024)
            compression_ratio = (1 - db_size_mb / csv_size_mb) * 100
            
            print(f"ğŸ’¾ ì €ì¥ ê³µê°„:")
            print(f"   â€¢ ì›ë³¸ CSV: {csv_size_mb:.1f} MB")
            print(f"   â€¢ DuckDB: {db_size_mb:.1f} MB")
            print(f"   â€¢ ì••ì¶•ë¥ : {compression_ratio:.1f}% ì ˆì•½\n")
    
    def export_to_parquet(
        self, 
        table_name: str = "influenza_data",
        parquet_path: str = "influenza_data.parquet"
    ):
        """
        DuckDB í…Œì´ë¸”ì„ Parquet íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
        Parquetì€ ì»¬ëŸ¼ ê¸°ë°˜ í¬ë§·ìœ¼ë¡œ ë¶„ì„ ì¿¼ë¦¬ì— ë§¤ìš° íš¨ìœ¨ì 
        
        Parameters:
        -----------
        table_name : str
            ë‚´ë³´ë‚¼ í…Œì´ë¸” ì´ë¦„
        parquet_path : str
            ì €ì¥í•  Parquet íŒŒì¼ ê²½ë¡œ
        """
        self.connect()
        
        print(f"\nğŸ“¤ Parquet ë‚´ë³´ë‚´ê¸°: {table_name} â†’ {parquet_path}")
        start_time = time.time()
        
        self.conn.execute(f"""
            COPY {table_name} TO '{parquet_path}' 
            (FORMAT PARQUET, COMPRESSION 'ZSTD')
        """)
        
        elapsed = time.time() - start_time
        parquet_size_mb = Path(parquet_path).stat().st_size / (1024 * 1024)
        
        print(f"âœ… ì™„ë£Œ! ({elapsed:.2f}ì´ˆ)")
        print(f"   â€¢ íŒŒì¼ í¬ê¸°: {parquet_size_mb:.1f} MB\n")
    
    def load_data(
        self,
        table_name: str = "influenza_data",
        columns: Optional[List[str]] = None,
        where: Optional[str] = None,
        limit: Optional[int] = None,
        order_by: Optional[str] = None
    ) -> pd.DataFrame:
        """
        DuckDB í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ Pandas DataFrameìœ¼ë¡œ ë¡œë“œ
        
        Parameters:
        -----------
        table_name : str
            ë¡œë“œí•  í…Œì´ë¸” ì´ë¦„
        columns : List[str], optional
            ë¡œë“œí•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì»¬ëŸ¼)
        where : str, optional
            WHERE ì ˆ ì¡°ê±´ (ì˜ˆ: "season_norm >= 2020")
        limit : int, optional
            ë¡œë“œí•  ìµœëŒ€ í–‰ ìˆ˜
        order_by : str, optional
            ì •ë ¬ ê¸°ì¤€ (ì˜ˆ: "date DESC")
        
        Returns:
        --------
        pd.DataFrame
            ë¡œë“œëœ ë°ì´í„°
        """
        self.connect()
        
        # SQL ì¿¼ë¦¬ êµ¬ì„± (ì»¬ëŸ¼ëª…ì„ ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
        if columns is None:
            select_cols = "*"
        else:
            # ì»¬ëŸ¼ëª…ì„ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
            select_cols = ", ".join([f'"{col}"' for col in columns])
        
        query = f"SELECT {select_cols} FROM {table_name}"
        
        if where:
            query += f" WHERE {where}"
        
        if order_by:
            query += f" ORDER BY {order_by}"
        
        if limit:
            query += f" LIMIT {limit}"
        
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        print(f"ì¿¼ë¦¬: {query[:100]}{'...' if len(query) > 100 else ''}")
        
        start_time = time.time()
        df = self.conn.execute(query).df()
        elapsed = time.time() - start_time
        
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} í–‰ Ã— {df.shape[1]} ì—´ ({elapsed:.2f}ì´ˆ)\n")
        
        return df
    
    def get_table_info(self, table_name: str = "influenza_data"):
        """
        í…Œì´ë¸” ì •ë³´ ì¶œë ¥ (ì»¬ëŸ¼ëª…, ë°ì´í„° íƒ€ì…, ìƒ˜í”Œ ë°ì´í„°)
        
        Parameters:
        -----------
        table_name : str
            í™•ì¸í•  í…Œì´ë¸” ì´ë¦„
        """
        self.connect()
        
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ í…Œì´ë¸” ì •ë³´: {table_name}")
        print(f"{'='*60}")
        
        # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
        schema = self.conn.execute(f"DESCRIBE {table_name}").df()
        print("\nì»¬ëŸ¼ ì •ë³´:")
        print(schema.to_string(index=False))
        
        # í–‰ ìˆ˜
        row_count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        print(f"\nì´ í–‰ ìˆ˜: {row_count:,}")
        
        # ìƒ˜í”Œ ë°ì´í„°
        print("\nìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5í–‰):")
        sample = self.conn.execute(f"SELECT * FROM {table_name} LIMIT 5").df()
        print(sample.to_string())
        
        # ë‚ ì§œ ë²”ìœ„ (label ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •)
        try:
            date_range = self.conn.execute(f"""
                SELECT 
                    MIN(label) as start_period,
                    MAX(label) as end_period
                FROM {table_name}
            """).df()
            print("\nê¸°ê°„:")
            print(date_range.to_string(index=False))
        except:
            pass
        
        print(f"{'='*60}\n")
    
    def create_indices(self, table_name: str = "influenza_data"):
        """
        ìì£¼ ì¿¼ë¦¬í•˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„± (ì¿¼ë¦¬ ì†ë„ í–¥ìƒ)
        
        Parameters:
        -----------
        table_name : str
            ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  í…Œì´ë¸” ì´ë¦„
        """
        self.connect()
        
        print(f"\nğŸ” ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # DuckDBëŠ” ìë™ìœ¼ë¡œ ì¿¼ë¦¬ ìµœì í™”ë¥¼ í•˜ì§€ë§Œ,
        # ëª…ì‹œì ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤
        # ì°¸ê³ : DuckDBëŠ” ì¸ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰
        
        print(f"âœ… DuckDBëŠ” ìë™ ìµœì í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n")
    
    def optimize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (VACUUM, ANALYZE)"""
        self.connect()
        
        print(f"\nğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì¤‘...")
        
        # ANALYZE: í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸ë¡œ ì¿¼ë¦¬ ìµœì í™”
        self.conn.execute("ANALYZE")
        
        print(f"âœ… ìµœì í™” ì™„ë£Œ\n")


def convert_csv_to_duckdb(
    csv_path: str,
    db_path: str = "influenza_data.duckdb",
    table_name: str = "influenza_data"
):
    """
    í¸ì˜ í•¨ìˆ˜: CSVë¥¼ DuckDBë¡œ ë³€í™˜
    
    Parameters:
    -----------
    csv_path : str
        ë³€í™˜í•  CSV íŒŒì¼ ê²½ë¡œ
    db_path : str
        ìƒì„±í•  DuckDB íŒŒì¼ ê²½ë¡œ
    table_name : str
        í…Œì´ë¸” ì´ë¦„
    """
    with TimeSeriesDB(db_path) as db:
        db.import_csv_to_db(csv_path, table_name)
        db.get_table_info(table_name)
        db.optimize_database()
    
    print(f"âœ… ë³€í™˜ ì™„ë£Œ: {db_path}")
    return db_path


def load_from_duckdb(
    db_path: str = "influenza_data.duckdb",
    table_name: str = "influenza_data",
    **kwargs
) -> pd.DataFrame:
    """
    í¸ì˜ í•¨ìˆ˜: DuckDBì—ì„œ ë°ì´í„° ë¡œë“œ
    
    Parameters:
    -----------
    db_path : str
        DuckDB íŒŒì¼ ê²½ë¡œ
    table_name : str
        ë¡œë“œí•  í…Œì´ë¸” ì´ë¦„
    **kwargs
        load_data() í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì
    
    Returns:
    --------
    pd.DataFrame
        ë¡œë“œëœ ë°ì´í„°
    """
    with TimeSeriesDB(db_path) as db:
        return db.load_data(table_name, **kwargs)


def fetch_latest_data_from_api(api_url: str = None, dataset_ids: List[str] = None) -> pd.DataFrame:
    """
    APIë¥¼ í†µí•´ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    
    Parameters:
    -----------
    api_url : str, optional
        API ì„œë²„ URL (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ API_URL)
    dataset_ids : List[str], optional
        ê°€ì ¸ì˜¬ ë°ì´í„°ì…‹ ID ë¦¬ìŠ¤íŠ¸
    
    Returns:
    --------
    pd.DataFrame
        APIì—ì„œ ê°€ì ¸ì˜¨ ìµœì‹  ë°ì´í„°
    """
    if api_url is None:
        api_url = os.getenv('API_URL', 'http://localhost:3000')
    
    if dataset_ids is None:
        # ê¸°ë³¸ ë°ì´í„°ì…‹ ID ë¦¬ìŠ¤íŠ¸
        dataset_ids = [
            'ds_0101', 'ds_0102', 'ds_0103', 'ds_0104', 'ds_0105', 
            'ds_0106', 'ds_0107', 'ds_0108', 'ds_0109', 'ds_0110',
            'ds_0701', 'ds_0801', 'ds_0901'
        ]
    
    print(f"\n{'='*60}")
    print(f"ğŸŒ APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
    print(f"{'='*60}")
    print(f"API URL: {api_url}")
    print(f"ë°ì´í„°ì…‹ ê°œìˆ˜: {len(dataset_ids)}")
    
    all_dataframes = []
    
    for idx, dsid in enumerate(dataset_ids, 1):
        print(f"\n[{idx}/{len(dataset_ids)}] {dsid} ë¡œë”© ì¤‘...")
        
        try:
            request_url = f"{api_url}/download"
            request_body = {"dsid": dsid, "returnData": True}  # ë°ì´í„°ë¥¼ ì§ì ‘ ë°˜í™˜ ìš”ì²­
            
            response = requests.post(
                request_url,
                json=request_body,
                timeout=300
            )
            
            if response.status_code != 200:
                print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                continue
            
            result = response.json()
            if not result.get('ok'):
                print(f"  âš ï¸ API ì—ëŸ¬: {result.get('error')}")
                continue
            
            # API ì‘ë‹µì—ì„œ ì§ì ‘ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ ì €ì¥í•˜ì§€ ì•ŠìŒ)
            api_data = result.get('result', {}).get('data', [])
            
            if api_data:
                df = pd.DataFrame(api_data)
                df['dataset_id'] = dsid
                all_dataframes.append(df)
                print(f"  âœ… ì™„ë£Œ: {len(api_data)} ë ˆì½”ë“œ (ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì²˜ë¦¬)")
        
        except Exception as e:
            print(f"  âš ï¸ ì˜¤ë¥˜: {e}")
            continue
        
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(0.5)
    
    if not all_dataframes:
        print(f"\nâš ï¸ ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return pd.DataFrame()
    
    # ëª¨ë“  ë°ì´í„° ë³‘í•©
    df_latest = pd.concat(all_dataframes, ignore_index=True)
    print(f"\nâœ… ìµœì‹  ë°ì´í„° ë³‘í•© ì™„ë£Œ: {df_latest.shape}")
    
    return df_latest


def load_historical_data(before_dir: str = 'data/before') -> pd.DataFrame:
    """
    ê³¼ê±° ë°ì´í„° CSV íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ë³‘í•©
    
    Parameters:
    -----------
    before_dir : str
        ê³¼ê±° ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
    
    Returns:
    --------
    pd.DataFrame
        ë³‘í•©ëœ ê³¼ê±° ë°ì´í„°
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ ê³¼ê±° ë°ì´í„° ë¡œë”©")
    print(f"{'='*60}")
    print(f"ë””ë ‰í† ë¦¬: {before_dir}")
    
    before_path = Path(before_dir)
    if not before_path.exists():
        print(f"âš ï¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {before_dir}")
        return pd.DataFrame()
    
    csv_files = list(before_path.glob("*.csv"))
    print(f"ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
    
    if not csv_files:
        return pd.DataFrame()
    
    # ë°ì´í„°ì…‹ IDë³„ë¡œ ë¶„ë¥˜
    data_by_dsid = {}
    
    for filepath in csv_files:
        filename = filepath.name
        # íŒŒì¼ëª… íŒŒì‹±: flu-0105-2022.csv -> dsid=0105, year=2022
        parts = filename.replace('.csv', '').split('-')
        if len(parts) != 3:
            continue
        
        dsid = f"ds_{parts[1]}"
        
        try:
            df = pd.read_csv(filepath)
            if dsid not in data_by_dsid:
                data_by_dsid[dsid] = []
            data_by_dsid[dsid].append(df)
        except Exception as e:
            print(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({filename}): {e}")
    
    # ë°ì´í„°ì…‹ë³„ë¡œ ë³‘í•©
    all_dataframes = []
    for dsid, df_list in data_by_dsid.items():
        df_combined = pd.concat(df_list, ignore_index=True)
        df_combined['dataset_id'] = dsid
        all_dataframes.append(df_combined)
        print(f"  {dsid}: {len(df_list)}ê°œ íŒŒì¼, {len(df_combined)} ë ˆì½”ë“œ")
    
    if not all_dataframes:
        return pd.DataFrame()
    
    df_historical = pd.concat(all_dataframes, ignore_index=True)
    print(f"\nâœ… ê³¼ê±° ë°ì´í„° ë³‘í•© ì™„ë£Œ: {df_historical.shape}")
    
    return df_historical


def merge_and_update_database(
    db_path: str = "influenza_data.duckdb",
    table_name: str = "influenza_data",
    fetch_latest: bool = True,
    api_url: str = None,
    before_dir: str = 'data/before'
):
    """
    1. APIë¡œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    2. ê³¼ê±° ë°ì´í„° ë¡œë”©
    3. ë°ì´í„° ë³‘í•©
    4. DuckDBì— ì €ì¥
    
    Parameters:
    -----------
    db_path : str
        DuckDB ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
    table_name : str
        í…Œì´ë¸” ì´ë¦„
    fetch_latest : bool
        APIì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ì§€ ì—¬ë¶€
    api_url : str, optional
        API ì„œë²„ URL
    before_dir : str
        ê³¼ê±° ë°ì´í„° ë””ë ‰í† ë¦¬
    """
    print("\n" + "="*60)
    print("ğŸ”„ ë°ì´í„° ë³‘í•© ë° DuckDB ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤")
    print("="*60)
    
    all_data = []
    
    # 1. APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    if fetch_latest:
        print("\n[ë‹¨ê³„ 1/3] APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
        df_latest = fetch_latest_data_from_api(api_url=api_url)
        if not df_latest.empty:
            all_data.append(df_latest)
            print(f"âœ… ìµœì‹  ë°ì´í„°: {df_latest.shape}")
        else:
            print("âš ï¸ ìµœì‹  ë°ì´í„° ì—†ìŒ")
    else:
        print("\n[ë‹¨ê³„ 1/3] ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ê±´ë„ˆëœ€")
    
    # 2. ê³¼ê±° ë°ì´í„° ë¡œë”©
    print("\n[ë‹¨ê³„ 2/2] ê³¼ê±° ë°ì´í„° ë¡œë”©")
    df_historical = load_historical_data(before_dir=before_dir)
    if not df_historical.empty:
        all_data.append(df_historical)
        print(f"âœ… ê³¼ê±° ë°ì´í„°: {df_historical.shape}")
    else:
        print("âš ï¸ ê³¼ê±° ë°ì´í„° ì—†ìŒ")
    
    # 3. ëª¨ë“  ë°ì´í„° ë³‘í•©
    print("\n[ë‹¨ê³„ 3/3] ë°ì´í„° ë³‘í•© ë° DuckDB ì €ì¥")
    if not all_data:
        print("âš ï¸ ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    df_merged = pd.concat(all_data, ignore_index=True)
    
    # ì¤‘ë³µ ì œê±° (year, week, dataset_id ê¸°ì¤€)
    if all(['year' in df_merged.columns, 'week' in df_merged.columns]):
        print(f"ì¤‘ë³µ ì œê±° ì „: {len(df_merged)} í–‰")
        df_merged = df_merged.drop_duplicates(
            subset=['year', 'week'] if 'dataset_id' not in df_merged.columns else ['year', 'week', 'dataset_id'],
            keep='last'
        )
        print(f"ì¤‘ë³µ ì œê±° í›„: {len(df_merged)} í–‰")
    
    print(f"\nìµœì¢… ë³‘í•© ë°ì´í„°: {df_merged.shape}")
    
    # DuckDBì— ì €ì¥
    with TimeSeriesDB(db_path) as db:
        print(f"\nDuckDBì— ì €ì¥ ì¤‘...")
        start_time = time.time()
        
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
        db.conn.execute(f"DROP TABLE IF EXISTS {table_name}")
        db.conn.execute(f"CREATE TABLE {table_name} AS SELECT * FROM df_merged")
        
        elapsed = time.time() - start_time
        row_count = db.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        
        print(f"âœ… DuckDB ì €ì¥ ì™„ë£Œ!")
        print(f"   â€¢ í…Œì´ë¸”: {table_name}")
        print(f"   â€¢ í–‰ ìˆ˜: {row_count:,}")
        print(f"   â€¢ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        
        db.optimize_database()
    
    # CSVë¡œë„ ì €ì¥ (ë°±ì—…)
    csv_output_dir = Path("data/merged")
    csv_output_dir.mkdir(parents=True, exist_ok=True)
    csv_output = csv_output_dir / "merged_influenza_data.csv"
    print(f"\nCSV ë°±ì—… ì €ì¥ ì¤‘: {csv_output}")
    df_merged.to_csv(csv_output, index=False)
    csv_size_mb = csv_output.stat().st_size / (1024 * 1024)
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_size_mb:.1f} MB")
    
    print("\n" + "="*60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    import sys
    
    # ì‚¬ìš© ì˜ˆì œ
    print("=" * 60)
    print("DuckDB ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°")
    print("=" * 60)
    
    # ëª…ë ¹í–‰ ì¸ì í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--update":
        # ì—…ë°ì´íŠ¸ ëª¨ë“œ: API + ê³¼ê±° ë°ì´í„° + ê¸°ì¡´ ë°ì´í„° ë³‘í•©
        print("\nğŸ”„ ì—…ë°ì´íŠ¸ ëª¨ë“œ: APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì™€ì„œ ë³‘í•©")
        merge_and_update_database(
            db_path="influenza_data.duckdb",
            table_name="influenza_data",
            fetch_latest=True,
            before_dir='data/before'
        )
    else:
        # ê¸°ë³¸ ëª¨ë“œ: ê¸°ì¡´ CSVë¥¼ DuckDBë¡œ ë³€í™˜
        csv_file = "data/merged/merged_influenza_data.csv"
        if Path(csv_file).exists():
            db_path = convert_csv_to_duckdb(
                csv_path=csv_file,
                db_path="influenza_data.duckdb",
                table_name="influenza_data"
            )
            
            print("\n" + "=" * 60)
            print("ì‚¬ìš© ì˜ˆì œ:")
            print("=" * 60)
            print("""
# ì „ì²´ ë°ì´í„° ë¡œë“œ
df = load_from_duckdb()

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¡œë“œ
df = load_from_duckdb(columns=['year', 'week', 'ì˜ì‚¬í™˜ì ë¶„ìœ¨'])

# ì¡°ê±´ë¶€ ë¡œë“œ
df = load_from_duckdb(where="year >= 2020")

# ìµœê·¼ 1000ê°œ ë°ì´í„°ë§Œ
df = load_from_duckdb(limit=1000, order_by="year DESC, week DESC")

# API + ê³¼ê±° ë°ì´í„° + ê¸°ì¡´ ë°ì´í„° ë³‘í•©í•˜ì—¬ ì—…ë°ì´íŠ¸
python db_utils.py --update
            """)
        else:
            print(f"\nâš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
            print("\nğŸ’¡ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ DuckDBë¥¼ ìƒì„±í•˜ë ¤ë©´:")
            print("   python db_utils.py --update")
