
"""
PostgreSQLì„ ì‚¬ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

PostgreSQLì€ ê°•ë ¥í•œ íŠ¸ëœì­ì…˜, í™•ì¥ì„±, SQL í‘œì¤€ ì§€ì›ì„ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ RDBMSì…ë‹ˆë‹¤.
ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , ì™¸ë¶€ ì—°ê²° ë° ë¶„ì„ì— ì í•©í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- CSVë¥¼ PostgreSQL í…Œì´ë¸”ë¡œ ì„í¬íŠ¸
- SQL ì¿¼ë¦¬ë¥¼ í†µí•œ ìœ ì—°í•œ ë°ì´í„° í•„í„°ë§
- Pandasì™€ì˜ í†µí•©
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- APIë¥¼ í†µí•œ ìµœì‹  ë°ì´í„° ìë™ ì—…ë°ì´íŠ¸
"""

import psycopg2
import psycopg2.extras
import pandas as pd
from pathlib import Path
from typing import Optional, List
import time
import os
import json
import requests
from dotenv import load_dotenv
import warnings

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# SSL ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class TimeSeriesDB:
    """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ PostgreSQLë¡œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, host=None, dbname=None, user=None, password=None, port=5432):
        self.host = host or os.getenv('PG_HOST', 'localhost')
        self.dbname = dbname or os.getenv('PG_DB', 'influenza')
        self.user = user or os.getenv('PG_USER', 'postgres')
        self.password = password or os.getenv('PG_PASSWORD', 'postgres')
        self.port = int(port or os.getenv('PG_PORT', 5432))
        self.conn = None

    def insert_dataframe(self, df: pd.DataFrame, table_name: str = "influenza_data", if_exists: str = "append"):
        """
        DataFrame ë°ì´í„°ë¥¼ PostgreSQL í…Œì´ë¸”ì— ì ì¬
        (ì»¬ëŸ¼ëª… ë§¤í•‘ ë° ê²°ì¸¡ì¹˜ None ì²˜ë¦¬)
        """
        self.connect()
        # í•œê¸€â†’ì˜ë¬¸ ë§¤í•‘ (í…Œì´ë¸” ìƒì„±ê³¼ ë™ì¼)
        col_map = {
            'ì—°ë„': 'year',
            'ì£¼ì°¨': 'week',
            'ì—°ë ¹ëŒ€': 'age_group',
            'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
            'ì…ì›í™˜ì ìˆ˜': 'hospitalization',
            'ì•„í˜•': 'subtype',
            'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
            'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate',
            'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients',
        }
        df = df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})
        # ê²°ì¸¡ì¹˜ Noneìœ¼ë¡œ ë³€í™˜
        df = df.where(pd.notnull(df), None)
        columns = list(df.columns)
        values = df.values.tolist()
        placeholders = ','.join(['%s'] * len(columns))
        col_names = ','.join([f'"{col}"' for col in columns])
        sql = f'INSERT INTO {table_name} ({col_names}) VALUES ({placeholders})'
        with self.conn.cursor() as cur:
            psycopg2.extras.execute_batch(cur, sql, values)
        self.conn.commit()
        print(f"âœ… ë°ì´í„° {len(df)}ê±´ ì ì¬ ì™„ë£Œ: {table_name}")

    def create_table_from_dataframe(self, df: pd.DataFrame, table_name: str = "influenza_data", if_exists: str = "fail"):
        """
        DataFrameì˜ ì»¬ëŸ¼ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PostgreSQL í…Œì´ë¸” ìƒì„±
        (í•œê¸€ ì»¬ëŸ¼ëª…ì€ ì˜ë¬¸ìœ¼ë¡œ ë§¤í•‘ í•„ìš”)
        """
        self.connect()
        # í•œê¸€â†’ì˜ë¬¸ ë§¤í•‘ (ê¸°ì¡´ col_map í™œìš©)
        col_map = {
            'ì—°ë„': 'year',
            'ì£¼ì°¨': 'week',
            'ì—°ë ¹ëŒ€': 'age_group',
            'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
            'ì…ì›í™˜ì ìˆ˜': 'hospitalization',
            'ì•„í˜•': 'subtype',
            'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
            'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate',
            'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients',
        }
        columns = []
        for col in df.columns:
            col_eng = col_map.get(col, col)
            # ê°„ë‹¨í•œ íƒ€ì… ì¶”ë¡  (float, int, str)
            sample = df[col].dropna()
            if not sample.empty:
                v = sample.iloc[0]
                if isinstance(v, float):
                    col_type = 'DOUBLE PRECISION'
                elif isinstance(v, int):
                    col_type = 'INTEGER'
                else:
                    col_type = 'TEXT'
            else:
                col_type = 'TEXT'
            columns.append(f'"{col_eng}" {col_type}')
        col_defs = ', '.join(columns)
        sql = f'CREATE TABLE {table_name} ({col_defs})'
        if if_exists == "replace":
            with self.conn.cursor() as cur:
                cur.execute(f"DROP TABLE IF EXISTS {table_name}")
            self.conn.commit()
        elif if_exists == "fail":
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìƒì„±í•˜ì§€ ì•ŠìŒ
            with self.conn.cursor() as cur:
                cur.execute(f"SELECT to_regclass('{table_name}')")
                exists = cur.fetchone()[0]
            if exists:
                print(f"âš ï¸ ì´ë¯¸ í…Œì´ë¸”ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {table_name}")
                return
        with self.conn.cursor() as cur:
            cur.execute(sql)
        self.conn.commit()
        print(f"âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_name}")
    """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ PostgreSQLë¡œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, host=None, dbname=None, user=None, password=None, port=5432):
        self.host = host or os.getenv('PG_HOST', 'localhost')
        self.dbname = dbname or os.getenv('PG_DB', 'influenza')
        self.user = user or os.getenv('PG_USER', 'postgres')
        self.password = password or os.getenv('PG_PASSWORD', 'postgres')
        self.port = int(port or os.getenv('PG_PORT', 5432))
        self.conn = None

    def connect(self):
        if self.conn is None:
            self.conn = psycopg2.connect(
                host=self.host,
                dbname=self.dbname,
                user=self.user,
                password=self.password,
                port=self.port
            )
            print(f"âœ… PostgreSQL ì—°ê²°ë¨: {self.dbname}@{self.host}:{self.port}")
        return self.conn

    def close(self):
        if self.conn:
            self.conn.close()
            self.conn = None
            print("âœ… PostgreSQL ì—°ê²° ì¢…ë£Œë¨")
    
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.connect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.close()
    
    def import_csv_to_db(
        self,
        csv_path: str,
        table_name: str = "influenza_data",
        chunk_size: int = 100000,
        show_progress: bool = True
    ):
        """
        ëŒ€ìš©ëŸ‰ CSVë¥¼ PostgreSQL í…Œì´ë¸”ë¡œ ì„í¬íŠ¸
        (DuckDBì˜ read_csv_autoì™€ ë‹¬ë¦¬ pandas+copy_from ì‚¬ìš©)
        """
        # ...êµ¬í˜„ì€ ì´í›„ ë‹¨ê³„ì—ì„œ ì¶”ê°€...
        pass
    
    def export_to_parquet(self, *args, **kwargs):
        """
        (DuckDB ì „ìš© ê¸°ëŠ¥) PostgreSQLì—ì„œëŠ” ì§ì ‘ Parquet ë‚´ë³´ë‚´ê¸° ë¯¸ì§€ì›.
        í•„ìš”ì‹œ pandas DataFrameìœ¼ë¡œ export í›„ pyarrow ë“±ìœ¼ë¡œ ì €ì¥ ê°€ëŠ¥.
        """
        print("âš ï¸ PostgreSQLì€ Parquet ì§ì ‘ ë‚´ë³´ë‚´ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        pass
    
    def load_data(
        self,
        table_name: str = "influenza_data",
        columns: Optional[List[str]] = None,
        where: Optional[str] = None,
        limit: Optional[int] = None,
        order_by: Optional[str] = None
    ) -> pd.DataFrame:
        """
        PostgreSQL í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ Pandas DataFrameìœ¼ë¡œ ë¡œë“œ
        """
        self.connect()
        if columns is None:
            select_cols = "*"
        else:
            select_cols = ", ".join([f'"{col}"' for col in columns])
        query = f"SELECT {select_cols} FROM {table_name}"
        if where:
            query += f" WHERE {where}"
        if order_by:
            query += f" ORDER BY {order_by}"
        if limit:
            query += f" LIMIT {limit}"
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        print(f"ì¿¼ë¦¬: {query[:100]}{'...' if len(query) > 100 else ''}")
        start_time = time.time()
        df = pd.read_sql_query(query, self.conn)
        elapsed = time.time() - start_time
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,} í–‰ Ã— {df.shape[1]} ì—´ ({elapsed:.2f}ì´ˆ)\n")
        return df
    
    def get_table_info(self, table_name: str = "influenza_data"):
        """
        í…Œì´ë¸” ì •ë³´ ì¶œë ¥ (ì»¬ëŸ¼ëª…, ë°ì´í„° íƒ€ì…, ìƒ˜í”Œ ë°ì´í„°)
        """
        self.connect()
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ í…Œì´ë¸” ì •ë³´: {table_name}")
        print(f"{'='*60}")
        # PostgreSQLì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        query = f"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}'"
        schema = pd.read_sql_query(query, self.conn)
        print("\nì»¬ëŸ¼ ì •ë³´:")
        print(schema.to_string(index=False))
        # í–‰ ìˆ˜
        row_count = pd.read_sql_query(f"SELECT COUNT(*) FROM {table_name}", self.conn).iloc[0, 0]
        print(f"\nì´ í–‰ ìˆ˜: {row_count:,}")
        # ìƒ˜í”Œ ë°ì´í„°
        print("\nìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5í–‰):")
        sample = pd.read_sql_query(f"SELECT * FROM {table_name} LIMIT 5", self.conn)
        print(sample.to_string(index=False))
        print(f"{'='*60}\n")
    
    def create_indices(self, table_name: str = "influenza_data", columns: Optional[list] = None):
        """
        PostgreSQL ì¸ë±ìŠ¤ ìƒì„± (ëª…ì‹œì ìœ¼ë¡œ ì§€ì • í•„ìš”)
        """
        self.connect()
        if columns:
            for col in columns:
                sql = f"CREATE INDEX IF NOT EXISTS idx_{table_name}_{col} ON {table_name} (\"{col}\");"
                with self.conn.cursor() as cur:
                    cur.execute(sql)
            self.conn.commit()
            print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {columns}")
        else:
            print("âš ï¸ ì¸ë±ìŠ¤ ìƒì„±í•  ì»¬ëŸ¼ì„ ì§€ì •í•˜ì„¸ìš”.")
    
    def optimize_database(self):
        """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (ANALYZE)"""
        self.connect()
        with self.conn.cursor() as cur:
            cur.execute("ANALYZE;")
        self.conn.commit()
        print(f"âœ… PostgreSQL ANALYZE ì™„ë£Œ\n")



def load_from_postgres(
    table_name: str = "influenza_data",
    **kwargs
) -> pd.DataFrame:
    """
    í¸ì˜ í•¨ìˆ˜: PostgreSQLì—ì„œ ë°ì´í„° ë¡œë“œ
    """
    db = TimeSeriesDB()
    try:
        db.connect()
        return db.load_data(table_name, **kwargs)
    finally:
        db.close()


def fetch_latest_data_from_api(api_url: str = None, dataset_ids: List[str] = None) -> pd.DataFrame:
    """
    APIë¥¼ í†µí•´ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    
    Parameters:
    -----------
    api_url : str, optional
        API ì„œë²„ URL (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ API_URL)
    dataset_ids : List[str], optional
        ê°€ì ¸ì˜¬ ë°ì´í„°ì…‹ ID ë¦¬ìŠ¤íŠ¸
    
    Returns:
    --------
    pd.DataFrame
        APIì—ì„œ ê°€ì ¸ì˜¨ ìµœì‹  ë°ì´í„°
    """
    if api_url is None:
        api_url = os.getenv('API_URL', 'http://localhost:3000')
    
    if dataset_ids is None:
        # ê¸°ë³¸ ë°ì´í„°ì…‹ ID ë¦¬ìŠ¤íŠ¸
        dataset_ids = [
            'ds_0101', 'ds_0102', 'ds_0103', 'ds_0104', 'ds_0105', 
            'ds_0106', 'ds_0107', 'ds_0108', 'ds_0109', 'ds_0110',
            'ds_0701', 'ds_0801', 'ds_0901'
        ]
    
    print(f"\n{'='*60}")
    print(f"ğŸŒ APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
    print(f"{'='*60}")
    print(f"API URL: {api_url}")
    print(f"ë°ì´í„°ì…‹ ê°œìˆ˜: {len(dataset_ids)}")
    
    all_dataframes = []
    
    for idx, dsid in enumerate(dataset_ids, 1):
        print(f"\n[{idx}/{len(dataset_ids)}] {dsid} ë¡œë”© ì¤‘...")
        
        try:
            request_url = f"{api_url}/download"
            request_body = {"dsid": dsid, "returnData": True}  # ë°ì´í„°ë¥¼ ì§ì ‘ ë°˜í™˜ ìš”ì²­
            
            response = requests.post(
                request_url,
                json=request_body,
                timeout=300
            )
            
            if response.status_code != 200:
                print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                continue
            
            result = response.json()
            if not result.get('ok'):
                print(f"  âš ï¸ API ì—ëŸ¬: {result.get('error')}")
                continue
            
            # API ì‘ë‹µì—ì„œ ì§ì ‘ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ ì €ì¥í•˜ì§€ ì•ŠìŒ)
            api_data = result.get('result', {}).get('data', [])
            
            if api_data:
                df = pd.DataFrame(api_data)
                df['dataset_id'] = dsid
                all_dataframes.append(df)
                print(f"  âœ… ì™„ë£Œ: {len(api_data)} ë ˆì½”ë“œ (ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì²˜ë¦¬)")
        
        except Exception as e:
            print(f"  âš ï¸ ì˜¤ë¥˜: {e}")
            continue
        
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(0.5)
    
    if not all_dataframes:
        print(f"\nâš ï¸ ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return pd.DataFrame()
    
    # ëª¨ë“  ë°ì´í„° ë³‘í•©
    df_latest = pd.concat(all_dataframes, ignore_index=True)
    print(f"\nâœ… ìµœì‹  ë°ì´í„° ë³‘í•© ì™„ë£Œ: {df_latest.shape}")
    
    return df_latest


def load_historical_data(before_dir: str = 'data/before') -> pd.DataFrame:
    """
    ê³¼ê±° ë°ì´í„° CSV íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ë³‘í•©
    
    Parameters:
    -----------
    before_dir : str
        ê³¼ê±° ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
    
    Returns:
    --------
    pd.DataFrame
        ë³‘í•©ëœ ê³¼ê±° ë°ì´í„°
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ ê³¼ê±° ë°ì´í„° ë¡œë”©")
    print(f"{'='*60}")
    print(f"ë””ë ‰í† ë¦¬: {before_dir}")
    
    before_path = Path(before_dir)
    if not before_path.exists():
        print(f"âš ï¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {before_dir}")
        return pd.DataFrame()
    
    csv_files = list(before_path.glob("*.csv"))
    print(f"ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ")
    
    if not csv_files:
        return pd.DataFrame()
    
    # ë°ì´í„°ì…‹ IDë³„ë¡œ ë¶„ë¥˜
    data_by_dsid = {}
    
    for filepath in csv_files:
        filename = filepath.name
        # íŒŒì¼ëª… íŒŒì‹±: flu-0105-2022.csv -> dsid=0105, year=2022
        parts = filename.replace('.csv', '').split('-')
        if len(parts) != 3:
            continue
        
        dsid = f"ds_{parts[1]}"
        
        try:
            df = pd.read_csv(filepath)
            if dsid not in data_by_dsid:
                data_by_dsid[dsid] = []
            data_by_dsid[dsid].append(df)
        except Exception as e:
            print(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({filename}): {e}")
    
    # ë°ì´í„°ì…‹ë³„ë¡œ ë³‘í•©
    all_dataframes = []
    for dsid, df_list in data_by_dsid.items():
        df_combined = pd.concat(df_list, ignore_index=True)
        df_combined['dataset_id'] = dsid
        all_dataframes.append(df_combined)
        print(f"  {dsid}: {len(df_list)}ê°œ íŒŒì¼, {len(df_combined)} ë ˆì½”ë“œ")
    
    if not all_dataframes:
        return pd.DataFrame()
    
    df_historical = pd.concat(all_dataframes, ignore_index=True)
    print(f"\nâœ… ê³¼ê±° ë°ì´í„° ë³‘í•© ì™„ë£Œ: {df_historical.shape}")
    
    return df_historical


def consolidate_by_year_week(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë°ì´í„°ì…‹ë³„ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ í†µí•©:
    - ì—°ë„+ì£¼ì°¨+ì—°ë ¹ëŒ€ë¥¼ ê¸°ë³¸ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì—°ë ¹ëŒ€ë³„ ë°ì´í„° ìœ ì§€
    - ì•„í˜• ë°ì´í„°: ìš°ì„¸ ì•„í˜•ì„ ê° ì—°ë ¹ëŒ€ í–‰ì— ì¶”ê°€
    - ì…ì›í™˜ì ìˆ˜: ê°™ì€ í‚¤ë¥¼ ê°€ì§„ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì˜ ê°’ì„ í•©ì‚°
    
    Parameters:
    -----------
    df : pd.DataFrame
        ë³‘í•©í•  ë°ì´í„°í”„ë ˆì„
    
    Returns:
    --------
    pd.DataFrame
        ì˜¬ë°”ë¥´ê²Œ í†µí•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    print("\nğŸ”„ ë°ì´í„° í†µí•© ì¤‘...")
    print(f"í†µí•© ì „: {len(df)} í–‰")
    
    # ì—°ë„ì™€ ì£¼ì°¨ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'ì—°ë„' not in df.columns or 'ì£¼ì°¨' not in df.columns:
        print("âš ï¸ 'ì—°ë„' ë˜ëŠ” 'ì£¼ì°¨' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í†µí•©í•˜ì§€ ì•Šê³  ë°˜í™˜í•©ë‹ˆë‹¤.")
        return df
    
    # ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°
    meta_columns = ['dsId', 'origin', 'contentType', 'originalData', 'parsedData', 'collectedAt', 'id']
    columns_to_drop = [col for col in meta_columns if col in df.columns]
    
    if columns_to_drop:
        print(f"ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì œê±°: {columns_to_drop}")
        df = df.drop(columns=columns_to_drop)
    
    # 1ë‹¨ê³„: ì•„í˜• ë°ì´í„° ì¶”ì¶œ (ì—°ë„+ì£¼ì°¨ë³„ ìš°ì„¸ ì•„í˜•)
    dominant_subtypes = pd.DataFrame()
    if 'ì•„í˜•' in df.columns and 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ' in df.columns:
        print("\n[1ë‹¨ê³„] ì•„í˜• ë°ì´í„° ì²˜ë¦¬: ì—°ë„/ì£¼ì°¨ë³„ ìµœê³  ê²€ì¶œë¥  ì•„í˜• ì„ íƒ")
        
        # 'ê²€ì¶œë¥ ' ê°’ ì œê±° ë° ì•„í˜• ë°ì´í„°ë§Œ ì¶”ì¶œ
        df_subtype = df[(df['ì•„í˜•'].notna()) & (df['ì•„í˜•'] != 'ê²€ì¶œë¥ ')].copy()
        
        if not df_subtype.empty:
            # ê° ì—°ë„/ì£¼ì°¨ì—ì„œ ê°€ì¥ ë†’ì€ ê²€ì¶œë¥ ì„ ê°€ì§„ ì•„í˜• ì°¾ê¸°
            idx_max = df_subtype.groupby(['ì—°ë„', 'ì£¼ì°¨'])['ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ '].idxmax()
            dominant_subtypes = df_subtype.loc[idx_max, ['ì—°ë„', 'ì£¼ì°¨', 'ì•„í˜•']].copy()
            print(f"  ì¶”ì¶œëœ ìš°ì„¸ ì•„í˜•: {len(dominant_subtypes)} ê±´")
            
            # ì•„í˜• í–‰ ì œê±° (ì—°ë ¹ëŒ€ ê¸°ë°˜ í–‰ë§Œ ìœ ì§€)
            df = df[df['ì•„í˜•'].isna() | (df['ì•„í˜•'] == 'ê²€ì¶œë¥ ')].copy()
            if 'ì•„í˜•' in df.columns:
                df = df.drop(columns=['ì•„í˜•'])
    
    # 2ë‹¨ê³„: ì—°ë ¹ëŒ€ ê¸°ë°˜ ë°ì´í„° í†µí•©
    print(f"\n[2ë‹¨ê³„] ì—°ë ¹ëŒ€ë³„ ë°ì´í„° í†µí•©")
    
    # ê·¸ë£¹í™” í‚¤: ì—°ë„, ì£¼ì°¨, ì—°ë ¹ëŒ€
    if 'ì—°ë ¹ëŒ€' not in df.columns:
        print("âš ï¸ 'ì—°ë ¹ëŒ€' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        groupby_cols = ['ì—°ë„', 'ì£¼ì°¨']
    else:
        groupby_cols = ['ì—°ë„', 'ì£¼ì°¨', 'ì—°ë ¹ëŒ€']
    
    # ê° ì»¬ëŸ¼ë³„ ì§‘ê³„ ë°©ì‹ ì •ì˜
    aggregation_dict = {}
    
    for col in df.columns:
        if col in groupby_cols:
            continue
        elif col == 'dataset_id':
            # dataset_idëŠ” ë‚˜ì¤‘ì— ì œê±°
            aggregation_dict[col] = lambda x: ', '.join(sorted(set(str(v) for v in x if pd.notna(v))))
        elif col == 'ì…ì›í™˜ì ìˆ˜':
            # ì…ì›í™˜ì ìˆ˜ëŠ” í•©ì‚° (ds_0103 + ds_0104)
            def sum_patients(x):
                values = [v for v in x if pd.notna(v)]
                if not values:
                    return None
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê°’ë§Œ í•©ì‚°
                numeric_values = []
                for v in values:
                    try:
                        numeric_values.append(float(v))
                    except:
                        pass
                return sum(numeric_values) if numeric_values else None
            
            aggregation_dict[col] = sum_patients
        elif col == 'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì':
            # ì‘ê¸‰ì‹¤ í™˜ìë„ í•©ì‚°
            def sum_emergency(x):
                values = [v for v in x if pd.notna(v)]
                if not values:
                    return None
                numeric_values = []
                for v in values:
                    try:
                        numeric_values.append(float(v))
                    except:
                        pass
                return sum(numeric_values) if numeric_values else None
            
            aggregation_dict[col] = sum_emergency
        elif col in ['ì˜ì‚¬í™˜ì ë¶„ìœ¨', 'ì˜ˆë°©ì ‘ì¢…ë¥ ']:
            # í‰ê· ê°’ ì‚¬ìš©
            aggregation_dict[col] = lambda x: pd.Series([v for v in x if pd.notna(v)]).mean() if any(pd.notna(v) for v in x) else None
        else:
            # ê¸°íƒ€: ì²« ë²ˆì§¸ ìœ íš¨ê°’
            aggregation_dict[col] = lambda x: next((v for v in x if pd.notna(v)), None)
    
    # ê·¸ë£¹í™” ë° ì§‘ê³„
    df_consolidated = df.groupby(groupby_cols, as_index=False).agg(aggregation_dict)
    
    # 3ë‹¨ê³„: ìš°ì„¸ ì•„í˜• ì •ë³´ ë³‘í•©
    if not dominant_subtypes.empty:
        print(f"\n[3ë‹¨ê³„] ìš°ì„¸ ì•„í˜• ì •ë³´ ë³‘í•©")
        df_consolidated = pd.merge(
            df_consolidated, 
            dominant_subtypes, 
            on=['ì—°ë„', 'ì£¼ì°¨'], 
            how='left'
        )
        print(f"  ì•„í˜• ì •ë³´ ì¶”ê°€ ì™„ë£Œ")
    
    # dataset_id ì»¬ëŸ¼ ì œê±°
    if 'dataset_id' in df_consolidated.columns:
        df_consolidated = df_consolidated.drop(columns=['dataset_id'])
    
    print(f"\ní†µí•© í›„: {len(df_consolidated)} í–‰")
    
    # í†µí•© ê²°ê³¼ ìš”ì•½
    if 'ì—°ë ¹ëŒ€' in df_consolidated.columns:
        age_groups = df_consolidated['ì—°ë ¹ëŒ€'].unique()
        print(f"ê³ ìœ  ì—°ë ¹ëŒ€: {len(age_groups)}ê°œ - {', '.join(sorted(age_groups)[:10])}")
    
    if 'ì•„í˜•' in df_consolidated.columns:
        subtypes = df_consolidated['ì•„í˜•'].value_counts()
        print(f"\nì•„í˜• ë¶„í¬:")
        for subtype, count in subtypes.items():
            print(f"  {subtype}: {count}ê±´")
    
    # í•œê¸€â†’ì˜ë¬¸ ì»¬ëŸ¼ëª… ë§¤í•‘ (ëˆ„ë½ ë°©ì§€)
    col_map = {
        'ì—°ë„': 'year',
        'ì£¼ì°¨': 'week',
        'ì—°ë ¹ëŒ€': 'age_group',
        'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
        'ì…ì›í™˜ì ìˆ˜': 'hospitalization',
        'ì•„í˜•': 'subtype',
        'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
        'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate',
        'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients',
    }
    df_consolidated = df_consolidated.rename(columns=col_map)
    return df_consolidated


def merge_and_update_database(
    table_name: str = "influenza_data",
    fetch_latest: bool = True,
    api_url: str = None,
    before_dir: str = 'data/before',
    consolidate: bool = True
):
    """
    API, ê³¼ê±° ë°ì´í„°, ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ PostgreSQLì— ì—…ë°ì´íŠ¸
    
    Parameters
    ----------
    table_name : str
        PostgreSQL í…Œì´ë¸” ì´ë¦„
    fetch_latest : bool
        APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¬ì§€ ì—¬ë¶€
    api_url : str
        API URL (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    before_dir : str
        ê³¼ê±° ë°ì´í„° ë””ë ‰í† ë¦¬
    consolidate : bool
        ê°™ì€ ì—°ë„/ì£¼ì°¨ ë°ì´í„°ë¥¼ í•œ í–‰ìœ¼ë¡œ í†µí•©í• ì§€ ì—¬ë¶€ (ê¸°ë³¸: True)
    """
    print("\n" + "="*60)
    print("ğŸ”„ ë°ì´í„° ë³‘í•© ë° PostgreSQL ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤")
    print("="*60)
    
    all_data = []
    
    # 1. APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    if fetch_latest:
        print("\n[ë‹¨ê³„ 1/4] APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
        df_latest = fetch_latest_data_from_api(api_url=api_url)
        if not df_latest.empty:
            all_data.append(df_latest)
            print(f"âœ… ìµœì‹  ë°ì´í„°: {df_latest.shape}")
        else:
            print("âš ï¸ ìµœì‹  ë°ì´í„° ì—†ìŒ")
    else:
        print("\n[ë‹¨ê³„ 1/4] ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ê±´ë„ˆëœ€")
    
    # 2. ê³¼ê±° ë°ì´í„° ë¡œë”©
    print("\n[ë‹¨ê³„ 2/4] ê³¼ê±° ë°ì´í„° ë¡œë”©")
    df_historical = load_historical_data(before_dir=before_dir)
    if not df_historical.empty:
        all_data.append(df_historical)
        print(f"âœ… ê³¼ê±° ë°ì´í„°: {df_historical.shape}")
    else:
        print("âš ï¸ ê³¼ê±° ë°ì´í„° ì—†ìŒ")
    
    # 3. ëª¨ë“  ë°ì´í„° ë³‘í•©
    print("\n[ë‹¨ê³„ 3/4] ë°ì´í„° ë³‘í•©")
    if not all_data:
        print("âš ï¸ ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    df_merged = pd.concat(all_data, ignore_index=True)
    print(f"ì´ˆê¸° ë³‘í•© ë°ì´í„°: {df_merged.shape}")
    
    # 3-1. ë°ì´í„° í†µí•© (ê°™ì€ ì—°ë„/ì£¼ì°¨ë¥¼ í•œ í–‰ìœ¼ë¡œ)
    if consolidate:
        df_merged = consolidate_by_year_week(df_merged)
    
    print(f"\nìµœì¢… ë³‘í•© ë°ì´í„°: {df_merged.shape}")
    
    # PostgreSQLì— ì €ì¥
    with TimeSeriesDB() as db:
        print(f"\nPostgreSQLì— ì €ì¥ ì¤‘...")
        start_time = time.time()
        
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
        with db.conn.cursor() as cur:
            cur.execute(f"DROP TABLE IF EXISTS {table_name}")
            db.conn.commit()
        
        # í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì‚½ì…
        db.create_table_from_dataframe(df_merged, table_name, if_exists="replace")
        db.insert_dataframe(df_merged, table_name)
        
        elapsed = time.time() - start_time
        with db.conn.cursor() as cur:
            cur.execute(f"SELECT COUNT(*) FROM {table_name}")
            row_count = cur.fetchone()[0]
        
        print(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ!")
        print(f"   â€¢ í…Œì´ë¸”: {table_name}")
        print(f"   â€¢ í–‰ ìˆ˜: {row_count:,}")
        print(f"   â€¢ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
    
    # CSVë¡œë„ ì €ì¥ (ë°±ì—…)
    csv_output = "merged_influenza_data.csv"
    print(f"\nCSV ë°±ì—… ì €ì¥ ì¤‘: {csv_output}")
    df_merged.to_csv(csv_output, index=False)
    csv_size_mb = Path(csv_output).stat().st_size / (1024 * 1024)
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_size_mb:.1f} MB")
    
    print("\n" + "="*60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*60)


if __name__ == "__main__":
    import sys
    
    # ì‚¬ìš© ì˜ˆì œ
    print("=" * 60)
    print("PostgreSQL ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°")
    print("=" * 60)
    
    # ëª…ë ¹í–‰ ì¸ì í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--update":
        # ì—…ë°ì´íŠ¸ ëª¨ë“œ: API + ê³¼ê±° ë°ì´í„° + ê¸°ì¡´ ë°ì´í„° ë³‘í•©
        print("\nğŸ”„ ì—…ë°ì´íŠ¸ ëª¨ë“œ: APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì™€ì„œ ë³‘í•©")
        merge_and_update_database(
            table_name="influenza_data",
            fetch_latest=True,
            before_dir='data/before'
        )
    else:
        # ê¸°ë³¸ ëª¨ë“œ: ê¸°ì¡´ CSVë¥¼ PostgreSQLë¡œ ë³€í™˜
        csv_file = "merged_influenza_data.csv"
        if Path(csv_file).exists():
            print(f"\nğŸ“„ CSV íŒŒì¼ì„ PostgreSQLë¡œ ë³€í™˜ ì¤‘: {csv_file}")
            
            # CSV ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(csv_file)
            
            # PostgreSQLì— ì €ì¥
            with TimeSeriesDB() as db:
                db.create_table_from_dataframe(df, "influenza_data", if_exists="replace")
                db.insert_dataframe(df, "influenza_data")
            
            print("\n" + "=" * 60)
            print("ì‚¬ìš© ì˜ˆì œ:")
            print("=" * 60)
            print("""
# ì „ì²´ ë°ì´í„° ë¡œë“œ
df = load_from_postgres()

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¡œë“œ
df = load_from_postgres(columns=['year', 'week', 'ili'])

# ì¡°ê±´ë¶€ ë¡œë“œ
df = load_from_postgres(where="year >= 2020")

# ìµœê·¼ 1000ê°œ ë°ì´í„°ë§Œ
df = load_from_postgres(limit=1000, order_by="year DESC, week DESC")

# API + ê³¼ê±° ë°ì´í„° + ê¸°ì¡´ ë°ì´í„° ë³‘í•©í•˜ì—¬ ì—…ë°ì´íŠ¸
python db_utils.py --update
            """)
        else:
            print(f"\nâš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
            print("\nğŸ’¡ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ PostgreSQLì„ ìƒì„±í•˜ë ¤ë©´:")
            print("   python db_utils.py --update")
