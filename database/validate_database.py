"""
ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•© ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ì›ë³¸ CSV ë°ì´í„°ì™€ ë³‘í•©ëœ DuckDB ë°ì´í„° ë¹„êµ
- ë°ì´í„° ì†ì‹¤ ë° ë³‘í•© ì˜¤ë¥˜ ê²€ì¦
"""

import pandas as pd
from pathlib import Path
from db_utils import load_from_postgres
from tabulate import tabulate

def load_original_csvs(before_dir='/Volumes/ExternalSSD/Workspace/influenza-prediction-model/data/before'):
    """ì›ë³¸ CSV íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„°ì…‹ë³„ë¡œ ë¶„ë¥˜"""
    print("\n" + "="*100)
    print("ğŸ“‚ ì›ë³¸ CSV íŒŒì¼ ë¡œë“œ ì¤‘...")
    print("="*100)
    
    before_path = Path(before_dir)
    csv_files = sorted(before_path.glob("*.csv"))
    
    print(f"ë°œê²¬ëœ CSV íŒŒì¼: {len(csv_files)}ê°œ\n")
    
    # ë°ì´í„°ì…‹ë³„ë¡œ ë¶„ë¥˜
    data_by_dsid = {}
    
    for filepath in csv_files:  # ëª¨ë“  íŒŒì¼ ë¡œë“œ
        filename = filepath.name
        
        # íŒŒì¼ëª… íŒŒì‹±
        parts = filename.replace('.csv', '').split('-')
        if len(parts) != 3:
            continue
        
        dsid = f"ds_{parts[1]}"
        year = parts[2]
        
        try:
            df = pd.read_csv(filepath)
            
            if dsid not in data_by_dsid:
                data_by_dsid[dsid] = []
            data_by_dsid[dsid].append({
                'year': year,
                'filename': filename,
                'data': df,
                'columns': list(df.columns)
            })
        except Exception as e:
            print(f"   âš ï¸ {filename} ì½ê¸° ì˜¤ë¥˜: {e}\n")
    
    return data_by_dsid


def validate_merge():
    """ë³‘í•© ê³¼ì • ê²€ì¦"""
    
    print("\n" + "="*100)
    print("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•© ê²€ì¦")
    print("="*100)
    
    # 1. ì›ë³¸ CSV ë°ì´í„° ë¡œë“œ
    print("\n[1ë‹¨ê³„] ì›ë³¸ CSV ë°ì´í„° ë¡œë“œ")
    original_data = load_original_csvs()
    
    # 2. PostgreSQL ë°ì´í„° ë¡œë“œ
    print("\n[2ë‹¨ê³„] PostgreSQL ë°ì´í„° ë¡œë“œ")
    from db_utils import load_from_postgres
    db_data = load_from_postgres()
    print(db_data.columns)
    
    print(f"\në³‘í•©ëœ ë°ì´í„°ë² ì´ìŠ¤:")
    print(f"  - í–‰ ìˆ˜: {len(db_data)}")
    print(f"  - ì»¬ëŸ¼: {list(db_data.columns)}")
    # ì»¬ëŸ¼ëª… ë§¤í•‘ (í•œê¸€â†’ì˜ë¬¸, ì˜ë¬¸â†’í•œê¸€ ëª¨ë‘ ì§€ì›)
    col_map = {
        'ì—°ë„': 'year', 'ì£¼ì°¨': 'week', 'ì—°ë ¹ëŒ€': 'age_group', 'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
        'ì…ì›í™˜ì ìˆ˜': 'hospitalization', 'ì•„í˜•': 'subtype', 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
        'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate', 'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients'
    }
    # ì—­ë°©í–¥ë„ ì¶”ê°€
    col_map.update({v: k for k, v in col_map.items()})
    def get_col(df, *candidates):
        for c in candidates:
            if c in df.columns:
                return c
            if col_map.get(c) and col_map[c] in df.columns:
                return col_map[c]
        raise KeyError(f"ì»¬ëŸ¼ í›„ë³´ {candidates} ì¤‘ í•´ë‹¹ë˜ëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {df.columns}")

    # ì—°ë„/ì£¼ì°¨ ì»¬ëŸ¼ëª… ë™ì  ì ‘ê·¼
    year_col = get_col(db_data, 'ì—°ë„', 'year')
    week_col = get_col(db_data, 'ì£¼ì°¨', 'week')
    print(f"  - ì—°ë„ ë²”ìœ„: {db_data[year_col].min():.0f} ~ {db_data[year_col].max():.0f}")
    
    # 3. ìƒ˜í”Œ ë¹„êµ
    print("\n[3ë‹¨ê³„] ë°ì´í„° ìƒ˜í”Œ ë¹„êµ")
    print("\n" + "="*100)
    print("ğŸ” íŠ¹ì • ì—°ë„/ì£¼ì°¨ì˜ ì›ë³¸ ë°ì´í„° vs ë³‘í•© ë°ì´í„° ë¹„êµ")
    print("="*100)
    
    # 2017ë…„ 36ì£¼ ë°ì´í„° ë¹„êµ
    test_year = 2017
    test_week = 36
    
    db_sample = db_data[(db_data['year'] == test_year) & (db_data['week'] == test_week)]
    
    print(f"\në³‘í•©ëœ ë°ì´í„° ({test_year}ë…„ {test_week}ì£¼):")
    print(tabulate(db_sample, headers='keys', tablefmt='simple', showindex=False))
    
    # 4. ê° ë°ì´í„°ì…‹ë³„ ì›ë³¸ í™•ì¸
    print("\n[4ë‹¨ê³„] ë°ì´í„°ì…‹ë³„ ì›ë³¸ ë°ì´í„° í™•ì¸")
    print("\n" + "="*100)
    print("ğŸ“Š ê° dataset_idì˜ ì›ë³¸ ë°ì´í„° êµ¬ì¡°")
    print("="*100)
    
    for dsid, files in original_data.items():
        print(f"\nğŸ”¹ {dsid}:")
        if files:
            sample_df = files[0]['data']
            print(f"   ì»¬ëŸ¼ ëª©ë¡: {list(sample_df.columns)}")
            print(f"   ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3í–‰):")
            print(f"   {sample_df.head(3).to_string(index=False, max_colwidth=30)}")
    
    # 5. ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ì›ì¸ ë¶„ì„
    print("\n[5ë‹¨ê³„] ì»¬ëŸ¼ë³„ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ë¶„ì„")
    print("\n" + "="*100)
    print("ğŸ“Š ê° ë°ì´í„°ì…‹ì´ ê°€ì§„ ì»¬ëŸ¼ ë§µí•‘")
    print("="*100)
    
    column_mapping = {}
    for dsid, files in original_data.items():
        if files:
            cols = set(files[0]['data'].columns)
            column_mapping[dsid] = cols
            print(f"\n{dsid}:")
            print(f"  {', '.join(sorted(cols))}")
    
    # 6. ë³‘í•© í›„ ê²°ì¸¡ì¹˜ ë¶„ì„
    print("\n[6ë‹¨ê³„] ë³‘í•© í›„ ê²°ì¸¡ì¹˜ ë¶„ì„")
    print("\n" + "="*100)
    print("ğŸ“Š ê° ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ í˜„í™©")
    print("="*100)
    
    missing_info = []
    for col in db_data.columns:
        missing_count = db_data[col].isna().sum()
        missing_pct = (missing_count / len(db_data) * 100)
        has_data_count = len(db_data) - missing_count
        
        missing_info.append({
            'ì»¬ëŸ¼ëª…': col,
            'ìœ íš¨ ë°ì´í„°': has_data_count,
            'ê²°ì¸¡ì¹˜': missing_count,
            'ê²°ì¸¡ì¹˜(%)': f"{missing_pct:.1f}%"
        })
    
    print(tabulate(missing_info, headers='keys', tablefmt='simple', showindex=False))
    
    # 7. ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì ë°ì´í„° ì¶”ì 
    print("\n[7ë‹¨ê³„] 'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì' ë°ì´í„° ì¶”ì ")
    print("\n" + "="*100)
    print("ğŸ” ì›ë³¸ CSVì—ì„œ ì‘ê¸‰ì‹¤ ë°ì´í„° ê²€ìƒ‰")
    print("="*100)
    
    for dsid, files in original_data.items():
        for file_info in files[:1]:  # ê° ë°ì´í„°ì…‹ì˜ ì²« íŒŒì¼ë§Œ
            df = file_info['data']
            # ì‘ê¸‰ì‹¤ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
            emergency_cols = [col for col in df.columns if 'ì‘ê¸‰' in col or 'emergency' in col.lower()]
            if emergency_cols:
                print(f"\nâœ… {dsid} ({file_info['filename']}):")
                print(f"   ì‘ê¸‰ì‹¤ ê´€ë ¨ ì»¬ëŸ¼: {emergency_cols}")
                print(f"   ìƒ˜í”Œ ë°ì´í„°:")
                print(f"   {df[emergency_cols].head(5).to_string(index=False)}")
            else:
                print(f"\nâŒ {dsid}: ì‘ê¸‰ì‹¤ ê´€ë ¨ ì»¬ëŸ¼ ì—†ìŒ")
    
    # 8. ì•„í˜• ë°ì´í„° ë‹¤ì–‘ì„± í™•ì¸
    print("\n[8ë‹¨ê³„] 'subtype' ë°ì´í„° ë‹¤ì–‘ì„± í™•ì¸")
    print("\n" + "="*100)
    print("ğŸ” ì›ë³¸ CSV ë° ë³‘í•© ë°ì´í„°ì—ì„œ subtype ë‹¤ì–‘ì„± í™•ì¸")
    print("="*100)
    
    # ì›ë³¸ CSVì—ì„œ subtype ë°ì´í„° í™•ì¸
    for dsid, files in original_data.items():
        has_subtype = False
        for file_info in files[:5]:  # ê° ë°ì´í„°ì…‹ì˜ ì²˜ìŒ 5ê°œ íŒŒì¼
            if 'subtype' in file_info['columns']:
                if not has_subtype:
                    print(f"\nâœ… {dsid}:")
                    has_subtype = True
                
                df = file_info['data']
                unique_subtypes = df['subtype'].unique()
                print(f"   {file_info['year']}ë…„: {len(unique_subtypes)}ê°œ subtype - {', '.join(map(str, unique_subtypes[:10]))}")
    
    # ë³‘í•©ëœ ë°ì´í„°ì—ì„œ subtype í™•ì¸
    print(f"\në³‘í•©ëœ ë°ì´í„°ë² ì´ìŠ¤ì˜ subtype ë‹¤ì–‘ì„±:")
    unique_db_subtypes = db_data['subtype'].unique()
    print(f"  ì´ {len(unique_db_subtypes)}ê°œì˜ ê³ ìœ  subtype:")
    for subtype in unique_db_subtypes[:20]:
        count = (db_data['subtype'] == subtype).sum()
        print(f"    - {subtype}: {count}ê±´")
    
    # 9. 2017ë…„ 36ì£¼ ë°ì´í„° ìƒì„¸ ë¹„êµ
    print("\n[9ë‹¨ê³„] 2017ë…„ 36ì£¼ ë°ì´í„° ìƒì„¸ ë¹„êµ")
    print("\n" + "="*100)
    print("ğŸ” ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•© ë°ì´í„° ë¹„êµ")
    print("="*100)
    
    test_year = 2017
    test_week = 36
    
    print(f"\nì›ë³¸ CSV ë°ì´í„° ({test_year}ë…„ {test_week}ì£¼):")
    for dsid, files in sorted(original_data.items()):
        for file_info in files:
            if file_info['year'] == str(test_year):
                df = file_info['data']
                week_data = df[df['week'] == test_week] if 'week' in df.columns else pd.DataFrame()
                if not week_data.empty:
                    print(f"\n  {dsid} ({file_info['filename']}):")
                    print(f"  {week_data.to_string(index=False, max_colwidth=30)}")
    
    print(f"\në³‘í•©ëœ ë°ì´í„° ({test_year}ë…„ {test_week}ì£¼):")
    db_sample = db_data[(db_data['year'] == test_year) & (db_data['week'] == test_week)]
    print(tabulate(db_sample, headers='keys', tablefmt='simple', showindex=False, maxcolwidths=30))
    
    # 10. ë¬¸ì œì  ê²€ì¦
    print("\n[10ë‹¨ê³„] ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦")
    print("\n" + "="*100)
    print("ğŸ” ë³‘í•© ì „í›„ ë°ì´í„° ë¹„êµ ë° ë¬¸ì œì  ê²€ì¦")
    print("="*100)
    
    issues = []
    
    # ë¬¸ì œ 1: ì—°ë ¹ëŒ€ ì†ì‹¤ í™•ì¸
    age_group_loss = False
    if 'age_group' in db_data.columns:
        unique_ages = db_data['age_group'].nunique()
        if unique_ages < 7:  # ìµœì†Œ 7ê°œ ì—°ë ¹ëŒ€ëŠ” ìˆì–´ì•¼ í•¨
            age_group_loss = True
            issues.append(f"ì—°ë ¹ëŒ€ ë°ì´í„° ì†ì‹¤: {unique_ages}ê°œë§Œ ì¡´ì¬ (ì˜ˆìƒ: 7ê°œ ì´ìƒ)")
    
    # ë¬¸ì œ 2: subtype ë‹¤ì–‘ì„± í™•ì¸
    subtype_loss = False
    if 'subtype' in db_data.columns:
        unique_subtypes = db_data['subtype'].nunique()
        if unique_subtypes < 3:  # ìµœì†Œ 3ê°œ subtype (A(H1N1)pdm09, A(H3N2), B)
            subtype_loss = True
            issues.append(f"subtype ë°ì´í„° ì†ì‹¤: {unique_subtypes}ê°œë§Œ ì¡´ì¬ (ì˜ˆìƒ: 3ê°œ ì´ìƒ)")
    
    # ë¬¸ì œ 3: ì…ì›í™˜ì ìˆ˜ í•©ì‚° í™•ì¸ (2017ë…„ 36ì£¼ ì˜ˆì‹œ)
    test_sample = db_data[(db_data['year'] == 2017) & (db_data['week'] == 36) & (db_data['age_group'] == '65ì„¸ì´ìƒ')]
    if not test_sample.empty and 'hospitalization' in test_sample.columns:
        merged_patients = test_sample['hospitalization'].iloc[0]
        # ì›ë³¸: ds_0103=8, ds_0104=1 -> í•©ê³„ 9
        if merged_patients < 9:
            issues.append(f"ì…ì›í™˜ì ìˆ˜ í•©ì‚° ì˜¤ë¥˜: 2017ë…„ 36ì£¼ 65ì„¸ì´ìƒ {merged_patients}ëª… (ì˜ˆìƒ: 9ëª…)")
    
    # ë¬¸ì œ 4: ë°ì´í„° ê³¼ë„í•œ ì¶•ì†Œ í™•ì¸
    expected_min_rows = 3000  # ìµœì†Œ 3000í–‰ ì´ìƒì€ ìˆì–´ì•¼ í•¨
    if len(db_data) < expected_min_rows:
        issues.append(f"ë°ì´í„° ê³¼ë„í•œ ì¶•ì†Œ: {len(db_data)}í–‰ (ì˜ˆìƒ: {expected_min_rows}í–‰ ì´ìƒ)")
    
    # ë¬¸ì œ 5: í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ í™•ì¸
    required_columns = ['year', 'week', 'age_group', 'ili', 'hospitalization', 'subtype', 'detection_rate']
    missing_columns = [col for col in required_columns if col not in db_data.columns]
    if missing_columns:
        issues.append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {', '.join(missing_columns)}")
    
    # ë¬¸ì œ 6: ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ì€ ì»¬ëŸ¼ í™•ì¸ (80% ì´ìƒ)
    high_missing_cols = []
    for col in db_data.columns:
        if col in ['year', 'week', 'age_group', 'subtype']:  # í•„ìˆ˜ í‚¤ ì»¬ëŸ¼ì€ ì œì™¸
            continue
        missing_rate = db_data[col].isna().sum() / len(db_data) * 100
        if missing_rate > 80:
            high_missing_cols.append(f"{col} ({missing_rate:.1f}%)")
    
    if high_missing_cols:
        issues.append(f"ê³¼ë„í•œ ê²°ì¸¡ì¹˜ ë°œê²¬ (>80%): {', '.join(high_missing_cols)}")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*100)
    if issues:
        print("âš ï¸ ë¬¸ì œì  ë°œê²¬!")
        print("="*100)
        for i, issue in enumerate(issues, 1):
            print(f"\në¬¸ì œì  {i}: {issue}")
    else:
        print("âœ… ë¬¸ì œì  ì—†ìŒ!")
        print("="*100)
        print("\nëª¨ë“  ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"  â€¢ ì´ í–‰ ìˆ˜: {len(db_data):,}í–‰")
        print(f"  â€¢ ê³ ìœ  ì—°ë ¹ëŒ€: {db_data['age_group'].nunique()}ê°œ")
        print(f"  â€¢ ê³ ìœ  subtype: {db_data['subtype'].nunique()}ê°œ")
        if not test_sample.empty:
            print(f"  â€¢ ì…ì›í™˜ì ìˆ˜ í•©ì‚°: ì •ìƒ (2017ë…„ 36ì£¼ 65ì„¸ì´ìƒ {merged_patients}ëª…)")
    
    print("\n" + "="*100)
    print("âœ… ê²€ì¦ ì™„ë£Œ!")
    print("="*100)


if __name__ == "__main__":
    validate_merge()
