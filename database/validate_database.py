"""
PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ì¸í”Œë£¨ì—”ì ë°ì´í„°: ì „ì²´ ì»¬ëŸ¼ ì •ë ¬ ê²€ì¦ (ili, detection_rate, hospitalization, emergency_patients ë“±)
- íŠ¸ë Œë“œ ë°ì´í„°: Google, Naver, Twitter Trends ë°ì´í„° ê²€ì¦
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path

# patchTST.py importë¥¼ ìœ„í•´ ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

from patchTST import load_and_prepare
try:
    from .db_utils import load_from_postgres, load_trends_from_postgres
except ImportError:
    from db_utils import load_from_postgres, load_trends_from_postgres


def validate_influenza_data():
    """ì¸í”Œë£¨ì—”ì ë°ì´í„° ê²€ì¦ (ì „ì²´ ì»¬ëŸ¼ ì •ë ¬)"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ” ì¸í”Œë£¨ì—”ì ë°ì´í„° ì •ë ¬ ê²€ì¦")
    print(f"{'='*80}\n")

    # Step 1: CSV ì›ë³¸ì—ì„œ 19-49ì„¸ ì¶”ì¶œ
    print(f"ğŸ“„ STEP 1: CSV ì›ë³¸ (19-49ì„¸)")
    print(f"{'='*80}")
    csv_path = parent_dir / 'merged_influenza_data.csv'
    
    if not csv_path.exists():
        print(f"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        print(f"   ë¨¼ì € 'python database/update_database.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    df_csv = pd.read_csv(csv_path)
    df_19_49 = df_csv[df_csv['age_group'] == '19-49ì„¸'].copy()
    df_19_49 = df_19_49.sort_values(['year', 'week']).reset_index(drop=True)
    print(f"âœ… 19-49ì„¸ ë°ì´í„°: {df_19_49.shape}")

    # Step 2: patchTST.pyë¡œ íŒŒì‹±
    print(f"\nğŸ”§ STEP 2: patchTST.pyë¡œ íŒŒì‹±")
    print(f"{'='*80}")
    df_pg = load_from_postgres('influenza_data')
    X, y, labels, feat_names = load_and_prepare(df=df_pg, use_exog='all')
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: X={X.shape}, y={y.shape}")
    print(f"   Features: {feat_names}")

    # Step 3: Xë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    print(f"\nğŸ”„ STEP 3: íŒŒì‹± ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜")
    print(f"{'='*80}")
    df_parsed = pd.DataFrame(X, columns=feat_names)
    df_parsed['ili'] = y

    # labelsì—ì„œ year, week ì¶”ì¶œ
    df_parsed['year'] = pd.Series(labels).str.extract(r'(\d{4})-\d{4}').astype(float)
    df_parsed['week'] = pd.Series(labels).str.extract(r'W(\d+)').astype(float)

    print(f"âœ… ë³€í™˜ ì™„ë£Œ: {df_parsed.shape}")
    print(f"\nì²˜ìŒ 5í–‰:")
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¶œë ¥
    display_cols = ['year', 'week', 'ili']
    for col in ['detection_rate', 'hospitalization', 'emergency_patients']:
        if col in df_parsed.columns:
            display_cols.append(col)
    print(df_parsed[display_cols].head())

    # Step 4: ê° ì»¬ëŸ¼ë³„ ë¹„êµ
    print(f"\nğŸ¯ STEP 4: ì»¬ëŸ¼ë³„ ê°’ ì¼ì¹˜ ê²€ì¦")
    print(f"{'='*80}")

    # ë¹„êµí•  ì»¬ëŸ¼ ëª©ë¡ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
    compare_columns = [col for col in ['ili', 'detection_rate', 'hospitalization', 'emergency_patients'] 
                       if col in df_parsed.columns]

    results = []

    for col in compare_columns:
        print(f"\n{'='*60}")
        print(f"[{col}] ê²€ì¦")
        print(f"{'='*60}")
        
        if col not in df_19_49.columns:
            print(f"âš ï¸  CSV ì›ë³¸ì— {col} ì»¬ëŸ¼ ì—†ìŒ")
            continue
        
        if col not in df_parsed.columns:
            print(f"âš ï¸  íŒŒì‹± ë°ì´í„°ì— {col} ì»¬ëŸ¼ ì—†ìŒ")
            continue
        
        # ê°’ ì¶”ì¶œ
        min_len = min(len(df_19_49), len(df_parsed))
        vals_csv = pd.to_numeric(df_19_49[col], errors='coerce').iloc[:min_len].values
        vals_parsed = pd.to_numeric(df_parsed[col], errors='coerce').iloc[:min_len].values
        
        # NaN ì²˜ë¦¬
        valid_mask = ~(np.isnan(vals_csv) | np.isnan(vals_parsed))
        n_valid = valid_mask.sum()
        
        if n_valid == 0:
            print(f"âš ï¸  ë¹„êµ ê°€ëŠ¥í•œ ìœ íš¨ ë°ì´í„° ì—†ìŒ")
            continue
        
        # ì°¨ì´ ê³„ì‚°
        diff = np.abs(vals_csv[valid_mask] - vals_parsed[valid_mask])
        max_diff = diff.max()
        mean_diff = diff.mean()
        median_diff = np.median(diff)
        num_match = (diff < 0.0001).sum()
        match_pct = num_match / n_valid * 100
        
        print(f"\nğŸ“Š ë¹„êµ ê²°ê³¼:")
        print(f"   ë¹„êµ ê°€ëŠ¥í•œ í–‰: {n_valid}/{min_len}ê°œ")
        print(f"   ìµœëŒ€ ì°¨ì´:      {max_diff:.6f}")
        print(f"   í‰ê·  ì°¨ì´:      {mean_diff:.6f}")
        print(f"   ì¤‘ê°„ê°’ ì°¨ì´:    {median_diff:.6f}")
        print(f"   ì¼ì¹˜í•˜ëŠ” í–‰:    {num_match}/{n_valid} ({match_pct:.1f}%)")
        
        # ê²°ê³¼ ì €ì¥
        results.append({
            'column': col,
            'valid_rows': n_valid,
            'max_diff': max_diff,
            'mean_diff': mean_diff,
            'match_count': num_match,
            'match_pct': match_pct
        })
        
        # íŒ¬ë°ë¯¹ ê¸°ê°„ ì œì™¸ ë¹„êµ
        pandemic_mask = (
            ((df_19_49['year'].iloc[:min_len] == 2020) & (df_19_49['week'].iloc[:min_len] >= 14)) |
            (df_19_49['year'].iloc[:min_len] == 2021) |
            ((df_19_49['year'].iloc[:min_len] == 2022) & (df_19_49['week'].iloc[:min_len] <= 22))
        ).values
        
        non_pandemic_mask = valid_mask & ~pandemic_mask
        n_non_pandemic = non_pandemic_mask.sum()
        
        if n_non_pandemic > 0:
            diff_non_pandemic = np.abs(vals_csv[non_pandemic_mask] - vals_parsed[non_pandemic_mask])
            num_match_non_pandemic = (diff_non_pandemic < 0.0001).sum()
            match_pct_non_pandemic = num_match_non_pandemic / n_non_pandemic * 100
            
            print(f"\n   ğŸ“Œ íŒ¬ë°ë¯¹ ê¸°ê°„ ì œì™¸ ë¹„êµ:")
            print(f"      ë¹„êµ í–‰:        {n_non_pandemic}ê°œ")
            print(f"      ì¼ì¹˜í•˜ëŠ” í–‰:    {num_match_non_pandemic}/{n_non_pandemic} ({match_pct_non_pandemic:.1f}%)")
        
        # ì°¨ì´ê°€ í° í–‰ (íŒ¬ë°ë¯¹ ì œì™¸)
        if n_non_pandemic > 0 and match_pct_non_pandemic < 100:
            print(f"\n   âš ï¸  íŒ¬ë°ë¯¹ ì œì™¸ ì‹œì—ë„ ë¶ˆì¼ì¹˜ ë°œê²¬!")
            
            # non_pandemic_maskì˜ ì¸ë±ìŠ¤ë¥¼ ì›ë˜ ë°°ì—´ë¡œ ë³µì›
            non_pandemic_indices = np.where(non_pandemic_mask)[0]
            diff_at_indices = np.abs(vals_csv[non_pandemic_mask] - vals_parsed[non_pandemic_mask])
            top_5_local_idx = np.argsort(diff_at_indices)[-min(5, len(diff_at_indices)):][::-1]
            top_5_global_idx = non_pandemic_indices[top_5_local_idx]
            
            print(f"      ì°¨ì´ê°€ í° ìƒìœ„ {len(top_5_global_idx)}ê°œ í–‰:")
            print(f"      {'í–‰':>5} {'ë…„ë„':>6} {'ì£¼ì°¨':>4} {'CSV ì›ë³¸':>12} {'íŒŒì‹±':>12} {'ì°¨ì´':>12}")
            print(f"      {'-'*60}")
            
            for idx in top_5_global_idx:
                year_val = df_19_49.iloc[idx]['year']
                week_val = df_19_49.iloc[idx]['week']
                csv_val = vals_csv[idx]
                parsed_val = vals_parsed[idx]
                diff_val = abs(csv_val - parsed_val)
                print(f"      {idx:>5} {year_val:>6.0f} {week_val:>4.0f} {csv_val:>12.2f} {parsed_val:>12.2f} {diff_val:>12.4f}")

    # Step 5: ì²˜ìŒ 20ê°œ ìƒ˜í”Œ ìƒì„¸ ë¹„êµ
    print(f"\n{'='*80}")
    print(f"ğŸ” STEP 5: ì²˜ìŒ 20ê°œ ìƒ˜í”Œ ìƒì„¸ ë¹„êµ")
    print(f"{'='*80}")

    print(f"\n{'í–‰':>5} {'ë…„ë„':>6} {'ì£¼ì°¨':>4} {'ì»¬ëŸ¼':>20} {'CSV ì›ë³¸':>12} {'íŒŒì‹±':>12} {'ì°¨ì´':>12} {'ìƒíƒœ':>6}")
    print(f"{'-'*85}")

    for i in range(min(20, len(df_19_49), len(df_parsed))):
        year_val = df_19_49.iloc[i]['year']
        week_val = df_19_49.iloc[i]['week']
        
        for col in compare_columns:
            if col in df_19_49.columns and col in df_parsed.columns:
                csv_val = pd.to_numeric(df_19_49.iloc[i][col], errors='coerce')
                parsed_val = pd.to_numeric(df_parsed.iloc[i][col], errors='coerce')
                
                if pd.notna(csv_val) and pd.notna(parsed_val):
                    diff_val = abs(csv_val - parsed_val)
                    status = "âœ…" if diff_val < 0.0001 else "âš ï¸"
                    print(f"{i:>5} {year_val:>6.0f} {week_val:>4.0f} {col:>20} {csv_val:>12.2f} {parsed_val:>12.2f} {diff_val:>12.6f} {status:>6}")

    # Step 6: ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ STEP 6: ì¸í”Œë£¨ì—”ì ë°ì´í„° ê²€ì¦ ìš”ì•½")
    print(f"{'='*80}")

    summary_df = pd.DataFrame(results)
    if len(summary_df) > 0:
        print(f"\nì „ì²´ ì»¬ëŸ¼ ì¼ì¹˜ë„:")
        print(summary_df.to_string(index=False))
        
        all_match = summary_df['match_pct'].min()
        if all_match >= 99.9:
            print(f"\nâœ… ëª¨ë“  ì»¬ëŸ¼ì´ ê±°ì˜ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤! (ìµœì†Œ {all_match:.1f}%)")
            return True
        elif all_match >= 90:
            print(f"\nâš ï¸  ì¼ë¶€ ë¶ˆì¼ì¹˜ê°€ ìˆì§€ë§Œ ëŒ€ë¶€ë¶„ ì¼ì¹˜í•©ë‹ˆë‹¤. (ìµœì†Œ {all_match:.1f}%)")
            print(f"    ë¶ˆì¼ì¹˜ëŠ” ì£¼ë¡œ íŒ¬ë°ë¯¹ ê¸°ê°„ ë³´ê°„ìœ¼ë¡œ ì¸í•œ ê²ƒì…ë‹ˆë‹¤.")
            return True
        else:
            print(f"\nâŒ ì‹¬ê°í•œ ë¶ˆì¼ì¹˜ ë°œê²¬! (ìµœì†Œ {all_match:.1f}%)")
            print(f"    ì •ë ¬ ë¡œì§ì„ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
            return False
    else:
        print(f"\nâš ï¸  ê²€ì¦ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False


def validate_trends_data():
    """íŠ¸ë Œë“œ ë°ì´í„° ê²€ì¦ (Google, Naver, Twitter)"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ” íŠ¸ë Œë“œ ë°ì´í„° ê²€ì¦")
    print(f"{'='*80}\n")
    
    # Step 1: CSV ë°±ì—… íŒŒì¼ ë¡œë“œ
    print(f"ğŸ“„ STEP 1: CSV ë°±ì—… íŒŒì¼ ë¡œë“œ")
    print(f"{'='*80}")
    csv_path = parent_dir / 'trends_data.csv'
    
    if not csv_path.exists():
        print(f"âš ï¸  íŠ¸ë Œë“œ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        print(f"   ë¨¼ì € 'python database/update_database.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    df_csv = pd.read_csv(csv_path)
    df_csv = df_csv.sort_values(['year', 'week']).reset_index(drop=True)
    print(f"âœ… CSV ë°±ì—…: {df_csv.shape}")
    print(f"   ì»¬ëŸ¼ ìˆ˜: {len(df_csv.columns)}")
    
    # íŠ¸ë Œë“œ ì»¬ëŸ¼ ë¶„ë¥˜
    google_cols = [c for c in df_csv.columns if c.startswith('google_')]
    naver_cols = [c for c in df_csv.columns if c.startswith('naver_')]
    twitter_cols = [c for c in df_csv.columns if c.startswith('twitter_')]
    
    print(f"   Google Trends: {len(google_cols)}ê°œ ì»¬ëŸ¼")
    print(f"   Naver Trends: {len(naver_cols)}ê°œ ì»¬ëŸ¼")
    print(f"   Twitter Trends: {len(twitter_cols)}ê°œ ì»¬ëŸ¼")
    
    # Step 2: PostgreSQL íŠ¸ë Œë“œ DB ë¡œë“œ
    print(f"\nğŸ”§ STEP 2: PostgreSQL trends DB ë¡œë“œ")
    print(f"{'='*80}")
    
    try:
        df_pg = load_trends_from_postgres()
        if df_pg.empty:
            print(f"âŒ PostgreSQL trends DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        
        df_pg = df_pg.sort_values(['year', 'week']).reset_index(drop=True)
        print(f"âœ… PostgreSQL ë¡œë“œ: {df_pg.shape}")
        print(f"   ì»¬ëŸ¼ ìˆ˜: {len(df_pg.columns)}")
    except Exception as e:
        print(f"âŒ PostgreSQL ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"   ë¨¼ì € 'python database/update_database.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    # Step 3: ë°ì´í„° ë¹„êµ
    print(f"\nğŸ¯ STEP 3: CSV vs PostgreSQL ë¹„êµ")
    print(f"{'='*80}")
    
    # í–‰ ìˆ˜ ë¹„êµ
    if len(df_csv) != len(df_pg):
        print(f"âš ï¸  í–‰ ìˆ˜ ë¶ˆì¼ì¹˜:")
        print(f"   CSV: {len(df_csv)}í–‰")
        print(f"   PostgreSQL: {len(df_pg)}í–‰")
    else:
        print(f"âœ… í–‰ ìˆ˜ ì¼ì¹˜: {len(df_csv)}í–‰")
    
    # ì»¬ëŸ¼ ìˆ˜ ë¹„êµ
    if len(df_csv.columns) != len(df_pg.columns):
        print(f"\nâš ï¸  ì»¬ëŸ¼ ìˆ˜ ë¶ˆì¼ì¹˜:")
        print(f"   CSV: {len(df_csv.columns)}ê°œ")
        print(f"   PostgreSQL: {len(df_pg.columns)}ê°œ")
        
        csv_only = set(df_csv.columns) - set(df_pg.columns)
        pg_only = set(df_pg.columns) - set(df_csv.columns)
        
        if csv_only:
            print(f"   CSVì—ë§Œ ìˆëŠ” ì»¬ëŸ¼: {csv_only}")
        if pg_only:
            print(f"   PostgreSQLì—ë§Œ ìˆëŠ” ì»¬ëŸ¼: {pg_only}")
    else:
        print(f"âœ… ì»¬ëŸ¼ ìˆ˜ ì¼ì¹˜: {len(df_csv.columns)}ê°œ")
    
    # Step 4: ê°’ ì¼ì¹˜ ê²€ì¦
    print(f"\nğŸ” STEP 4: ê°’ ì¼ì¹˜ ê²€ì¦")
    print(f"{'='*80}")
    
    common_cols = list(set(df_csv.columns) & set(df_pg.columns))
    common_cols = [c for c in common_cols if c not in ['year', 'week']]
    
    results = []
    
    for col in common_cols:
        vals_csv = pd.to_numeric(df_csv[col], errors='coerce').values
        vals_pg = pd.to_numeric(df_pg[col], errors='coerce').values
        
        min_len = min(len(vals_csv), len(vals_pg))
        vals_csv = vals_csv[:min_len]
        vals_pg = vals_pg[:min_len]
        
        # NaN ì²˜ë¦¬
        valid_mask = ~(np.isnan(vals_csv) | np.isnan(vals_pg))
        n_valid = valid_mask.sum()
        
        if n_valid == 0:
            continue
        
        # ì°¨ì´ ê³„ì‚°
        diff = np.abs(vals_csv[valid_mask] - vals_pg[valid_mask])
        max_diff = diff.max()
        mean_diff = diff.mean()
        num_match = (diff < 0.0001).sum()
        match_pct = num_match / n_valid * 100
        
        results.append({
            'column': col,
            'valid_rows': n_valid,
            'max_diff': max_diff,
            'mean_diff': mean_diff,
            'match_count': num_match,
            'match_pct': match_pct
        })
    
    # Step 5: ìš”ì•½
    print(f"\nğŸ“‹ STEP 5: íŠ¸ë Œë“œ ë°ì´í„° ê²€ì¦ ìš”ì•½")
    print(f"{'='*80}")
    
    if len(results) > 0:
        summary_df = pd.DataFrame(results)
        
        # ìƒìœ„/í•˜ìœ„ 10ê°œë§Œ í‘œì‹œ
        print(f"\nì¼ì¹˜ë„ê°€ ë‚®ì€ ìƒìœ„ 10ê°œ ì»¬ëŸ¼:")
        print(summary_df.nsmallest(min(10, len(summary_df)), 'match_pct').to_string(index=False))
        
        print(f"\nì¼ì¹˜ë„ê°€ ë†’ì€ ìƒìœ„ 10ê°œ ì»¬ëŸ¼:")
        print(summary_df.nlargest(min(10, len(summary_df)), 'match_pct').to_string(index=False))
        
        all_match = summary_df['match_pct'].min()
        avg_match = summary_df['match_pct'].mean()
        
        print(f"\nì „ì²´ í†µê³„:")
        print(f"   ê²€ì¦ ì»¬ëŸ¼ ìˆ˜: {len(summary_df)}ê°œ")
        print(f"   í‰ê·  ì¼ì¹˜ë„: {avg_match:.2f}%")
        print(f"   ìµœì†Œ ì¼ì¹˜ë„: {all_match:.2f}%")
        
        if all_match >= 99.9:
            print(f"\nâœ… ëª¨ë“  íŠ¸ë Œë“œ ì»¬ëŸ¼ì´ ê±°ì˜ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤!")
            return True
        elif all_match >= 95:
            print(f"\nâœ… ëŒ€ë¶€ë¶„ì˜ íŠ¸ë Œë“œ ì»¬ëŸ¼ì´ ì¼ì¹˜í•©ë‹ˆë‹¤.")
            return True
        else:
            print(f"\nâš ï¸  ì¼ë¶€ ì»¬ëŸ¼ì—ì„œ ë¶ˆì¼ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"    í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return False
    else:
        print(f"\nâš ï¸  ê²€ì¦ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False


if __name__ == "__main__":
    print(f"\n{'='*80}")
    print(f"ğŸ“Š PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ê²€ì¦")
    print(f"{'='*80}")
    
    success_count = 0
    total_tests = 2
    
    # ì¸í”Œë£¨ì—”ì ë°ì´í„° ê²€ì¦
    if validate_influenza_data():
        success_count += 1
        print(f"\nâœ… ì¸í”Œë£¨ì—”ì ë°ì´í„° ê²€ì¦ ì™„ë£Œ!")
    else:
        print(f"\nâŒ ì¸í”Œë£¨ì—”ì ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨!")
    
    # íŠ¸ë Œë“œ ë°ì´í„° ê²€ì¦
    if validate_trends_data():
        success_count += 1
        print(f"\nâœ… íŠ¸ë Œë“œ ë°ì´í„° ê²€ì¦ ì™„ë£Œ!")
    else:
        print(f"\nâŒ íŠ¸ë Œë“œ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨!")
    
    # ìµœì¢… ê²°ê³¼
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ìµœì¢… ê²€ì¦ ê²°ê³¼")
    print(f"{'='*80}")
    print(f"\nê²€ì¦ ê²°ê³¼: {success_count}/{total_tests} ì„±ê³µ")
    
    if success_count == total_tests:
        print(f"\nâœ… ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì™„ë£Œ!")
        print(f"   ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    elif success_count > 0:
        print(f"\nâš ï¸  ì¼ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨")
        print(f"   ì‹¤íŒ¨í•œ í•­ëª©ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.")
        print(f"   python database/update_database.py")
    else:
        print(f"\nâŒ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨")
        print(f"   ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”:")
        print(f"   python database/update_database.py")
    
    print(f"\n{'='*80}")
