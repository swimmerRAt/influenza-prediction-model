#!/usr/bin/env python3
"""
API ë°ì´í„° vs merged_influenza_data.csv ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

GFID APIì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ë¡œì»¬ merged_influenza_data.csvì™€ ë¹„êµí•©ë‹ˆë‹¤.

ë¹„êµ í•­ëª©:
1. APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° í†µê³„
2. ë³‘í•© ë°ì´í„° í†µê³„
3. ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ë¹„êµ
4. ê°’ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
from pathlib import Path

# api_client ì„í¬íŠ¸
try:
    from api_client import (
        get_recent_etl_data,
        get_etl_data_by_date_range,
        is_auth_configured,
        INFLUENZA_DATASETS,
    )
except ImportError:
    from database.api_client import (
        get_recent_etl_data,
        get_etl_data_by_date_range,
        is_auth_configured,
        INFLUENZA_DATASETS,
    )


# ë°ì´í„°ì…‹ ID â†’ ì»¬ëŸ¼ ë§¤í•‘
DATASET_COLUMN_MAPPING = {
    'ds_0101': {
        'name': 'ILI (ì˜ì‚¬í™˜ì ë¶„ìœ¨)',
        'value_col': 'ì˜ì‚¬í™˜ì ë¶„ìœ¨',
        'merged_col': 'ili',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
    'ds_0103': {
        'name': 'SARI (ì¤‘ì¦ê¸‰ì„±í˜¸í¡ê¸°ê°ì—¼ì¦)',
        'value_col': 'ì…ì›í™˜ì ìˆ˜',
        'merged_col': 'hospitalization',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
    'ds_0104': {
        'name': 'ARI (ê¸‰ì„±í˜¸í¡ê¸°ê°ì—¼ì¦)',
        'value_col': 'ì…ì›í™˜ì ìˆ˜',
        'merged_col': 'hospitalization',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
    'ds_0105': {
        'name': 'I-RISS (ê²€ì‚¬ê¸°ê´€ ê²€ì¶œë¥ )',
        'value_col': 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ',
        'merged_col': 'detection_rate',
        'group_col': 'ì•„í˜•'
    },
    'ds_0106': {
        'name': 'K-RISS (ì˜ì›ê¸‰ ê²€ì¶œë¥ )',
        'value_col': 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ',
        'merged_col': 'detection_rate',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
    'ds_0107': {
        'name': 'í˜¸í¡ê¸°ë³‘ì›ì²´ ê²€ì¶œí˜„í™©',
        'value_col': 'ê²€ì¶œë¥ ',
        'merged_col': 'detection_rate',
        'group_col': 'ì•„í˜•'
    },
    'ds_0108': {
        'name': 'ì¸í”Œë£¨ì—”ì í‘œë³¸ê°ì‹œ',
        'value_col': 'ê²€ì¶œë¥ ',
        'merged_col': 'detection_rate',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
    'ds_0109': {
        'name': 'NEDIS (ì‘ê¸‰ì‹¤ í™˜ì)',
        'value_col': 'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì',  # APIì—ì„œ ë°˜í™˜í•˜ëŠ” ì‹¤ì œ ì»¬ëŸ¼ëª…
        'value_col_alt': ['í™˜ì ìˆ˜', 'ì¸í”Œë£¨ì—”ì í™˜ì', 'ì‘ê¸‰ì‹¤ í™˜ì'],  # ëŒ€ì²´ ì»¬ëŸ¼ëª…
        'merged_col': 'emergency_patients',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
    'ds_0110': {
        'name': 'ì˜ˆë°©ì ‘ì¢…ë¥ ',
        'value_col': 'ì˜ˆë°©ì ‘ì¢…ë¥ ',  # APIì—ì„œ ë°˜í™˜í•˜ëŠ” ì‹¤ì œ ì»¬ëŸ¼ëª…
        'value_col_alt': ['ì ‘ì¢…ë¥ ', 'ì ‘ì¢…ìœ¨'],  # ëŒ€ì²´ ì»¬ëŸ¼ëª…
        'merged_col': 'vaccine_rate',
        'group_col': 'ì—°ë ¹ëŒ€'
    },
}


def flatten_parsed_data(data_list):
    """
    API ì‘ë‹µì˜ parsedData í•„ë“œë¥¼ í”Œë˜íŠ¼
    """
    flattened = []
    
    for item in data_list:
        if isinstance(item, dict):
            if 'parsedData' in item and isinstance(item['parsedData'], dict):
                flat_item = item['parsedData'].copy()
                if 'collectedAt' in item:
                    flat_item['collectedAt'] = item['collectedAt']
                flattened.append(flat_item)
            elif 'parsedData' in item and isinstance(item['parsedData'], str):
                # JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                import json
                try:
                    parsed = json.loads(item['parsedData'])
                    if isinstance(parsed, list):
                        for p in parsed:
                            if isinstance(p, dict):
                                if 'collectedAt' in item:
                                    p['collectedAt'] = item['collectedAt']
                                flattened.append(p)
                    elif isinstance(parsed, dict):
                        if 'collectedAt' in item:
                            parsed['collectedAt'] = item['collectedAt']
                        flattened.append(parsed)
                except:
                    pass
            else:
                flattened.append(item)
        else:
            flattened.append(item)
    
    return flattened


def fetch_api_data(dsid, cnt=50):
    """
    APIì—ì„œ íŠ¹ì • ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
    (originë³„ë¡œ ì‹¤ì œ ë°ì´í„° ì¡°íšŒ)
    
    Returns:
        DataFrame ë˜ëŠ” None
    """
    try:
        from api_client import get_etl_data_by_origin
    except ImportError:
        from database.api_client import get_etl_data_by_origin
    
    import json
    
    mapping = DATASET_COLUMN_MAPPING.get(dsid, {})
    name = mapping.get('name', dsid)
    
    print(f"   ğŸ“¡ [{dsid}] {name} ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    try:
        # 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„°ë¡œ origin ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        meta_data = get_recent_etl_data(dsid, cnt)
        
        if not meta_data:
            print(f"      âš ï¸  ë©”íƒ€ë°ì´í„° ì—†ìŒ")
            return None
        
        # 2ë‹¨ê³„: unique origin ì¶”ì¶œ
        origins = set()
        for item in meta_data:
            if isinstance(item, dict) and 'origin' in item:
                origins.add(item['origin'])
        
        if not origins:
            print(f"      âš ï¸  origin ì—†ìŒ")
            return None
        
        print(f"      ğŸ“‹ {len(origins)}ê°œ origin ë°œê²¬, ë°ì´í„° ì¡°íšŒ ì¤‘...")
        
        # 3ë‹¨ê³„: ê° originì—ì„œ ì‹¤ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 10ê°œ)
        all_data = []
        max_origins = min(len(origins), 10)
        
        for i, origin in enumerate(list(origins)[:max_origins]):
            try:
                origin_data = get_etl_data_by_origin(dsid, origin)
                
                if origin_data:
                    # parsedData ì¶”ì¶œ
                    if isinstance(origin_data, list):
                        for item in origin_data:
                            if isinstance(item, dict) and 'parsedData' in item:
                                parsed = item['parsedData']
                                if isinstance(parsed, str):
                                    try:
                                        parsed = json.loads(parsed)
                                    except:
                                        continue
                                if isinstance(parsed, list):
                                    all_data.extend(parsed)
                                elif isinstance(parsed, dict):
                                    all_data.append(parsed)
                    elif isinstance(origin_data, dict) and 'parsedData' in origin_data:
                        parsed = origin_data['parsedData']
                        if isinstance(parsed, str):
                            try:
                                parsed = json.loads(parsed)
                            except:
                                continue
                        if isinstance(parsed, list):
                            all_data.extend(parsed)
                        elif isinstance(parsed, dict):
                            all_data.append(parsed)
            except Exception as e:
                pass
        
        if not all_data:
            print(f"      âš ï¸  parsedData ì—†ìŒ")
            return None
        
        df = pd.DataFrame(all_data)
        
        # BOM ë¬¸ì ì œê±° ë° ì»¬ëŸ¼ëª… ì •ê·œí™”
        df.columns = [col.replace('\ufeff', '').strip() for col in df.columns]
        
        # ì—°ë„/ì£¼ì°¨ ì»¬ëŸ¼ í™•ì¸ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        year_col = None
        week_col = None
        
        # ì—°ë„ ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if col in ['ì—°ë„', 'year', 'ë…„ë„']:
                year_col = col
                break
        
        # ì£¼ì°¨ ì»¬ëŸ¼ ì°¾ê¸°
        for col in df.columns:
            if col in ['ì£¼ì°¨', 'week', 'ì£¼']:
                week_col = col
                break
        
        # ìˆ˜ì§‘ ê¸°ê°„ì—ì„œ ì—°ë„/ì£¼ì°¨ ì¶”ì¶œ ì‹œë„
        if (year_col is None or week_col is None) and 'ìˆ˜ì§‘ ê¸°ê°„' in df.columns:
            # "2025ë…„ 5ì£¼" í˜•ì‹ì—ì„œ ì¶”ì¶œ
            import re
            def extract_year_week(val):
                if pd.isna(val):
                    return None, None
                match = re.match(r'(\d{4})ë…„\s*(\d+)ì£¼', str(val))
                if match:
                    return int(match.group(1)), int(match.group(2))
                return None, None
            
            extracted = df['ìˆ˜ì§‘ ê¸°ê°„'].apply(extract_year_week)
            df['year'] = extracted.apply(lambda x: x[0])
            df['week'] = extracted.apply(lambda x: x[1])
            df = df.dropna(subset=['year', 'week'])
            if len(df) > 0:
                df['year'] = df['year'].astype(int)
                df['week'] = df['week'].astype(int)
                print(f"      âœ… {len(df)}ê±´ ì¡°íšŒ ì™„ë£Œ (ìˆ˜ì§‘ ê¸°ê°„ì—ì„œ ì¶”ì¶œ)")
                return df
        
        if year_col and week_col:
            df['year'] = pd.to_numeric(df[year_col], errors='coerce').astype('Int64')
            df['week'] = pd.to_numeric(df[week_col], errors='coerce').astype('Int64')
            df = df.dropna(subset=['year', 'week'])
            if len(df) > 0:
                df['year'] = df['year'].astype(int)
                df['week'] = df['week'].astype(int)
                print(f"      âœ… {len(df)}ê±´ ì¡°íšŒ ì™„ë£Œ")
                return df
        
        print(f"      âš ï¸  ì—°ë„/ì£¼ì°¨ ì»¬ëŸ¼ ì—†ìŒ: {list(df.columns)[:8]}")
        return None
    
    except Exception as e:
        print(f"      âŒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


def load_merged_data(merged_path='merged_influenza_data.csv'):
    """
    merged_influenza_data.csv ë¡œë“œ
    """
    if not os.path.exists(merged_path):
        print(f"âŒ {merged_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    df = pd.read_csv(merged_path)
    print(f"\nğŸ“Š ë³‘í•© ë°ì´í„° ë¡œë“œ: {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
    return df


def compare_dataset(api_df, merged_df, dsid):
    """
    íŠ¹ì • ë°ì´í„°ì…‹ì˜ API ë°ì´í„°ì™€ ë³‘í•© ë°ì´í„° ë¹„êµ
    """
    mapping = DATASET_COLUMN_MAPPING.get(dsid, {})
    value_col = mapping.get('value_col')
    value_col_alt = mapping.get('value_col_alt', [])  # ëŒ€ì²´ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
    merged_col = mapping.get('merged_col')
    group_col = mapping.get('group_col')
    name = mapping.get('name', dsid)
    
    if not merged_col:
        return None
    
    result = {
        'dsid': dsid,
        'name': name,
        'api_rows': len(api_df),
        'matched': 0,
        'exact_match': 0,
        'mismatch': 0,
        'match_rate': 0.0
    }
    
    # API ë°ì´í„°ì—ì„œ value_col ì°¾ê¸° (ê¸°ë³¸ ì»¬ëŸ¼ëª… + ëŒ€ì²´ ì»¬ëŸ¼ëª…)
    actual_value_col = None
    candidates = [value_col] + value_col_alt if value_col else value_col_alt
    
    for col in candidates:
        if col and col in api_df.columns:
            actual_value_col = col
            break
    
    if not actual_value_col:
        # ì»¬ëŸ¼ëª…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì¶œë ¥
        print(f"      âš ï¸  [{dsid}] ê°’ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. API ì»¬ëŸ¼: {list(api_df.columns)}")
        return result
    
    # group_col ì°¾ê¸°
    if group_col not in api_df.columns:
        print(f"      âš ï¸  [{dsid}] ê·¸ë£¹ ì»¬ëŸ¼ '{group_col}'ì´ ì—†ìŒ. API ì»¬ëŸ¼: {list(api_df.columns)}")
        return result
    
    api_compare = api_df[['year', 'week', group_col, actual_value_col]].copy()
    api_compare = api_compare.rename(columns={
        group_col: 'age_group',
        actual_value_col: 'api_value'
    })
    api_compare['api_value'] = pd.to_numeric(api_compare['api_value'], errors='coerce')
    
    # ë³‘í•© ë°ì´í„° ì¤€ë¹„
    merged_compare = merged_df[['year', 'week', 'age_group', merged_col]].copy()
    merged_compare = merged_compare.rename(columns={merged_col: 'merged_value'})
    merged_compare['merged_value'] = pd.to_numeric(merged_compare['merged_value'], errors='coerce')
    
    # ì¡°ì¸
    comparison = api_compare.merge(
        merged_compare,
        on=['year', 'week', 'age_group'],
        how='inner'
    )
    
    result['matched'] = len(comparison)
    
    if len(comparison) > 0:
        # ì •í™• ì¼ì¹˜ í™•ì¸ (ì°¨ì´ 0.01 ì´í•˜)
        comparison['diff'] = abs(comparison['api_value'] - comparison['merged_value'])
        exact_match = (comparison['diff'] < 0.01) | (comparison['api_value'].isna() & comparison['merged_value'].isna())
        result['exact_match'] = exact_match.sum()
        result['mismatch'] = len(comparison) - result['exact_match']
        result['match_rate'] = result['exact_match'] / len(comparison) * 100
    
    return result


def main():
    print("="*70)
    print("ğŸ“Š API ë°ì´í„° vs merged_influenza_data.csv ë¹„êµ")
    print("="*70)
    
    # 1. ì¸ì¦ í™•ì¸
    print("\n[1] API ì¸ì¦ í™•ì¸")
    if not is_auth_configured():
        print("   âŒ GFID API ì¸ì¦ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— GFID_CLIENT_ID, GFID_CLIENT_SECRETë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return
    print("   âœ… API ì¸ì¦ ì„¤ì • ì™„ë£Œ")
    
    # 2. ë³‘í•© ë°ì´í„° ë¡œë“œ
    print("\n[2] ë³‘í•© ë°ì´í„° ë¡œë“œ")
    merged_df = load_merged_data('merged_influenza_data.csv')
    if merged_df is None:
        return
    
    print(f"   ì—°ë„ ë²”ìœ„: {merged_df['year'].min()} ~ {merged_df['year'].max()}")
    print(f"   ì£¼ì°¨ ë²”ìœ„: {merged_df['week'].min()} ~ {merged_df['week'].max()}")
    
    # 3. API ë°ì´í„° ì¡°íšŒ ë° ë¹„êµ
    print("\n[3] API ë°ì´í„° ì¡°íšŒ ë° ë¹„êµ")
    
    results = []
    api_data_dict = {}
    
    for dsid in ['ds_0101', 'ds_0105', 'ds_0106', 'ds_0109', 'ds_0110']:
        api_df = fetch_api_data(dsid, cnt=200)
        
        if api_df is not None and len(api_df) > 0:
            api_data_dict[dsid] = api_df
            result = compare_dataset(api_df, merged_df, dsid)
            if result:
                results.append(result)
    
    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“‹ ë¹„êµ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    
    if not results:
        print("\nâš ï¸  ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   APIê°€ parsedDataë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•„ ë©”íƒ€ë°ì´í„°ë§Œ ì¡°íšŒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n{'ë°ì´í„°ì…‹':<12} {'ì´ë¦„':<25} {'APIí–‰':<8} {'ë§¤ì¹­':<8} {'ì¼ì¹˜':<8} {'ë¶ˆì¼ì¹˜':<8} {'ì¼ì¹˜ìœ¨':<10}")
    print("-" * 85)
    
    total_matched = 0
    total_exact = 0
    
    for r in results:
        print(f"{r['dsid']:<12} {r['name']:<25} {r['api_rows']:<8} {r['matched']:<8} "
              f"{r['exact_match']:<8} {r['mismatch']:<8} {r['match_rate']:.1f}%")
        total_matched += r['matched']
        total_exact += r['exact_match']
    
    print("-" * 85)
    if total_matched > 0:
        overall_rate = total_exact / total_matched * 100
        print(f"{'í•©ê³„':<12} {'':<25} {'':<8} {total_matched:<8} {total_exact:<8} "
              f"{total_matched - total_exact:<8} {overall_rate:.1f}%")
    
    # 5. ìƒì„¸ ë¶„ì„ (ds_0101ë§Œ)
    if 'ds_0101' in api_data_dict:
        print("\n" + "="*70)
        print("ğŸ” ìƒì„¸ ë¶„ì„: ds_0101 (ILI ë°ì´í„°)")
        print("="*70)
        
        api_df = api_data_dict['ds_0101']
        
        # ì—°ë„/ì£¼ì°¨ ì»¤ë²„ë¦¬ì§€
        api_year_weeks = set(zip(api_df['year'], api_df['week']))
        merged_year_weeks = set(zip(merged_df['year'], merged_df['week']))
        
        common = api_year_weeks & merged_year_weeks
        api_only = api_year_weeks - merged_year_weeks
        merged_only = merged_year_weeks - api_year_weeks
        
        print(f"\n[ì—°ë„/ì£¼ì°¨ ì»¤ë²„ë¦¬ì§€]")
        print(f"   API ê³ ìœ  ì¡°í•©: {len(api_year_weeks)}ê°œ")
        print(f"   ë³‘í•© ê³ ìœ  ì¡°í•©: {len(merged_year_weeks)}ê°œ")
        print(f"   ê³µí†µ: {len(common)}ê°œ")
        
        if api_only:
            print(f"\n   APIì—ë§Œ ìˆëŠ” ì¡°í•© (ì²˜ìŒ 5ê°œ):")
            for yw in sorted(api_only)[:5]:
                print(f"      - {yw[0]}ë…„ {yw[1]}ì£¼")
        
        if merged_only:
            print(f"\n   ë³‘í•©ì—ë§Œ ìˆëŠ” ì¡°í•©: {len(merged_only)}ê°œ")
    
    print("\n" + "="*70)
    print("âœ… ë¹„êµ ì™„ë£Œ!")
    print("="*70)
    
    # ê²°ë¡ 
    if results:
        avg_rate = sum(r['match_rate'] for r in results) / len(results)
        if avg_rate >= 99:
            print("\nğŸ‰ ë°ì´í„°ê°€ ì •í™•í•˜ê²Œ ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        elif avg_rate >= 90:
            print("\nâœ… ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ ì •í™•í•˜ê²Œ ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâš ï¸  ì¼ë¶€ ë°ì´í„° ë¶ˆì¼ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ìƒì„¸ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
