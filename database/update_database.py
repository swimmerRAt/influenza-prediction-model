
"""
PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì—…ë°ì´íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. APIì—ì„œ ì¸í”Œë£¨ì—”ì ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ILI, ë°±ì‹ ë¥ , ì…ì›í™˜ì ë“±)
2. data/before í´ë”ì˜ ê³¼ê±° ë°ì´í„° ë¡œë”©
3. ëª¨ë“  ì¸í”Œë£¨ì—”ì ë°ì´í„° ë³‘í•© ë° PostgreSQL influenza DBì— ì €ì¥
4. APIì—ì„œ íŠ¸ë Œë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Google, Naver, Twitter)
5. íŠ¸ë Œë“œ ë°ì´í„° ë³‘í•© ë° PostgreSQL trends DBì— ì €ì¥

API í´ë¼ì´ì–¸íŠ¸: src_jaehong/api/ íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ êµ¬í˜„ (api_client.py)
"""

try:
    # ëª¨ë“ˆë¡œì„œ import ë  ë•Œ
    from .db_utils import merge_and_update_database, TimeSeriesDB
    from .api_client import (
        get_recent_etl_data,
        get_etl_data_by_date_range,
        get_etl_data_by_season,
        fetch_trend_data_from_api,
        fetch_all_influenza_data,
        INFLUENZA_DATASETS,
        is_auth_configured,
    )
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ë  ë•Œ
    from db_utils import merge_and_update_database, TimeSeriesDB
    from api_client import (
        get_recent_etl_data,
        get_etl_data_by_date_range,
        get_etl_data_by_season,
        fetch_trend_data_from_api,
        fetch_all_influenza_data,
        INFLUENZA_DATASETS,
        is_auth_configured,
    )

import os
import pandas as pd
import requests

from dotenv import load_dotenv
import warnings
from datetime import datetime

load_dotenv()
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# =========================
# íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë“¤
# =========================

def parse_date_to_year_week(date_str):
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ year, weekìœ¼ë¡œ ë³€í™˜
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "2024-01-15", "2024-W03", "2024-01-15T00:00:00.000Z")
    
    Returns:
        tuple: (year, week)
    """
    if date_str is None or pd.isna(date_str):
        return None, None
    
    date_str = str(date_str).strip()
    if not date_str:
        return None, None
    
    try:
        # ISO ì£¼ì°¨ í˜•ì‹ (ì˜ˆ: "2024-W03")
        if 'W' in date_str:
            parts = date_str.split('-W')
            return int(parts[0]), int(parts[1].split('-')[0])  # "2024-W03-1" í˜•ì‹ ì²˜ë¦¬
        
        # "ì—°ë„-ì£¼ì°¨" í˜•ì‹ (ì˜ˆ: "2024-03" ë˜ëŠ” "202403")
        if len(date_str) == 7 and '-' in date_str:
            parts = date_str.split('-')
            year, week = int(parts[0]), int(parts[1])
            if 1 <= week <= 53:
                return year, week
        
        if len(date_str) == 6 and date_str.isdigit():
            year = int(date_str[:4])
            week = int(date_str[4:])
            if 1 <= week <= 53:
                return year, week
        
        # ì¼ë°˜ ë‚ ì§œ/ì‹œê°„ í˜•ì‹ (ì˜ˆ: "2024-01-15", "2024-01-15T00:00:00.000Z")
        date_obj = pd.to_datetime(date_str)
        # ISO ì£¼ì°¨ ê³„ì‚° (ì›”ìš”ì¼ ì‹œì‘)
        year = date_obj.isocalendar()[0]
        week = date_obj.isocalendar()[1]
        return int(year), int(week)
    except Exception as e:
        return None, None


def extract_year_week_from_data(df, dsid):
    """
    DataFrameì—ì„œ year, week ì»¬ëŸ¼ ì¶”ì¶œ/ìƒì„±
    
    ë‹¤ì–‘í•œ ë‚ ì§œ ì»¬ëŸ¼ëª…ê³¼ í˜•ì‹ì„ ì§€ì›:
    - ì§ì ‘ì ì¸ year/week ì»¬ëŸ¼
    - ë‚ ì§œ ì»¬ëŸ¼ (date, datetime, collected_at, collectedAt ë“±)
    - í•œê¸€ ì»¬ëŸ¼ (ì—°ë„, ì£¼ì°¨, ë‚ ì§œ, ìˆ˜ì§‘ì¼ ë“±)
    
    Args:
        df: ì›ë³¸ DataFrame
        dsid: ë°ì´í„°ì…‹ ID (ë””ë²„ê¹…ìš©)
    
    Returns:
        DataFrame: year, week ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    print(f"   ğŸ” [{dsid}] year/week ì»¬ëŸ¼ ì¶”ì¶œ ì‹œì‘...")
    print(f"   ğŸ“‹ [{dsid}] ì›ë³¸ ì»¬ëŸ¼: {list(df.columns)}")
    
    # 1. ì´ë¯¸ year/week ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
    if 'year' in df.columns and 'week' in df.columns:
        print(f"   âœ… [{dsid}] year/week ì»¬ëŸ¼ ì´ë¯¸ ì¡´ì¬")
        return df
    
    # 1-1. í•œê¸€ ì»¬ëŸ¼ì¸ ê²½ìš°
    if 'ì—°ë„' in df.columns and 'ì£¼ì°¨' in df.columns:
        df['year'] = df['ì—°ë„'].astype(int)
        df['week'] = df['ì£¼ì°¨'].astype(int)
        print(f"   âœ… [{dsid}] ì—°ë„/ì£¼ì°¨ â†’ year/week ë³€í™˜ ì™„ë£Œ")
        return df
    
    # 2. ë‚ ì§œ ì»¬ëŸ¼ í›„ë³´ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)
    date_col_candidates = [
        # API ë©”íƒ€ë°ì´í„° í•„ë“œ
        'collectedAt', 'collected_at', 'createdAt', 'created_at',
        'updatedAt', 'updated_at',
        # ì¼ë°˜ ë‚ ì§œ í•„ë“œ
        'date', 'datetime', 'time', 'timestamp',
        # ì£¼ì°¨ ê´€ë ¨ í•„ë“œ
        'year_week', 'yearWeek', 'week_date', 'weekDate',
        # í•œê¸€ í•„ë“œ
        'ë‚ ì§œ', 'ìˆ˜ì§‘ì¼', 'ê¸°ì¤€ì¼', 'ì¡°íšŒì¼', 'ìˆ˜ì§‘ì‹œê°„',
    ]
    
    # 3. ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
    date_col = None
    for candidate in date_col_candidates:
        if candidate in df.columns:
            date_col = candidate
            break
        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ì—¬ ì°¾ê¸°
        for col in df.columns:
            if col.lower() == candidate.lower():
                date_col = col
                break
        if date_col:
            break
    
    if date_col:
        print(f"   ğŸ” [{dsid}] ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: '{date_col}'")
        print(f"   ğŸ“‹ [{dsid}] ìƒ˜í”Œ ê°’: {df[date_col].head(3).tolist()}")
        
        # year, week ì»¬ëŸ¼ ìƒì„±
        year_week_data = df[date_col].apply(parse_date_to_year_week)
        df['year'] = year_week_data.apply(lambda x: x[0])
        df['week'] = year_week_data.apply(lambda x: x[1])
        
        # None ê°’ í™•ì¸
        null_count = df['year'].isna().sum()
        if null_count > 0:
            print(f"   âš ï¸  [{dsid}] {null_count}ê°œ í–‰ì˜ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ (ì œê±°ë¨)")
            df = df.dropna(subset=['year', 'week'])
        
        if len(df) > 0:
            df['year'] = df['year'].astype(int)
            df['week'] = df['week'].astype(int)
            print(f"   âœ… [{dsid}] year/week ë³€í™˜ ì™„ë£Œ: {len(df)}ê±´")
            print(f"   ğŸ“Š [{dsid}] year ë²”ìœ„: {df['year'].min()}-{df['year'].max()}, week ë²”ìœ„: {df['week'].min()}-{df['week'].max()}")
        else:
            print(f"   âŒ [{dsid}] ë³€í™˜ëœ ë°ì´í„° ì—†ìŒ")
        
        return df
    
    # 4. ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° - ë‹¤ë¥¸ ë°©ë²• ì‹œë„
    print(f"   âš ï¸  [{dsid}] ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ë‹¤ë¥¸ ë°©ë²• ì‹œë„...")
    
    # 4-1. yearë§Œ ìˆê³  week ì»¬ëŸ¼ í›„ë³´ ì°¾ê¸°
    if 'year' in df.columns:
        week_candidates = ['week', 'week_num', 'weekNum', 'ì£¼ì°¨', 'wk']
        for candidate in week_candidates:
            if candidate in df.columns:
                df['week'] = df[candidate].astype(int)
                print(f"   âœ… [{dsid}] year + {candidate} â†’ week ì‚¬ìš©")
                return df
    
    # 4-2. ì—°ë„ë§Œ ìˆëŠ” ê²½ìš°
    if 'ì—°ë„' in df.columns:
        df['year'] = df['ì—°ë„'].astype(int)
        week_candidates = ['week', 'week_num', 'weekNum', 'ì£¼ì°¨', 'wk']
        for candidate in week_candidates:
            if candidate in df.columns:
                df['week'] = df[candidate].astype(int)
                print(f"   âœ… [{dsid}] ì—°ë„ â†’ year, {candidate} â†’ week ì‚¬ìš©")
                return df
    
    print(f"   âŒ [{dsid}] year/week ì»¬ëŸ¼ ìƒì„± ì‹¤íŒ¨. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
    return df


def flatten_parsed_data(data_list):
    """
    API ì‘ë‹µì˜ parsedData í•„ë“œë¥¼ í”Œë˜íŠ¼í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
    
    API ì‘ë‹µ êµ¬ì¡° ì˜ˆì‹œ:
    [
        {
            "dsId": "ds_0701",
            "origin": "uuid",
            "contentType": "application/json",
            "parsedData": {"ê²€ìƒ‰ì–´1": 100, "ê²€ìƒ‰ì–´2": 50, ...},
            "collectedAt": "2024-01-15T00:00:00.000Z"
        },
        ...
    ]
    
    Args:
        data_list: APIì—ì„œ ë°›ì€ raw ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        list: í”Œë˜íŠ¼ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    flattened = []
    
    for item in data_list:
        if isinstance(item, dict):
            # parsedData í•„ë“œê°€ ìˆëŠ” ê²½ìš°
            if 'parsedData' in item and isinstance(item['parsedData'], dict):
                flat_item = item['parsedData'].copy()
                # collectedAt ë“± ë©”íƒ€ë°ì´í„° ì¶”ê°€
                if 'collectedAt' in item:
                    flat_item['collectedAt'] = item['collectedAt']
                if 'dsId' in item:
                    flat_item['dsId'] = item['dsId']
                flattened.append(flat_item)
            else:
                # parsedDataê°€ ì—†ìœ¼ë©´ item ê·¸ëŒ€ë¡œ ì‚¬ìš©
                flattened.append(item)
        else:
            flattened.append(item)
    
    return flattened


def fetch_trend_data(dsid="ds_0701", cnt=100):
    """
    GFID APIì—ì„œ íŠ¸ë Œë“œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° year/week ë³€í™˜
    (src_jaehong/api/etlDataApi.js íŒ¨í„´ ì‚¬ìš©)
    
    1ë‹¨ê³„: recent APIë¡œ origin ëª©ë¡ ì¡°íšŒ
    2ë‹¨ê³„: ê° unique originì— ëŒ€í•´ ì‹¤ì œ ë°ì´í„°(parsedData) ì¡°íšŒ
    3ë‹¨ê³„: ëª¨ë“  ë°ì´í„° ë³‘í•© ë° year/week ë³€í™˜
    
    Args:
        dsid: ë°ì´í„°ì…‹ ID (ds_0701=Google, ds_0801=Naver, ds_0901=Twitter)
        cnt: ìµœê·¼ ë°ì´í„° ê±´ìˆ˜ (origin ì¡°íšŒìš©)
    
    Returns:
        DataFrame: year, week ì»¬ëŸ¼ì´ ì¶”ê°€ëœ íŠ¸ë Œë“œ ë°ì´í„°
    """
    try:
        from .api_client import get_etl_data_by_origin
    except ImportError:
        from api_client import get_etl_data_by_origin
    
    dsid_names = {
        'ds_0701': 'Google Trends',
        'ds_0801': 'Naver Trends', 
        'ds_0901': 'Twitter Trends'
    }
    dsid_name = dsid_names.get(dsid, dsid)
    
    print(f"\nğŸ“¡ GFID APIì—ì„œ {dsid} ({dsid_name}) ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    
    try:
        # 1ë‹¨ê³„: recent APIë¡œ ë©”íƒ€ë°ì´í„°(origin ëª©ë¡) ì¡°íšŒ
        print(f"   [1/3] ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘...")
        meta_data = get_recent_etl_data(dsid, cnt)
        
        if not meta_data:
            print(f"   âš ï¸  [{dsid}] ë©”íƒ€ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame()
        
        print(f"   âœ… [{dsid}] ë©”íƒ€ë°ì´í„° ìˆ˜ì‹ : {len(meta_data)}ê±´")
        
        # 2ë‹¨ê³„: unique origin ëª©ë¡ ì¶”ì¶œ
        origins = set()
        origin_collected_at = {}  # origin -> collectedAt ë§¤í•‘
        
        for item in meta_data:
            if isinstance(item, dict) and 'origin' in item:
                origin = item['origin']
                origins.add(origin)
                if 'collectedAt' in item:
                    origin_collected_at[origin] = item['collectedAt']
        
        origins = list(origins)
        print(f"   [2/3] ê³ ìœ  origin ê°œìˆ˜: {len(origins)}ê°œ")
        
        if not origins:
            print(f"   âš ï¸  [{dsid}] originì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # 3ë‹¨ê³„: ê° originì—ì„œ ì‹¤ì œ ë°ì´í„°(parsedData) ì¡°íšŒ
        print(f"   [3/3] ê° originì—ì„œ ì‹¤ì œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
        all_data = []
        success_count = 0
        
        # ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ ì œí•œ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
        max_origins = min(len(origins), 50)
        
        for i, origin in enumerate(origins[:max_origins]):
            try:
                origin_data = get_etl_data_by_origin(dsid, origin)
                
                if origin_data:
                    # parsedData ì¶”ì¶œ
                    if isinstance(origin_data, list):
                        for item in origin_data:
                            if isinstance(item, dict):
                                parsed = item.get('parsedData', {})
                                if parsed and isinstance(parsed, dict):
                                    # collectedAt ì¶”ê°€
                                    if 'collectedAt' in item:
                                        parsed['collectedAt'] = item['collectedAt']
                                    elif origin in origin_collected_at:
                                        parsed['collectedAt'] = origin_collected_at[origin]
                                    all_data.append(parsed)
                                    success_count += 1
                    elif isinstance(origin_data, dict):
                        parsed = origin_data.get('parsedData', {})
                        if parsed and isinstance(parsed, dict):
                            if 'collectedAt' in origin_data:
                                parsed['collectedAt'] = origin_data['collectedAt']
                            elif origin in origin_collected_at:
                                parsed['collectedAt'] = origin_collected_at[origin]
                            all_data.append(parsed)
                            success_count += 1
                            
            except Exception as e:
                # ê°œë³„ origin ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†
                pass
            
            # ì§„í–‰ë¥  í‘œì‹œ (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0:
                print(f"      ì§„í–‰: {i + 1}/{max_origins} origins ì²˜ë¦¬ë¨")
        
        print(f"   âœ… [{dsid}] ì‹¤ì œ ë°ì´í„° {len(all_data)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        
        if not all_data:
            print(f"   âš ï¸  [{dsid}] parsedDataê°€ ë¹„ì–´ ìˆìŒ. ë©”íƒ€ë°ì´í„°ë¡œ ëŒ€ì²´...")
            # ì‹¤íŒ¨ ì‹œ ë©”íƒ€ë°ì´í„°ë¼ë„ ì‚¬ìš©
            df = pd.DataFrame(meta_data)
        else:
            df = pd.DataFrame(all_data)
        
        if df.empty:
            print(f"   âš ï¸  [{dsid}] DataFrameì´ ë¹„ì–´ ìˆìŒ")
            return pd.DataFrame()
        
        print(f"   ğŸ“‹ [{dsid}] DataFrame ì»¬ëŸ¼: {list(df.columns)[:10]}...")
        print(f"   ğŸ“‹ [{dsid}] DataFrame í¬ê¸°: {df.shape}")
        
        # year, week ì»¬ëŸ¼ ì¶”ì¶œ/ìƒì„±
        df = extract_year_week_from_data(df, dsid)
        
        # year/week ì»¬ëŸ¼ ê²€ì¦
        if 'year' not in df.columns or 'week' not in df.columns:
            print(f"   âŒ [{dsid}] year/week ì»¬ëŸ¼ ìƒì„± ì‹¤íŒ¨")
            return pd.DataFrame()
        
        # ê²°ê³¼ ìš”ì•½
        if len(df) > 0:
            print(f"   âœ… [{dsid}] ìµœì¢… ë°ì´í„°: {len(df)}ê±´")
            print(f"   ğŸ“Š [{dsid}] year ë²”ìœ„: {df['year'].min()}-{df['year'].max()}")
            print(f"   ğŸ“Š [{dsid}] week ë²”ìœ„: {df['week'].min()}-{df['week'].max()}")
            # ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ í™•ì¸ (year, week, collectedAt ì œì™¸)
            data_cols = [c for c in df.columns if c not in ['year', 'week', 'collectedAt', 'dsId', 'origin', 'id', 'contentType']]
            if data_cols:
                print(f"   ğŸ“Š [{dsid}] ë°ì´í„° ì»¬ëŸ¼: {data_cols[:5]}{'...' if len(data_cols) > 5 else ''}")
        
        return df
    
    except Exception as e:
        print(f"   âŒ [{dsid}] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()


def merge_trend_data(google_df, naver_df, twitter_df):
    """
    3ê°œ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë³‘í•©
    
    Args:
        google_df: Google Trends ë°ì´í„°
        naver_df: Naver Trends ë°ì´í„°
        twitter_df: Twitter Trends ë°ì´í„°
    
    Returns:
        DataFrame: ë³‘í•©ëœ íŠ¸ë Œë“œ ë°ì´í„°
    """
    print("\nğŸ”— íŠ¸ë Œë“œ ë°ì´í„° ë³‘í•© ì¤‘...")
    
    # ê¸°ë³¸ ì‹œê°„ ì»¬ëŸ¼ (year, week) í™•ì¸
    all_dfs = []
    
    if not google_df.empty:
        # Google Trends ì»¬ëŸ¼ëª… ì •ê·œí™”
        google_df = google_df.rename(columns=lambda x: f"google_{x}" if x not in ['year', 'week', 'ì—°ë„', 'ì£¼ì°¨'] else x)
        all_dfs.append(google_df)
        print(f"   â€¢ Google Trends: {len(google_df)}ê±´, ì»¬ëŸ¼: {list(google_df.columns)[:5]}...")
    
    if not naver_df.empty:
        # Naver Trends ì»¬ëŸ¼ëª… ì •ê·œí™”
        naver_df = naver_df.rename(columns=lambda x: f"naver_{x}" if x not in ['year', 'week', 'ì—°ë„', 'ì£¼ì°¨'] else x)
        all_dfs.append(naver_df)
        print(f"   â€¢ Naver Trends: {len(naver_df)}ê±´, ì»¬ëŸ¼: {list(naver_df.columns)[:5]}...")
    
    if not twitter_df.empty:
        # Twitter Trends ì»¬ëŸ¼ëª… ì •ê·œí™”
        twitter_df = twitter_df.rename(columns=lambda x: f"twitter_{x}" if x not in ['year', 'week', 'ì—°ë„', 'ì£¼ì°¨'] else x)
        all_dfs.append(twitter_df)
        print(f"   â€¢ Twitter Trends: {len(twitter_df)}ê±´, ì»¬ëŸ¼: {list(twitter_df.columns)[:5]}...")
    
    if not all_dfs:
        print("   âš ï¸  ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # year, week ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    merged = all_dfs[0]
    for df in all_dfs[1:]:
        # í•œê¸€/ì˜ë¬¸ ì»¬ëŸ¼ëª… í†µì¼
        df = df.rename(columns={'ì—°ë„': 'year', 'ì£¼ì°¨': 'week'})
        merged = merged.rename(columns={'ì—°ë„': 'year', 'ì£¼ì°¨': 'week'})
        merged = merged.merge(df, on=['year', 'week'], how='outer')
    
    # ì •ë ¬
    if 'year' in merged.columns and 'week' in merged.columns:
        merged = merged.sort_values(['year', 'week']).reset_index(drop=True)
    
    print(f"   âœ… ë³‘í•© ì™„ë£Œ: {len(merged)}ê±´, {len(merged.columns)}ê°œ ì»¬ëŸ¼")
    return merged


def create_trends_table(db: TimeSeriesDB, table_name="trends_data"):
    """
    íŠ¸ë Œë“œ ë°ì´í„° í…Œì´ë¸” ìƒì„±
    
    Args:
        db: TimeSeriesDB ì¸ìŠ¤í„´ìŠ¤
        table_name: í…Œì´ë¸” ì´ë¦„
    """
    db.connect()
    
    # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
    with db.conn.cursor() as cur:
        cur.execute(f"DROP TABLE IF EXISTS {table_name}")
        db.conn.commit()
        print(f"   âœ… ê¸°ì¡´ {table_name} í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")


def insert_trends_data(db: TimeSeriesDB, df: pd.DataFrame, table_name="trends_data"):
    """
    íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ PostgreSQLì— ì‚½ì…
    
    Args:
        db: TimeSeriesDB ì¸ìŠ¤í„´ìŠ¤
        df: íŠ¸ë Œë“œ ë°ì´í„° DataFrame
        table_name: í…Œì´ë¸” ì´ë¦„
    """
    if df.empty:
        print("   âš ï¸  ì‚½ì…í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    db.connect()
    
    # ì»¬ëŸ¼ëª… ì •ê·œí™” (í•œê¸€ â†’ ì˜ë¬¸)
    df = df.rename(columns={'ì—°ë„': 'year', 'ì£¼ì°¨': 'week'})
    
    # í…Œì´ë¸” ìƒì„± (ìë™ìœ¼ë¡œ ì»¬ëŸ¼ íƒ€ì… ì¶”ë¡ )
    with db.conn.cursor() as cur:
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cur.execute(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_name = '{table_name}'
            )
        """)
        table_exists = cur.fetchone()[0]
        
        if not table_exists:
            # ë™ì ìœ¼ë¡œ CREATE TABLE ìƒì„±
            columns_def = []
            for col in df.columns:
                dtype = df[col].dtype
                if dtype in ['int64', 'int32']:
                    sql_type = 'INTEGER'
                elif dtype in ['float64', 'float32']:
                    sql_type = 'REAL'
                else:
                    sql_type = 'TEXT'
                columns_def.append(f'"{col}" {sql_type}')
            
            create_sql = f"CREATE TABLE {table_name} ({', '.join(columns_def)})"
            cur.execute(create_sql)
            db.conn.commit()
            print(f"   âœ… {table_name} í…Œì´ë¸” ìƒì„± ì™„ë£Œ ({len(columns_def)}ê°œ ì»¬ëŸ¼)")
    
    # ë°ì´í„° ì‚½ì…
    df = df.where(pd.notnull(df), None)
    columns = list(df.columns)
    values = df.values.tolist()
    placeholders = ','.join(['%s'] * len(columns))
    col_names = ','.join([f'"{col}"' for col in columns])
    sql = f'INSERT INTO {table_name} ({col_names}) VALUES ({placeholders})'
    
    with db.conn.cursor() as cur:
        import psycopg2.extras
        psycopg2.extras.execute_batch(cur, sql, values)
    db.conn.commit()
    
    print(f"   âœ… {table_name}ì— {len(df)}ê±´ ì‚½ì… ì™„ë£Œ")


def update_trends_database(
    db_name="trends",
    table_name="trends_data",
    cnt=500
):
    """
    íŠ¸ë Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤
    (GFID API ì§ì ‘ í˜¸ì¶œ - src_jaehong íŒ¨í„´)
    
    Args:
        db_name: PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        table_name: í…Œì´ë¸” ì´ë¦„
        cnt: ê° ë°ì´í„°ì…‹ë‹¹ ìµœê·¼ ê±´ìˆ˜
    """
    print("\n" + "="*60)
    print("ğŸ“Š íŠ¸ë Œë“œ ë°ì´í„° PostgreSQL ì—…ë°ì´íŠ¸ (GFID API ì§ì ‘ í˜¸ì¶œ)")
    print("="*60)
    
    # ì¸ì¦ ì„¤ì • í™•ì¸
    if not is_auth_configured():
        print("\nâš ï¸  GFID API ì¸ì¦ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
        print("   - GFID_CLIENT_ID")
        print("   - GFID_CLIENT_SECRET")
        return False
    
    # 1. GFID APIì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    google_df = fetch_trend_data("ds_0701", cnt)
    naver_df = fetch_trend_data("ds_0801", cnt)
    twitter_df = fetch_trend_data("ds_0901", cnt)
    
    # 2. ë°ì´í„° ë³‘í•©
    merged_df = merge_trend_data(google_df, naver_df, twitter_df)
    
    if merged_df.empty:
        print("\nâŒ íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì—†ì–´ ì—…ë°ì´íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False
    
    # 3. PostgreSQL ì—°ê²° (trends ë°ì´í„°ë² ì´ìŠ¤)
    print(f"\nğŸ’¾ PostgreSQL '{db_name}' ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
    db = TimeSeriesDB(dbname=db_name)
    
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
        db.connect()
        print(f"   âœ… '{db_name}' ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
    except Exception as e:
        print(f"   âš ï¸  '{db_name}' ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„± ì¤‘...")
        # postgres ê¸°ë³¸ DBì— ì—°ê²°í•˜ì—¬ trends DB ìƒì„±
        temp_db = TimeSeriesDB(dbname='postgres')
        temp_db.connect()
        temp_db.conn.autocommit = True
        with temp_db.conn.cursor() as cur:
            cur.execute(f"CREATE DATABASE {db_name}")
        temp_db.close()
        print(f"   âœ… '{db_name}' ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ë‹¤ì‹œ ì—°ê²°
        db = TimeSeriesDB(dbname=db_name)
        db.connect()
    
    # 4. í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì‚½ì…
    create_trends_table(db, table_name)
    insert_trends_data(db, merged_df, table_name)
    
    # 5. CSV ë°±ì—…
    csv_path = "trends_data.csv"
    merged_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\n   âœ… CSV ë°±ì—… ì™„ë£Œ: {csv_path}")
    
    db.close()
    
    print("\n" + "="*60)
    print("âœ… íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    print("="*60)
    print(f"\níŠ¸ë Œë“œ ë°ì´í„°:")
    print(f"  â€¢ ë°ì´í„°ë² ì´ìŠ¤: {db_name}")
    print(f"  â€¢ í…Œì´ë¸”: {table_name}")
    print(f"  â€¢ ë°ì´í„° ê±´ìˆ˜: {len(merged_df)}")
    print(f"  â€¢ ì»¬ëŸ¼ ìˆ˜: {len(merged_df.columns)}")
    print(f"  â€¢ ë°±ì—… íŒŒì¼: {csv_path}")
    
    return True


# =========================
# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# =========================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ“Š PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì—…ë°ì´íŠ¸")
    print("="*60)
    
    # í™˜ê²½ í™•ì¸
    print("\ní™˜ê²½ í™•ì¸:")
    print(f"  â€¢ GFID API ì¸ì¦ ì„¤ì •: {'âœ… ì™„ë£Œ' if is_auth_configured() else 'âŒ ë¯¸ì™„ë£Œ'}")
    print(f"  â€¢ data/before í´ë” ì¡´ì¬: {os.path.exists('data/before')}")
    print(f"  â€¢ merged_influenza_data.csv ì¡´ì¬: {os.path.exists('merged_influenza_data.csv')}")
    
    # GFID API ì¸ì¦ ì„¤ì • ì•ˆë‚´
    if not is_auth_configured():
        print("\nâš ï¸  GFID API ì¸ì¦ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— ë‹¤ìŒì„ ì„¤ì •í•˜ì„¸ìš”:")
        print("   GFID_CLIENT_ID=your_client_id")
        print("   GFID_CLIENT_SECRET=your_client_secret")
    
    # ì‚¬ìš©ì í™•ì¸
    print("\në‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:")
    print("\n[1ë‹¨ê³„] ì¸í”Œë£¨ì—”ì ë°ì´í„° ì—…ë°ì´íŠ¸ (influenza DB)")
    print("  1-1. APIì—ì„œ ì¸í”Œë£¨ì—”ì ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
    print("  1-2. data/before í´ë”ì˜ ê³¼ê±° ë°ì´í„° ë¡œë”©")
    print("  1-3. ëª¨ë“  ë°ì´í„° ë³‘í•© ë° ì¤‘ë³µ ì œê±°")
    print("  1-4. PostgreSQL influenza DBì— ì €ì¥")
    print("  1-5. CSVë¡œ ë°±ì—… (merged_influenza_data.csv)")
    
    # íŠ¸ë Œë“œ ë°ì´í„°ëŠ” í˜„ì¬ ë¹„í™œì„±í™” (APIê°€ parsedDataë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ)
    # ë‚˜ì¤‘ì— APIê°€ ìˆ˜ì •ë˜ë©´ ë‹¤ì‹œ í™œì„±í™” ê°€ëŠ¥
    print("\n[2ë‹¨ê³„] íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ (í˜„ì¬ ë¹„í™œì„±í™”)")
    print("  âš ï¸  íŠ¸ë Œë“œ APIê°€ ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆê°€")
    print("  âš ï¸  API ìˆ˜ì • í›„ ë‹¤ì‹œ í™œì„±í™” ì˜ˆì •")
    
    response = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
    
    if response == 'y':
        success_count = 0
        total_steps = 1  # í˜„ì¬ëŠ” ì¸í”Œë£¨ì—”ì ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸
        
        # 1ë‹¨ê³„: ì¸í”Œë£¨ì—”ì ë°ì´í„° ì—…ë°ì´íŠ¸
        print("\n" + "="*60)
        print("1ë‹¨ê³„: ì¸í”Œë£¨ì—”ì ë°ì´í„° ì—…ë°ì´íŠ¸")
        print("="*60)
        try:
            merge_and_update_database(
                table_name="influenza_data",
                fetch_latest=True,  # APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                before_dir='data/before',
                consolidate=True  # ìˆ˜ì •ëœ ë³‘í•© ë¡œì§ ì‚¬ìš©
            )
            success_count += 1
        except Exception as e:
            print(f"\nâŒ ì¸í”Œë£¨ì—”ì ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        # 2ë‹¨ê³„: íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ (í˜„ì¬ ë¹„í™œì„±í™”)
        # APIê°€ parsedDataë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•„ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆê°€
        # ë‚˜ì¤‘ì— APIê°€ ìˆ˜ì •ë˜ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ í™œì„±í™”
        print("\n" + "="*60)
        print("2ë‹¨ê³„: íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ (ê±´ë„ˆëœ€)")
        print("="*60)
        print("\nâš ï¸  íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("   í˜„ì¬ APIê°€ ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   API ìˆ˜ì • í›„ update_trends_database() í•¨ìˆ˜ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
        
        # === íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ (ë¹„í™œì„±í™”ë¨) ===
        # if is_auth_configured():
        #     print("\nğŸ“¡ GFID API ì§ì ‘ í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
        #     try:
        #         if update_trends_database():
        #             success_count += 1
        #     except Exception as e:
        #         print(f"\nâš ï¸  íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        #         import traceback
        #         traceback.print_exc()
        
        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*60)
        print("ğŸ“Š ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        print("="*60)
        print(f"\nì—…ë°ì´íŠ¸ ê²°ê³¼: {success_count}/{total_steps} ì„±ê³µ")
        
        if success_count >= 1:
            print("\nâœ… ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print("\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("  python patchTST.py")
            print("\nìƒì„±ëœ íŒŒì¼:")
            if os.path.exists('merged_influenza_data.csv'):
                print("  âœ“ merged_influenza_data.csv (ì¸í”Œë£¨ì—”ì ë°ì´í„°)")
            if os.path.exists('trends_data.csv'):
                print("  âœ“ trends_data.csv (íŠ¸ë Œë“œ ë°ì´í„°)")
        else:
            print("\nâŒ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            print("   ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    else:
        print("\nì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
