import math
from pathlib import Path
from typing import List, Tuple, Optional
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
import json
import os
from dotenv import load_dotenv
import warnings

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

# SSL ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# =========================
# ë°ì´í„°ì…‹ ID ë¦¬ìŠ¤íŠ¸ ì •ì˜
# =========================
DATASET_IDS = [
    'ds_0101', 'ds_0102', 'ds_0103', 'ds_0104', 'ds_0105', 'ds_0106', 'ds_0107', 'ds_0108', 'ds_0109', 'ds_0110',
    'ds_0701', 'ds_0801', 'ds_0901'
]

# =========================
# ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ Rate Limiter
# =========================
class AdaptiveRateLimiter:
    """
    ì„œë²„ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ìë™ìœ¼ë¡œ ìš”ì²­ ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í´ë˜ìŠ¤
    
    Features:
    - ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ìë™ ë”œë ˆì´ ì¡°ì ˆ
    - Exponential backoff ì¬ì‹œë„ ë¡œì§
    - ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„°ë§
    - ì„œë²„ ìƒíƒœ ê¸°ë°˜ adaptive throttling
    """
    
    def __init__(self, 
                 initial_delay=1.0,      # ì´ˆê¸° ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
                 max_delay=30.0,         # ìµœëŒ€ ë”œë ˆì´ (ì´ˆ)
                 min_delay=0.5,          # ìµœì†Œ ë”œë ˆì´ (ì´ˆ)
                 backoff_factor=2.0,     # ë°±ì˜¤í”„ ì¦ê°€ìœ¨
                 max_retries=5,          # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
                 slow_threshold=5.0,     # ëŠë¦° ì‘ë‹µ íŒë‹¨ ê¸°ì¤€ (ì´ˆ)
                 error_threshold=0.3):   # ì—ëŸ¬ìœ¨ ì„ê³„ê°’ (30%)
        
        self.current_delay = initial_delay
        self.initial_delay = initial_delay
        self.max_delay = max_delay
        self.min_delay = min_delay
        self.backoff_factor = backoff_factor
        self.max_retries = max_retries
        self.slow_threshold = slow_threshold
        self.error_threshold = error_threshold
        
        # í†µê³„ ì •ë³´
        self.request_count = 0
        self.error_count = 0
        self.total_response_time = 0.0
        self.recent_response_times = []  # ìµœê·¼ 10ê°œ ì‘ë‹µ ì‹œê°„
        self.consecutive_errors = 0
        
    def get_stats(self):
        """í˜„ì¬ í†µê³„ ì •ë³´ ë°˜í™˜"""
        avg_response_time = (self.total_response_time / self.request_count 
                            if self.request_count > 0 else 0)
        error_rate = (self.error_count / self.request_count 
                     if self.request_count > 0 else 0)
        
        return {
            'request_count': self.request_count,
            'error_count': self.error_count,
            'error_rate': error_rate,
            'avg_response_time': avg_response_time,
            'current_delay': self.current_delay,
            'consecutive_errors': self.consecutive_errors
        }
    
    def print_stats(self):
        """í†µê³„ ì •ë³´ ì¶œë ¥"""
        stats = self.get_stats()
        print(f"\nğŸ“Š [Rate Limiter í†µê³„]")
        print(f"   ì´ ìš”ì²­: {stats['request_count']}")
        print(f"   ì—ëŸ¬ ë°œìƒ: {stats['error_count']}")
        print(f"   ì—ëŸ¬ìœ¨: {stats['error_rate']:.1%}")
        print(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ")
        print(f"   í˜„ì¬ ë”œë ˆì´: {stats['current_delay']:.2f}ì´ˆ")
        print(f"   ì—°ì† ì—ëŸ¬: {stats['consecutive_errors']}íšŒ")
    
    def record_success(self, response_time):
        """ì„±ê³µí•œ ìš”ì²­ ê¸°ë¡ ë° ë”œë ˆì´ ê°ì†Œ"""
        self.request_count += 1
        self.total_response_time += response_time
        self.recent_response_times.append(response_time)
        
        # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
        if len(self.recent_response_times) > 10:
            self.recent_response_times.pop(0)
        
        # ì—°ì† ì—ëŸ¬ ì¹´ìš´í„° ë¦¬ì…‹
        self.consecutive_errors = 0
        
        # ì‘ë‹µì´ ë¹ ë¥´ë©´ ë”œë ˆì´ ê°ì†Œ (ì ì§„ì  íšŒë³µ)
        if response_time < self.slow_threshold * 0.5:
            self.current_delay = max(self.min_delay, self.current_delay * 0.9)
            print(f"   âš¡ ë¹ ë¥¸ ì‘ë‹µ ê°ì§€ â†’ ë”œë ˆì´ ê°ì†Œ: {self.current_delay:.2f}ì´ˆ")
        # ì‘ë‹µì´ ëŠë¦¬ë©´ ë”œë ˆì´ ì¦ê°€
        elif response_time > self.slow_threshold:
            old_delay = self.current_delay
            self.current_delay = min(self.max_delay, self.current_delay * 1.2)
            print(f"   ğŸ¢ ëŠë¦° ì‘ë‹µ ê°ì§€ ({response_time:.2f}ì´ˆ) â†’ ë”œë ˆì´ ì¦ê°€: {old_delay:.2f}ì´ˆ â†’ {self.current_delay:.2f}ì´ˆ")
    
    def record_error(self, error_type="unknown"):
        """ì—ëŸ¬ ë°œìƒ ê¸°ë¡ ë° ë”œë ˆì´ ì¦ê°€"""
        self.request_count += 1
        self.error_count += 1
        self.consecutive_errors += 1
        
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë”œë ˆì´ ì¦ê°€ (exponential backoff)
        old_delay = self.current_delay
        self.current_delay = min(self.max_delay, 
                                self.current_delay * self.backoff_factor)
        
        print(f"   âš ï¸ ì—ëŸ¬ ë°œìƒ ({error_type}) â†’ ë”œë ˆì´ ì¦ê°€: {old_delay:.2f}ì´ˆ â†’ {self.current_delay:.2f}ì´ˆ")
        
        # ì—ëŸ¬ìœ¨ì´ ë†’ìœ¼ë©´ ê²½ê³ 
        error_rate = self.error_count / self.request_count
        if error_rate > self.error_threshold:
            print(f"   ğŸš¨ ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€: {error_rate:.1%} (ì„ê³„ê°’: {self.error_threshold:.1%})")
    
    def wait(self):
        """ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°"""
        if self.request_count > 0:  # ì²« ìš”ì²­ì€ ëŒ€ê¸° ì•ˆ í•¨
            print(f"   â³ ì„œë²„ ë³´í˜¸ë¥¼ ìœ„í•´ {self.current_delay:.2f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(self.current_delay)
    
    def execute_with_retry(self, func, *args, **kwargs):
        """
        ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ í•¨ìˆ˜ ì‹¤í–‰
        
        Parameters:
        -----------
        func : callable
            ì‹¤í–‰í•  í•¨ìˆ˜
        *args, **kwargs
            í•¨ìˆ˜ì— ì „ë‹¬í•  ì¸ì
        
        Returns:
        --------
        í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼
        """
        for attempt in range(self.max_retries):
            try:
                # ìš”ì²­ ì „ ëŒ€ê¸°
                self.wait()
                
                # í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                result = func(*args, **kwargs)
                response_time = time.time() - start_time
                
                # ì„±ê³µ ê¸°ë¡
                self.record_success(response_time)
                print(f"   âœ… ìš”ì²­ ì„±ê³µ (ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ)")
                
                return result
                
            except requests.exceptions.Timeout as e:
                self.record_error("timeout")
                if attempt < self.max_retries - 1:
                    wait_time = self.current_delay * (self.backoff_factor ** attempt)
                    print(f"   ğŸ”„ íƒ€ì„ì•„ì›ƒ ë°œìƒ - {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{self.max_retries})")
                    time.sleep(wait_time)
                else:
                    print(f"   âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - íƒ€ì„ì•„ì›ƒ")
                    raise
                    
            except requests.exceptions.RequestException as e:
                self.record_error("connection")
                if attempt < self.max_retries - 1:
                    wait_time = self.current_delay * (self.backoff_factor ** attempt)
                    print(f"   ğŸ”„ ì—°ê²° ì—ëŸ¬ - {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{self.max_retries})")
                    time.sleep(wait_time)
                else:
                    print(f"   âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - ì—°ê²° ì—ëŸ¬")
                    raise
                    
            except Exception as e:
                self.record_error("other")
                # ì¼ë°˜ ì˜ˆì™¸ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
                print(f"   âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}")
                raise

# ì „ì—­ Rate Limiter ì¸ìŠ¤í„´ìŠ¤
_rate_limiter = None

def get_rate_limiter():
    """ì „ì—­ AdaptiveRateLimiter ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rate_limiter
    if _rate_limiter is None:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        initial_delay = float(os.getenv('RATE_LIMIT_INITIAL_DELAY', '1.0'))
        max_delay = float(os.getenv('RATE_LIMIT_MAX_DELAY', '30.0'))
        min_delay = float(os.getenv('RATE_LIMIT_MIN_DELAY', '0.5'))
        max_retries = int(os.getenv('RATE_LIMIT_MAX_RETRIES', '5'))
        
        _rate_limiter = AdaptiveRateLimiter(
            initial_delay=initial_delay,
            max_delay=max_delay,
            min_delay=min_delay,
            max_retries=max_retries
        )
        print(f"\nğŸ›¡ï¸ Adaptive Rate Limiter ì´ˆê¸°í™”")
        print(f"   ì´ˆê¸° ë”œë ˆì´: {initial_delay}ì´ˆ")
        print(f"   ìµœëŒ€ ë”œë ˆì´: {max_delay}ì´ˆ")
        print(f"   ìµœì†Œ ë”œë ˆì´: {min_delay}ì´ˆ")
        print(f"   ìµœëŒ€ ì¬ì‹œë„: {max_retries}íšŒ")
    return _rate_limiter

# =========================
# Keycloak ì¸ì¦ (auth.jsì™€ ë™ì¼í•œ êµ¬ì¡°)
# =========================
class KeycloakAuth:
    """Keycloak ì¸ì¦ ê´€ë¦¬ í´ë˜ìŠ¤ (auth.js êµ¬ì¡°ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„)"""
    
    def __init__(self):
        self.server_url = os.getenv('SERVER_URL', 'https://keycloak.211.238.12.60.nip.io:8100')
        self.realm = os.getenv('REALM', 'gfid-api')
        self.client_id = os.getenv('CLIENT_ID')
        self.client_secret = os.getenv('CLIENT_SECRET')
        
        # í† í° ìºì‹œ (auth.jsì˜ cached ê°ì²´ì™€ ë™ì¼)
        self.cached = {
            'access_token': None,
            'expires_at': 0
        }
        
        if not all([self.server_url, self.realm, self.client_id]):
            print("âš ï¸ Missing Keycloak env vars. Check .env file")
    
    def fetch_token(self):
        """
        Keycloak ì„œë²„ì—ì„œ í† í° ë°œê¸‰ (auth.jsì˜ fetchToken()ê³¼ ë™ì¼)
        .envì— ACCESS_TOKENì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        """
        # .envì— ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •ëœ ACCESS_TOKENì´ ìˆëŠ”ì§€ í™•ì¸
        manual_token = os.getenv('ACCESS_TOKEN')
        if manual_token:
            print("ğŸ“Œ .env íŒŒì¼ì˜ ACCESS_TOKEN ì‚¬ìš© (ìˆ˜ë™ ì„¤ì •)")
            now = int(time.time())
            # ìˆ˜ë™ í† í°ì€ ë§Œë£Œ ì‹œê°„ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ 1ì‹œê°„(3600ì´ˆ)ë¡œ ê°€ì •
            self.cached['access_token'] = manual_token
            self.cached['expires_at'] = now + 3600
            return self.cached
        
        # â‘  Keycloak í† í° ì—”ë“œí¬ì¸íŠ¸ URL ìƒì„±
        token_url = f"{self.server_url.rstrip('/')}/realms/{self.realm}/protocol/openid-connect/token"
        
        # â‘¡ OAuth2 Client Credentials ë°©ì‹ìœ¼ë¡œ ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
        data = {
            'grant_type': 'client_credentials',
            'client_id': self.client_id
        }
        if self.client_secret:
            data['client_secret'] = self.client_secret
        
        print(f"ğŸ” Keycloak ì„œë²„ì— í† í° ìš”ì²­ ì¤‘...")
        print(f"   URL: {token_url}")
        
        try:
            # â‘¢ Keycloak ì„œë²„ì— POST ìš”ì²­
            response = requests.post(
                token_url,
                data=data,
                headers={'Content-Type': 'application/x-www-form-urlencoded'},
                timeout=60,
                verify=False  # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”
            )
            
            if response.status_code == 200:
                # â‘£ ì‘ë‹µì—ì„œ í† í° ì¶”ì¶œ ë° ìºì‹œ ì €ì¥
                token_data = response.json()
                now = int(time.time())
                self.cached['access_token'] = token_data.get('access_token')
                self.cached['expires_at'] = now + token_data.get('expires_in', 300)
                
                print(f"âœ… ìë™ í† í° ë°œê¸‰ ì„±ê³µ!")
                return self.cached
            else:
                # â‘¤ ì—ëŸ¬ ì²˜ë¦¬
                print(f"âŒ Keycloak token fetch failed: {response.status_code}")
                if response.text:
                    print(f"   Response: {response.text}")
                raise Exception(f"Keycloak token request failed with status {response.status_code}")
                
        except requests.exceptions.Timeout:
            print("âŒ Keycloak token fetch timeout")
            print("ğŸ’¡ í•´ê²° ë°©ë²•: Postmanì—ì„œ í† í°ì„ ë°›ì•„ .env íŒŒì¼ì— ACCESS_TOKENìœ¼ë¡œ ì¶”ê°€í•˜ì„¸ìš”")
            raise Exception("Keycloak ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ. .envì— ACCESS_TOKENì„ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Keycloak token fetch error: {str(e)}")
            print("ğŸ’¡ í•´ê²° ë°©ë²•: Postmanì—ì„œ í† í°ì„ ë°›ì•„ .env íŒŒì¼ì— ACCESS_TOKENìœ¼ë¡œ ì¶”ê°€í•˜ì„¸ìš”")
            raise Exception(f"Keycloak ì—°ê²° ì‹¤íŒ¨. .envì— ACCESS_TOKENì„ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    
    def get_token(self):
        """
        í† í° ì¡°íšŒ - ìºì‹œëœ í† í° ë°˜í™˜ ë˜ëŠ” ìƒˆë¡œ ë°œê¸‰ (auth.jsì˜ getToken()ê³¼ ë™ì¼)
        """
        now = int(time.time())
        
        # ìºì‹œëœ í† í°ì´ ìœ íš¨í•œì§€ í™•ì¸ (ë§Œë£Œ 30ì´ˆ ì „ê¹Œì§€ ìœ íš¨)
        if self.cached['access_token'] and self.cached['expires_at'] - 30 > now:
            return self.cached['access_token']
        
        # í† í°ì´ ì—†ê±°ë‚˜ ë§Œë£Œë˜ì—ˆìœ¼ë©´ ìƒˆë¡œ ë°œê¸‰
        self.fetch_token()
        return self.cached['access_token']
    
    def get_token_info(self):
        """
        í† í° ì •ë³´ ì¡°íšŒ (auth.jsì˜ getTokenInfo()ì™€ ë™ì¼)
        """
        now = int(time.time())
        return {
            'hasToken': bool(self.cached['access_token']),
            'expiresAt': self.cached['expires_at'],
            'secondsUntilExpiry': max(0, self.cached['expires_at'] - now)
        }


# ì „ì—­ ì¸ì¦ ê°ì²´ ìƒì„±
_auth = None

def get_auth():
    """ì „ì—­ KeycloakAuth ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _auth
    if _auth is None:
        _auth = KeycloakAuth()
    return _auth


def fetch_data_directly_from_gfid(dsid=None, dsid_list=None):
    """
    Keycloak ì¸ì¦ í›„ GFID APIì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    (gfidClient.jsì˜ downloadDataset()ê³¼ ìœ ì‚¬í•œ êµ¬ì¡°)
    
    Parameters:
    -----------
    dsid : str, optional
        ë‹¨ì¼ ë°ì´í„°ì…‹ ID (í•˜ë‚˜ë§Œ ë¡œë“œí•  ê²½ìš°)
    dsid_list : list, optional
        ì—¬ëŸ¬ ë°ì´í„°ì…‹ ID ë¦¬ìŠ¤íŠ¸ (ì—¬ëŸ¬ ê°œ ë¡œë“œí•  ê²½ìš°, ê¸°ë³¸ê°’: DATASET_IDS)
    
    Returns:
    --------
    pd.DataFrame
        ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    print("\n" + "=" * 60)
    print("ğŸŒ GFID APIì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë”© (Python ë°©ì‹)")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ ID ì„¤ì •
    # USE_SINGLE_DATASET=trueì¼ ë•Œë§Œ DSID í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©, ì•„ë‹ˆë©´ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
    use_single = os.getenv('USE_SINGLE_DATASET', 'false').lower() == 'true'
    
    if dsid_list is None and dsid is None:
        if use_single:
            # ëª…ì‹œì ìœ¼ë¡œ ë‹¨ì¼ ë°ì´í„°ì…‹ ì‚¬ìš© ì„¤ì •ëœ ê²½ìš°ì—ë§Œ DSID í™˜ê²½ë³€ìˆ˜ í™•ì¸
            env_dsid = os.getenv('DSID')
            if env_dsid:
                dsid_list = [env_dsid]
                print(f"âš™ï¸  USE_SINGLE_DATASET=true â†’ ë‹¨ì¼ ë°ì´í„°ì…‹ ëª¨ë“œ")
            else:
                print(f"âš ï¸  USE_SINGLE_DATASET=trueì´ì§€ë§Œ DSID ë¯¸ì„¤ì • â†’ ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©")
                dsid_list = DATASET_IDS
        else:
            # ê¸°ë³¸ê°’: ì „ì²´ ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
            print(f"âš™ï¸  ê¸°ë³¸ ëª¨ë“œ â†’ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ")
            dsid_list = DATASET_IDS
    elif dsid is not None:
        dsid_list = [dsid]
    
    print(f"ğŸ“‹ ë¡œë“œí•  ë°ì´í„°ì…‹ ê°œìˆ˜: {len(dsid_list)}")
    if len(dsid_list) <= 10:
        print(f"   ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸: {dsid_list}")
    else:
        print(f"   ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸: {dsid_list[:5]} ... {dsid_list[-5:]}")
    
    from_date = os.getenv('FROM', '2025-01-01')
    to_date = os.getenv('TO', '2025-12-31')
    
    print(f"   - ë‚ ì§œ ë²”ìœ„: {from_date} ~ {to_date}")
    
    # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë¡œë”© ë° ë³‘í•©
    # Node.js API ì„œë²„(localhost:3000)ë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ
    all_dataframes = []
    
    api_url = os.getenv('API_URL', 'http://localhost:3000')
    print(f"   API ì„œë²„: {api_url}")
    
    # Rate Limiter í™œì„±í™”
    rate_limiter = get_rate_limiter()
    print(f"\nğŸ›¡ï¸ Rate Limiter í™œì„±í™” - ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ ëª¨ë“œ")
    
    for idx, current_dsid in enumerate(dsid_list, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“¥ [{idx}/{len(dsid_list)}] ë°ì´í„°ì…‹ ë¡œë”©: {current_dsid}")
        print(f"{'='*60}")
        
        try:
            # Rate Limiterë¥¼ ì‚¬ìš©í•˜ì—¬ fetch_data_from_api() í˜¸ì¶œ
            df_single = rate_limiter.execute_with_retry(
                fetch_data_from_api, 
                dsid=current_dsid, 
                api_url=api_url,
                _skip_rate_limiter=True  # ë‚´ë¶€ í˜¸ì¶œì´ë¯€ë¡œ ì¤‘ë³µ ì ìš© ë°©ì§€
            )
            
            if df_single is not None and not df_single.empty:
                # ë°ì´í„°ì…‹ IDë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€ (ì–´ë–¤ ë°ì´í„°ì…‹ì—ì„œ ì™”ëŠ”ì§€ ì¶”ì )
                df_single['dataset_id'] = current_dsid
                all_dataframes.append(df_single)
                print(f"   âœ… {current_dsid} ë¡œë“œ ì™„ë£Œ: {df_single.shape}")
            else:
                print(f"   âš ï¸ {current_dsid} ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨")
        except Exception as e:
            print(f"   âš ï¸ {current_dsid} ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì—°ì† ì—ëŸ¬ê°€ ë§ìœ¼ë©´ ì¤‘ë‹¨ ê³ ë ¤
            if rate_limiter.consecutive_errors >= 3:
                print(f"\nğŸš¨ ì—°ì† {rate_limiter.consecutive_errors}íšŒ ì—ëŸ¬ ë°œìƒ!")
                user_input = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
                if user_input != 'y':
                    print("ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
            continue
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    rate_limiter.print_stats()
    
    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ë³‘í•©
    if not all_dataframes:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ë°ì´í„° ë³‘í•© ì¤‘...")
    print(f"{'='*60}")
    
    # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•© (í–‰ ë°©í–¥ concatenation)
    df_merged = pd.concat(all_dataframes, ignore_index=True)
    
    print(f"âœ… ì „ì²´ ë³‘í•© ì™„ë£Œ!")
    print(f"   - ë¡œë“œëœ ë°ì´í„°ì…‹ ê°œìˆ˜: {len(all_dataframes)}")
    print(f"   - ìµœì¢… ë°ì´í„° í¬ê¸°: {df_merged.shape}")
    print(f"   - ì»¬ëŸ¼: {list(df_merged.columns)}")
    print(f"="*60 + "\n")
    
    return df_merged


def _fetch_single_dataset(dsid, from_date, to_date, access_token):
    """
    ë‹¨ì¼ ë°ì´í„°ì…‹ì„ GFID APIì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë‚´ë¶€ í•¨ìˆ˜
    
    Parameters:
    -----------
    dsid : str
        ë°ì´í„°ì…‹ ID
    from_date : str
        ì‹œì‘ ë‚ ì§œ
    to_date : str
        ì¢…ë£Œ ë‚ ì§œ
    access_token : str
        Keycloak ì•¡ì„¸ìŠ¤ í† í°
    
    Returns:
    --------
    pd.DataFrame or None
        ê°€ì ¸ì˜¨ ë°ì´í„°í”„ë ˆì„ ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    # GFID API í˜¸ì¶œ
    api_base = os.getenv('GFID_API_BASE', 'http://211.238.12.60:8084/data/api/v1')
    api_path = f"/etl_data/id/{dsid}/from/{from_date}/to/{to_date}"
    api_url = api_base + api_path
    
    print(f"   URL: {api_url}")
    
    headers = {'Authorization': f'Bearer {access_token}'}
    
    try:
        response = requests.get(api_url, headers=headers, timeout=300, verify=False)
        
        print(f"   ì‘ë‹µ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            print(f"   âš ï¸ API ìš”ì²­ ì‹¤íŒ¨! (ìƒíƒœ ì½”ë“œ: {response.status_code})")
            print(f"   ì‘ë‹µ: {response.text[:500]}")
            return None  # ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        
        result = response.json()
        
        # GFID_ITEMS_KEY ê²½ë¡œë¡œ ë°ì´í„° ì¶”ì¶œ (ì˜ˆ: 'body.data')
        items_key = os.getenv('GFID_ITEMS_KEY', 'body.data')
        data = result
        for key in items_key.split('.'):
            data = data.get(key, {})
        
        if not data:
            print(f"   âš ï¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            return None
        
        print(f"   - ë°›ì€ ë ˆì½”ë“œ ìˆ˜: {len(data)}")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data)
        print(f"   - DataFrame í¬ê¸°: {df.shape}")
        
        # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ë³€í™˜
        date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
        if date_columns:
            for col in date_columns:
                try:
                    df[col] = pd.to_datetime(df[col])
                except:
                    pass
        
        return df
        
    except Exception as e:
        print(f"   âš ï¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        return None

# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)
# =========================
print("=" * 60)
print("ğŸ” í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ê³¼ì • ë””ë²„ê¹…")
print("=" * 60)

# .env íŒŒì¼ ê²½ë¡œ í™•ì¸
env_path = Path.cwd() / '.env'
print(f"1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
print(f"2. .env íŒŒì¼ ê²½ë¡œ: {env_path}")
print(f"3. .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {env_path.exists()}")

# .env íŒŒì¼ ë¡œë“œ
load_result = load_dotenv(env_path, verbose=True, override=True)
print(f"4. .env ë¡œë“œ ê²°ê³¼: {load_result}")

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
use_api_raw = os.getenv('USE_API')
use_api_bool = os.getenv('USE_API', 'false').lower() == 'true'
dsid = os.getenv('DSID')
api_url = os.getenv('API_URL')

print(f"\nğŸ“‹ í™˜ê²½ë³€ìˆ˜ ê°’:")
print(f"   - USE_API (ì›ë³¸): '{use_api_raw}'")
print(f"   - USE_API (boolean): {use_api_bool}")
print(f"   - DSID: '{dsid}'")
print(f"   - API_URL: '{api_url}'")
print("=" * 60 + "\n")

# =========================
# Paths & device
# =========================
BASE_DIR = Path.cwd()
# ìš°ì„ ìˆœìœ„ë¡œ íƒìƒ‰ (ìƒˆ íŒŒì¼ -> êµ¬ íŒŒì¼ë“¤)
CANDIDATE_CSVS = [
    BASE_DIR / "suyeong/3_merged_influenza_vaccine_respiratory_weather.csv",
]

# =========================
# API ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =========================
def fetch_data_from_api(dsid=None, api_url=None, _skip_rate_limiter=False):
    """
    Node.js API ì„œë²„ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    dsid : str, optional
        ë°ì´í„°ì…‹ ID (ê¸°ë³¸ê°’ì€ í™˜ê²½ë³€ìˆ˜ DSID ì‚¬ìš©)
    api_url : str, optional
        API ì„œë²„ URL (ê¸°ë³¸ê°’: http://localhost:3000)
    _skip_rate_limiter : bool, optional
        Rate Limiter ì¤‘ë³µ ì ìš© ë°©ì§€ìš© (ë‚´ë¶€ìš©)
    
    Returns:
    --------
    pd.DataFrame
        APIë¡œë¶€í„° ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    """
    print("\n" + "=" * 60)
    print("ğŸŒ API ë°ì´í„° ë¡œë”© ì‹œì‘ (Node.js ì„œë²„ ê²½ìœ )")
    print("=" * 60)
    
    # API ì„œë²„ URL ì„¤ì •
    if api_url is None:
        api_url = os.getenv('API_URL', 'http://localhost:3000')
    print(f"1. API URL: {api_url}")
    
    # ë°ì´í„°ì…‹ ID ì„¤ì •
    if dsid is None:
        dsid = os.getenv('DSID')
    print(f"2. Dataset ID: {dsid}")
    
    if not dsid:
        raise ValueError("dsidê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ DSIDë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì¸ìë¡œ ì „ë‹¬í•˜ì„¸ìš”.")
    
    print(f"3. APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘... (dsid: {dsid})")
    
    try:
        # API ì„œë²„ì— ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìš”ì²­
        request_url = f"{api_url}/download"
        request_body = {"dsid": dsid}
        print(f"4. ìš”ì²­ URL: {request_url}")
        print(f"5. ìš”ì²­ Body: {request_body}")
        
        response = requests.post(
            request_url,
            json=request_body,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        print(f"6. ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨!")
            print(f"   ì‘ë‹µ ë‚´ìš©: {response.text}")
            raise Exception(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
        
        result = response.json()
        print(f"7. ì‘ë‹µ ì„±ê³µ ì—¬ë¶€: {result.get('ok')}")
        
        if not result.get('ok'):
            print(f"âŒ API ì—ëŸ¬ ë°œìƒ!")
            print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {result.get('error', 'Unknown error')}")
            raise Exception(f"API ì—ëŸ¬: {result.get('error', 'Unknown error')}")
        
        # í˜ì´ì§€ íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ì½ê¸°
        page_files = result.get('result', {}).get('pageFiles', [])
        print(f"8. ë°›ì€ í˜ì´ì§€ íŒŒì¼ ìˆ˜: {len(page_files)}")
        
        if not page_files:
            print(f"âŒ í˜ì´ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            raise Exception("APIë¡œë¶€í„° ë°›ì€ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"9. í˜ì´ì§€ íŒŒì¼ ëª©ë¡:")
        for i, pf in enumerate(page_files, 1):
            print(f"   {i}. {pf}")
        
        # ëª¨ë“  í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ í•©ì¹˜ê¸°
        all_data = []
        for idx, page_file in enumerate(page_files, 1):
            print(f"10-{idx}. íŒŒì¼ ì½ëŠ” ì¤‘: {page_file}")
            with open(page_file, 'r', encoding='utf-8') as f:
                page_data = json.load(f)
                print(f"      ë ˆì½”ë“œ ìˆ˜: {len(page_data)}")
                all_data.extend(page_data)
        
        print(f"11. ì´ ë ˆì½”ë“œ ìˆ˜: {len(all_data)}")
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(all_data)
        print(f"12. DataFrame ìƒì„± ì™„ë£Œ: {df.shape}")
        print(f"13. ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
        if date_columns:
            print(f"14. ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {date_columns}")
            for col in date_columns:
                try:
                    df[col] = pd.to_datetime(df[col])
                    print(f"    âœ… {col} â†’ datetime ë³€í™˜ ì™„ë£Œ")
                except Exception as e:
                    print(f"    âš ï¸ {col} â†’ datetime ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        print(f"âœ… API ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
        print("=" * 60 + "\n")
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨!")
        print(f"   ì—ëŸ¬: {str(e)}")
        print("=" * 60 + "\n")
        raise Exception(f"API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}. API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨!")
        print(f"   ì—ëŸ¬: {str(e)}")
        print("=" * 60 + "\n")
        raise Exception(f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")


def load_data_from_api_or_csv(use_api=None, dsid=None, csv_path=None):
    """
    API ë˜ëŠ” ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í†µí•© í•¨ìˆ˜
    
    Parameters:
    -----------
    use_api : bool, optional
        Trueë©´ API ì‚¬ìš©, Falseë©´ CSV íŒŒì¼ ì‚¬ìš© (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ USE_API)
    dsid : str, optional
        API ì‚¬ìš© ì‹œ ë°ì´í„°ì…‹ ID
    csv_path : Path, optional
        CSV ì‚¬ìš© ì‹œ íŒŒì¼ ê²½ë¡œ
    
    Returns:
    --------
    pd.DataFrame
        ë¡œë“œëœ ë°ì´í„°
    """
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ USE_API ì„¤ì • í™•ì¸
    if use_api is None:
        use_api = os.getenv('USE_API', 'false').lower() == 'true'
    
    print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ëª¨ë“œ ê²°ì •: use_api={use_api}")
    
    if use_api:
        print("=" * 50)
        print("ğŸŒ API ëª¨ë“œ: ì„œë²„ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
        print("=" * 50)
        df = fetch_data_from_api(dsid=dsid)
        print(f"âœ… APIë¡œë¶€í„° ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
        return df
    else:
        print("=" * 50)
        print("ğŸ“ CSV ëª¨ë“œ: ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        print("=" * 50)
        if csv_path is None:
            csv_path = pick_csv_path()
        df = pd.read_csv(csv_path)
        print(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {csv_path}, {df.shape}")
        return df

def pick_csv_path():
    for p in CANDIDATE_CSVS:
        if p.exists():
            return p
    raise FileNotFoundError("No input CSV found among:\n" + "\n".join(map(str, CANDIDATE_CSVS)))

# CSV_PATHëŠ” í•„ìš”í•  ë•Œë§Œ ì„¤ì • (API ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
print("\n" + "=" * 60)
print("ğŸ“‚ CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •")
print("=" * 60)

USE_API_MODE = os.getenv('USE_API', 'false').lower() == 'true'
print(f"USE_API_MODE ê²°ì •: {USE_API_MODE}")
print(f"   - ì›ë³¸ í™˜ê²½ë³€ìˆ˜ ê°’: '{os.getenv('USE_API')}'")
print(f"   - ì†Œë¬¸ì ë³€í™˜: '{os.getenv('USE_API', 'false').lower()}'")
print(f"   - 'true' ë¹„êµ ê²°ê³¼: {os.getenv('USE_API', 'false').lower() == 'true'}")

if not USE_API_MODE:
    print("\nâ¡ï¸ CSV ëª¨ë“œ - CSV íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤...")
    try:
        CSV_PATH = pick_csv_path()
        print(f"âœ… CSV íŒŒì¼ ë°œê²¬: {CSV_PATH.name}")
    except FileNotFoundError as e:
        print(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ê²€ìƒ‰í•œ ê²½ë¡œ: {CANDIDATE_CSVS}")
        print(f"ğŸ’¡ API ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ .envì—ì„œ USE_API=trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        CSV_PATH = None
else:
    print("\nâ¡ï¸ API ëª¨ë“œ - CSV íŒŒì¼ ê²€ìƒ‰ì„ ìƒëµí•©ë‹ˆë‹¤")
    CSV_PATH = None
    print("ğŸŒ API ëª¨ë“œ í™œì„±í™”ë¨ - CSV íŒŒì¼ ê²€ìƒ‰ ìƒëµ")

print("=" * 60 + "\n")

def pick_device():
    if torch.cuda.is_available():
        return "cuda"
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

DEVICE = pick_device()
SEED   = 42

print(f"ğŸ–¥ï¸ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {DEVICE}")
print(f"ğŸ² ëœë¤ ì‹œë“œ: {SEED}\n")


# =========================
# Hyperparameters
# =========================
EPOCHS      = 100
BATCH_SIZE  = 64        # ì†Œê·œëª¨ ì‹œê³„ì—´ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ë„ë¡ ì•½ê°„ ë‚®ì¶¤
SEQ_LEN     = 12
PRED_LEN    = 3
PATCH_LEN   = 4          # â† CNNì´ ìµœì†Œ 3~5 ì»¤ë„ ì ìš© ê°€ëŠ¥í•˜ë„ë¡ í™•ëŒ€
STRIDE      = 1

D_MODEL     = 128        # 4ì˜ ë°°ìˆ˜(ë©€í‹°ìŠ¤ì¼€ì¼ ë¶„ê¸° 4ê°œ í•©ì‚°)
N_HEADS     = 2
ENC_LAYERS  = 4
FF_DIM      = 128
DROPOUT     = 0.3        # ì•½ê°„ ê°•í™”
HEAD_HIDDEN = [64, 64]

LR              = 5e-4
WEIGHT_DECAY    = 5e-4
PATIENCE        = 60
WARMUP_EPOCHS   = 30

SCALER_TYPE     = "robust"   # ë…¸ì´ì¦ˆ/ê¼¬ë¦¬ê°’ ëŒ€ì‘ì— ìœ ë¦¬ (ì›í•˜ë©´ "standard"ë¡œ ë³€ê²½)

# ì™¸ìƒ íŠ¹ì§• ì‚¬ìš© ëª¨ë“œ: "auto"|"none"|"vax"|"resp"|"both"
USE_EXOG        = "all"

OUT_CSV          = str(BASE_DIR / "ili_predictions.csv")
PLOT_LAST_WINDOW = str(BASE_DIR / "plot_last_window.png")
PLOT_TEST_RECON  = str(BASE_DIR / "plot_test_reconstruction.png")
PLOT_MA_CURVES   = str(BASE_DIR / "plot_ma_curves.png")

# overlap ì¬êµ¬ì„± ê°€ì¤‘ì¹˜ (t+1ì„ ì¡°ê¸ˆ ë” ì‹ ë¢°)
RECON_W_START, RECON_W_END = 2.0, 0.5

# --- Feature switches ---
INCLUDE_SEASONAL_FEATS = True   # week_sin, week_cosë¥¼ ì…ë ¥ í”¼ì²˜ì— í¬í•¨í• ì§€

# =========================
# utils
# =========================
from datetime import date

def _iso_weeks_in_year(y: int) -> int:
    # ISO ë‹¬ë ¥ì˜ ë§ˆì§€ë§‰ ì£¼ ë²ˆí˜¸(52 ë˜ëŠ” 53)
    return date(y, 12, 28).isocalendar().week

def weekly_to_daily_interp(
    df: pd.DataFrame,
    season_col: str = "season_norm",
    week_col: str = "week",
    target_col: str = "ili",
) -> pd.DataFrame:
    """
    ì£¼ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì¼ ë‹¨ìœ„ë¡œ í™•ì¥(ì„ í˜•ë³´ê°„). season/week ì—†ìœ¼ë©´ labelì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜,
    ìµœí›„ì—ëŠ” ì—°ì† ì£¼ì°¨ë¥¼ ìƒì„±í•´ ë³´ê°„í•©ë‹ˆë‹¤.
    ë°˜í™˜: date ì»¬ëŸ¼ í¬í•¨í•œ ì¼ ë‹¨ìœ„ DF
    """
    df = df.copy()
    df.columns = df.columns.str.replace("\ufeff", "", regex=True).str.strip()

    # --- ì‹œì¦Œ/ì£¼ì°¨ í™•ë³´ ---
    has_season = season_col in df.columns
    has_week   = week_col in df.columns

    if not (has_season and has_week):
        # labelì—ì„œ ì‹œì¦Œ/ì£¼ì°¨ ì¶”ì¶œ ì‹œë„: "2024-2025 season - W29"
        if "label" in df.columns:
            import re
            def _parse_label(lbl):
                m = re.search(r"(\d{4}-\d{4}).*W\s*([0-9]+)", str(lbl))
                if m:
                    return m.group(1), int(m.group(2))
                return None
            parsed = df["label"].map(_parse_label)
            if not has_season:
                df[season_col] = [p[0] if p else np.nan for p in parsed]
                has_season = True
            if not has_week:
                df[week_col] = [p[1] if p else np.nan for p in parsed]
                has_week = True

    # ìµœí›„ì˜ ìˆ˜ë‹¨: season_normì´ ì—†ìœ¼ë©´ ë‹¨ì¼ ì‹œì¦Œìœ¼ë¡œ, week ì—†ìœ¼ë©´ 1..N
    if not has_season:
        # ì²« í–‰ì˜ ì—°ë„ë¥¼ ì°¾ì•„ ëŒ€ì²´ ì‹œì¦Œëª… ë§Œë“¤ê¸°
        # ì—†ìœ¼ë©´ "0000-0001"
        first_year = None
        if "date" in df.columns:
            try:
                first_year = pd.to_datetime(df["date"]).dt.year.min()
            except Exception:
                pass
        if first_year is None:
            first_year = pd.Timestamp.today().year
        df[season_col] = f"{first_year}-{first_year+1}"
        has_season = True

    if not has_week:
        df[week_col] = np.arange(1, len(df) + 1, dtype=int)
        has_week = True

    # ìˆ«ìí™”
    df[week_col] = pd.to_numeric(df[week_col], errors="coerce")
    # ì‹œì¦Œ ë¬¸ìì—´ ì •ê·œí™”
    def _norm_season_text_local(s: str) -> str:
        ss = str(s).replace("ì ˆê¸°", "")
        import re
        m = re.search(r"(\d{4})\s*-\s*(\d{4})", ss)
        return f"{m.group(1)}-{m.group(2)}" if m else ss.strip()
    df[season_col] = df[season_col].astype(str).map(_norm_season_text_local)

    # --- ISO ì£¼ ì‹œì‘ì¼ ì‚°ì¶œ (ì‹œì¦Œ ê·œì¹™ ë°˜ì˜) ---
    week_starts = []
    for _, row in df.iterrows():
        season = str(row[season_col])
        try:
            y0 = int(season.split("-")[0])
        except Exception:
            y0 = pd.Timestamp.today().year
        wk = int(row[week_col]) if not pd.isna(row[week_col]) else 1
        iso_year = y0 if wk >= 36 else (y0 + 1)
        # í•´ë‹¹ ISOë…„ì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì£¼ ë„˜ì§€ ì•Šë„ë¡ ë³´ì •
        wk = min(max(1, wk), _iso_weeks_in_year(iso_year))
        # ì›”ìš”ì¼(1) ê¸°ì¤€ ì£¼ ì‹œì‘ì¼
        week_starts.append(pd.Timestamp.fromisocalendar(iso_year, wk, 1))
    df["week_start"] = week_starts

    # --- ì¤‘ë³µ week_start ì²˜ë¦¬: ìˆ˜ì¹˜=mean, ë¹„ìˆ˜ì¹˜=first ---
    if df["week_start"].duplicated().any():
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        agg = {c: "mean" for c in num_cols}
        # ë¹„ìˆ˜ì¹˜ ì»¬ëŸ¼(ë¼ë²¨/ì‹œì¦Œ ë“±)ì€ ì²« ê°’ ìœ ì§€
        for c in df.columns:
            if c not in num_cols and c != "week_start":
                agg[c] = "first"
        df = df.groupby("week_start", as_index=False).agg(agg)

    # --- ì¼ ë‹¨ìœ„ ë¦¬ìƒ˜í”Œ ---
    df = df.set_index("week_start").sort_index()
    df_daily = df.resample("D").asfreq()

    # ìˆ˜ì¹˜í˜•ì€ ì„ í˜•ë³´ê°„
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in num_cols:
        df_daily[c] = df_daily[c].interpolate(method="linear", limit_direction="both")

    # ë²”ì£¼í˜•ì€ ì•ë’¤ ì±„ì›€
    cat_cols = [c for c in df.columns if c not in num_cols]
    for c in cat_cols:
        df_daily[c] = df_daily[c].ffill().bfill()

    # ê²°ê³¼
    out = df_daily.reset_index().rename(columns={"week_start": "date"})
    # dateëŠ” datetimeìœ¼ë¡œ ê°•ì œ
    out["date"] = pd.to_datetime(out["date"])
    return out
    
def set_seed(seed=42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def read_csv_kor(path: Path) -> pd.DataFrame:
    for enc in ["euc-kr", "cp949", "utf-8-sig", "utf-8"]:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            pass
    return pd.read_csv(path, encoding="utf-8", errors="replace")

def make_splits(n: int, train_ratio=0.7, val_ratio=0.15):
    n_train = int(n * train_ratio)
    n_val   = int(n * val_ratio)
    return (0, n_train), (n_train, n_train+n_val), (n_train+n_val, n)

def get_scaler(name=None):
    s = (name or SCALER_TYPE).lower()
    if s == "robust":  return RobustScaler()
    if s == "minmax":  return MinMaxScaler()
    return StandardScaler()

def _norm_season_text(s: str) -> str:
    ss = str(s).replace("ì ˆê¸°", "")
    import re
    m = re.search(r"(\d{4})\s*-\s*(\d{4})", ss)
    return f"{m.group(1)}-{m.group(2)}" if m else ss.strip()

# =========================
# data loader (multivariate-ready)
# =========================
def load_and_prepare(csv_path: Path = None, use_exog: str = "auto", df: pd.DataFrame = None) -> Tuple[np.ndarray, np.ndarray, list, list]:
    """
    Returns:
        X: (N, F) features (first column should be 'ili' to align with univariate fallback)
        y: (N,) target (ili)
        labels: list[str] for plotting ticks
        used_feat_names: list[str] feature column names (len=F)
    
    Parameters:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ (dfê°€ Noneì¼ ë•Œ ì‚¬ìš©)
        use_exog: ì™¸ìƒë³€ìˆ˜ ì‚¬ìš© ëª¨ë“œ
        df: ì´ë¯¸ ë¡œë“œëœ DataFrame (APIì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°)
    """
    if df is None:
        if csv_path is None:
            raise ValueError("csv_pathì™€ df ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        df = read_csv_kor(csv_path).copy()
    else:
        df = df.copy()
    df = weekly_to_daily_interp(df, season_col="season_norm", week_col="week", target_col="ili")
    # ì •ë ¬
# ì •ë ¬: ì£¼â†’ì¼ ë³€í™˜ í›„ì—ëŠ” date ê¸°ì¤€ìœ¼ë¡œë§Œ ì •ë ¬
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date").reset_index(drop=True)
    else:
        # (ê·¹íˆ ë“œë¬¸ fallback) dateê°€ ì—†ì„ ë•Œë§Œ ê¸°ì¡´ ë¡œì§
        if {"season_norm", "week"}.issubset(df.columns):
            df["season_norm"] = df["season_norm"].astype(str).map(_norm_season_text)
            df["week"] = pd.to_numeric(df["week"], errors="coerce")
            df = df.sort_values(["season_norm", "week"]).copy()
        elif "label" in df.columns:
            df = df.sort_values(["label"]).copy()

    # íƒ€ê¹ƒ
    if "ili" not in df.columns:
        raise ValueError("CSVì— 'ili' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    df["ili"] = pd.to_numeric(df["ili"], errors="coerce")
    if df["ili"].isna().any():
        df["ili"] = df["ili"].interpolate(method="linear", limit_direction="both").fillna(df["ili"].median())
    
    # --- âœ… Seasonality feature ì¶”ê°€ ---
    if "week" in df.columns:
        df["week_sin"] = np.sin(2 * np.pi * df["week"] / 52.0)
        df["week_cos"] = np.cos(2 * np.pi * df["week"] / 52.0)
    else:
        df["week_sin"] = 0.0
        df["week_cos"] = 0.0

    # --- âœ… Alias ë§¤í•‘ ---
    if "case_count" in df.columns and "respiratory_index" not in df.columns:
        df["respiratory_index"] = df["case_count"]

    # ê¸°í›„ í”¼ì²˜ í›„ë³´
    climate_feats = []
    if "wx_week_avg_temp" in df.columns:     climate_feats.append("wx_week_avg_temp")
    if "wx_week_avg_rain" in df.columns:     climate_feats.append("wx_week_avg_rain")
    if "wx_week_avg_humidity" in df.columns: climate_feats.append("wx_week_avg_humidity")

    # ì™¸ìƒ í›„ë³´ ì¡´ì¬ ì—¬ë¶€
    has_vax  = "vaccine_rate" in df.columns
    has_resp = "respiratory_index" in df.columns

    # ì–´ë–¤ íŠ¹ì§•ì„ ì“¸ì§€ ê²°ì •
    mode = use_exog.lower()
    if mode == "auto":
        chosen = ["ili"]
        if has_vax:  chosen.append("vaccine_rate")
        if has_resp: chosen.append("respiratory_index")
        chosen += climate_feats
    elif mode == "none":
        chosen = ["ili"]
    elif mode == "vax":
        chosen = ["ili"] + (["vaccine_rate"] if has_vax else [])
    elif mode == "resp":
        chosen = ["ili"] + (["respiratory_index"] if has_resp else [])
    elif mode == "both":
        chosen = ["ili"]
        if has_vax:  chosen.append("vaccine_rate")
        if has_resp: chosen.append("respiratory_index")
        chosen += climate_feats
    elif mode == "climate":
        chosen = ["ili"] + climate_feats
    elif mode == "all":
        chosen = ["ili"]
        if has_vax:  chosen.append("vaccine_rate")
        if has_resp: chosen.append("respiratory_index")
        chosen += climate_feats
    else:
        raise ValueError(f"Unknown USE_EXOG mode: {use_exog}")

    # ìˆ«ìí™” & ë³´ê°„
    for c in chosen:
        df[c] = pd.to_numeric(df[c], errors="coerce")
        if df[c].isna().any():
            df[c] = df[c].interpolate(method="linear", limit_direction="both").fillna(df[c].median())

    # ë¼ë²¨
    if "label" in df.columns and df["label"].notna().any():
        labels = df["label"].astype(str).tolist()
    elif {"season_norm","week"}.issubset(df.columns):
        labels = (df["season_norm"].astype(str) + " season - W" + df["week"].astype(int).astype(str)).tolist()
    else:
        labels = [f"idx_{i}" for i in range(len(df))]

    # X, y êµ¬ì„±
    feat_names = chosen[:]
    if INCLUDE_SEASONAL_FEATS and {"week_sin", "week_cos"}.issubset(df.columns):
        feat_names += ["week_sin", "week_cos"]

    # ì„ íƒëœ ì…ë ¥ í”¼ì²˜ ë¡œê·¸ ì°ê¸°
    print("[Data] Exogenous detected -> vaccine_rate:", has_vax, "| respiratory_index:", has_resp, "| climate_feats:", climate_feats)
    print("[Data] Selected feature columns (order) ->", feat_names)

    X = df[feat_names].to_numpy(dtype=float)
    y = df["ili"].to_numpy(dtype=float)
    return X, y, labels, feat_names

# =========================
# dataset
# =========================
class PatchTSTDataset(Dataset):
    """Multivariate X (N,F) + y (N,) -> (patchified) windows."""
    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len:int, pred_len:int, patch_len:int, stride:int):
        assert len(X) == len(y)
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)
        self.seq_len, self.pred_len = seq_len, pred_len
        self.patch_len, self.stride = patch_len, stride
        max_start = len(self.y) - (seq_len + pred_len)
        self.indices = list(range(max(0, max_start + 1)))

    def __len__(self): return len(self.indices)

    def __getitem__(self, idx):
        i = self.indices[idx]
        seq_X = self.X[i:i+self.seq_len, :]                      # (L, F)
        tgt_y = self.y[i+self.seq_len:i+self.seq_len+self.pred_len]  # (H,)

        # patchify along time axis
        patches = []
        pos = 0
        while pos + self.patch_len <= self.seq_len:
            patches.append(seq_X[pos:pos+self.patch_len, :])     # (patch_len, F)
            pos += self.stride
        X_patch = np.stack(patches, axis=0)                      # (P, patch_len, F)
        return torch.from_numpy(X_patch).float(), torch.from_numpy(tgt_y).float(), i

# =========================
# model (Multi-Scale CNN + TokenConvMixer + PatchTST + AttnPool)
# =========================
class MultiScaleCNNPatchEmbed(nn.Module):
    """
    (B, P, L, F) -> [ê° íŒ¨ì¹˜] ë©€í‹°ìŠ¤ì¼€ì¼ Conv1d ë¶„ê¸°(k=2/3/5, ë˜ í•˜ë‚˜ëŠ” dilation=2) â†’ GAP â†’ (B, P, D)
    - ë¶„ê¸° 4ê°œ ì¶œë ¥ concat â†’ D_MODEL
    - íŒ¨ì¹˜ ë‚´ë¶€ì˜ ê¸‰ê²©/ì™„ë§Œ/ì”ì§„ë™ íŒ¨í„´ì„ ë™ì‹œì— í¬ì°©
    """
    def __init__(self, in_features: int, patch_len: int, d_model: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % 4 == 0, "d_modelì€ 4ì˜ ë°°ìˆ˜ê°€ ë˜ì–´ì•¼ ë©€í‹°ìŠ¤ì¼€ì¼ ë¶„ê¸° í•©ì‚°ì´ ë§ìŠµë‹ˆë‹¤."
        out_ch = d_model // 4
    # ì»¤ë„ í¬ê¸°ë¥¼ patch_lenì— ë¹„ë¡€í•˜ê²Œ ì„¤ì •
        self.b2 = nn.Conv1d(in_features, out_ch, kernel_size=1, padding=0)
        self.b3 = nn.Conv1d(in_features, out_ch, kernel_size=3, padding=1)
        self.b5 = nn.Conv1d(in_features, out_ch, kernel_size=5, padding=2)
        self.bd = nn.Conv1d(in_features, out_ch, kernel_size=3, padding=2, dilation=2)

        self.bn   = nn.BatchNorm1d(d_model)
        self.act  = nn.GELU()
        self.pool = nn.AdaptiveAvgPool1d(1)   # (B*P, D, L) â†’ (B*P, D, 1)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        # x: (B, P, L, F)
        B, P, L, F = x.shape
        x = x.view(B*P, L, F).permute(0, 2, 1)        # (B*P, F, L)

        z = torch.cat([self.b2(x), self.b3(x), self.b5(x), self.bd(x)], dim=1)  # (B*P, D, L)
        z = self.act(self.bn(z))
        z = self.pool(z).squeeze(-1)                  # (B*P, D)
        z = self.drop(z)
        return z.view(B, P, -1)                       # (B, P, D)

class TokenConvMixer(nn.Module):
    """
    íŒ¨ì¹˜ í† í° ê°„(P ì¶•) ë¡œì»¬ ì—°ì†ì„± ê°•í™”: DepthwiseConv1d(P-ì¶•) + PointwiseConv1d
    ì…ë ¥/ì¶œë ¥: (B, P, D)
    """
    def __init__(self, d_model: int, dropout: float = 0.1):
        super().__init__()
        self.dw = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)
        self.pw = nn.Conv1d(d_model, d_model, kernel_size=1)
        self.bn = nn.BatchNorm1d(d_model)
        self.act = nn.GELU()
        self.drop = nn.Dropout(dropout)

    def forward(self, z):              # (B, P, D)
        y = z.permute(0, 2, 1)         # (B, D, P)
        y = self.dw(y)
        y = self.pw(y)
        y = self.bn(y)
        y = self.act(y)
        y = self.drop(y)
        y = y.permute(0, 2, 1)         # (B, P, D)
        return z + y                   # Residual

class PositionalEncoding(nn.Module):
    def __init__(self, d_model:int, max_len:int=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).float().unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))
        pe[:,0::2] = torch.sin(position*div)
        if d_model % 2 == 1:
            pe[:,1::2] = torch.cos(position*div)[:, :pe[:,1::2].shape[1]]
        else:
            pe[:,1::2] = torch.cos(position*div)
        self.register_buffer("pe", pe.unsqueeze(0))
    def forward(self, x):
        P = x.size(1)
        return x + self.pe[:, :P, :]

class AttnPool(nn.Module):
    """Learnable-query attention pooling over patch tokens."""
    def __init__(self, d_model:int):
        super().__init__()
        self.q = nn.Parameter(torch.randn(1, 1, d_model))
        self.proj = nn.Linear(d_model, d_model, bias=False)
    def forward(self, z):           # z: (B, P, D)
        B,P,D = z.shape
        q = self.q.expand(B, -1, -1)                       # (B,1,D)
        k = self.proj(z)                                   # (B,P,D)
        attn = torch.softmax((q @ k.transpose(1,2)) / (D**0.5), dim=-1)  # (B,1,P)
        pooled = attn @ z                                  # (B,1,D)
        return pooled.squeeze(1)                           # (B,D)

class PatchTSTModel(nn.Module):
    def __init__(self, in_features:int, patch_len:int, d_model:int, n_heads:int,
                 n_layers:int, ff_dim:int, dropout:float, pred_len:int, head_hidden:List[int]):
        super().__init__()
        # â‘  ë©€í‹°ìŠ¤ì¼€ì¼ CNN íŒ¨ì¹˜ ì„ë² ë”©
        self.embed = MultiScaleCNNPatchEmbed(in_features, patch_len, d_model, dropout=dropout*0.5)
        # â‘¡ íŒ¨ì¹˜ í† í° ê°„ ë¡œì»¬ ì—°ì†ì„± ë¯¹ì„œ
        self.mixer = nn.Sequential(
            TokenConvMixer(d_model, dropout=dropout),
            TokenConvMixer(d_model, dropout=dropout),
        )
        # â‘¢ PatchTST ì¸ì½”ë”
        self.posenc = PositionalEncoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim,
            dropout=dropout, batch_first=True, activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.pool = AttnPool(d_model)

        # â‘£ ì˜ˆì¸¡ í—¤ë“œ
        mlp, in_dim = [], d_model
        for h in head_hidden[:2]:
            mlp += [nn.Linear(in_dim, h), nn.GELU(), nn.Dropout(dropout)]
            in_dim = h
        mlp.append(nn.Linear(in_dim, pred_len))
        self.head = nn.Sequential(*mlp)

    def forward(self, x):
        # x: (B, P, L, F)
        z = self.embed(x)      # (B,P,D)
        z = self.mixer(z)      # (B,P,D)
        z = self.posenc(z)
        z = self.encoder(z)
        z = self.pool(z)       # (B,D)
        return self.head(z)    # (B,H)

    def correlation_loss(pred, true):
    # pred, true: (B, H)
        pred = pred - pred.mean(dim=1, keepdim=True)
        true = true - true.mean(dim=1, keepdim=True)
        corr = (pred * true).sum(dim=1) / (
            (pred.norm(dim=1) * true.norm(dim=1)) + 1e-6
        )
        return 1 - corr.mean()
    # =========================
# helpers
# =========================
def warmup_lr(ep:int, base_lr:float, warmup_epochs:int):
    if ep <= warmup_epochs:
        return base_lr * (ep / max(1, warmup_epochs))
    return base_lr

def batch_mae_in_original_units(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    p = pred_b.detach().cpu().numpy().reshape(-1, 1)
    t = y_b.detach().cpu().numpy().reshape(-1, 1)
    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)
    return float(np.mean(np.abs(p_orig - t_orig)))

def batch_corrcoef(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    """
    Pearson correlation coefficient (batch í‰ê· )
    pred_b, y_b: (B, H)
    """
    p = pred_b.detach().cpu().numpy().reshape(-1, 1)
    t = y_b.detach().cpu().numpy().reshape(-1, 1)
    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)

    if np.std(p_orig) < 1e-6 or np.std(t_orig) < 1e-6:
        return 0.0
    return float(np.corrcoef(p_orig, t_orig)[0,1])

# =========================
# train & evaluate
# =========================
def train_and_eval(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list):
    """
    X: (N,F), y: (N,), feat_names: ['ili', 'vaccine_rate', 'respiratory_index'] ë“±
    """
    set_seed(SEED)
    (s0,e0),(s1,e1),(s2,e2) = make_splits(len(y))
    X_tr, X_va, X_te = X[s0:e0], X[s1:e1], X[s2:e2]
    y_tr, y_va, y_te = y[s0:e0], y[s1:e1], y[s2:e2]
    lab_tr, lab_va, lab_te = labels[s0:e0], labels[s1:e1], labels[s2:e2]

    # ==== Scaling ====
    # Target scaler
    scaler_y = get_scaler()
    y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
    y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
    y_te_sc = scaler_y.transform(y_te.reshape(-1,1)).ravel()

    # Feature scaler (ì…ë ¥ íŠ¹ì§• ì „ì²´)
    scaler_x = get_scaler()
    X_tr_sc = scaler_x.fit_transform(X_tr)
    X_va_sc = scaler_x.transform(X_va)
    X_te_sc = scaler_x.transform(X_te)

    F = X.shape[1]
    print(f"[Shapes] X_tr:{X_tr.shape}, X_va:{X_va.shape}, X_te:{X_te.shape} | F={F}")
    print(f"[Info] Model input feature order -> {feat_names}")

    ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_va = PatchTSTDataset(X_va_sc, y_va_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_te = PatchTSTDataset(X_te_sc, y_te_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)

    # drop_last=False ë¡œ ë³€ê²½(ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ í•™ìŠµ ë°°ì¹˜ ë³´ì¥)
    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)

    model = PatchTSTModel(
        in_features=F, patch_len=PATCH_LEN, d_model=D_MODEL, n_heads=N_HEADS,
        n_layers=ENC_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT,
        pred_len=PRED_LEN, head_hidden=HEAD_HIDDEN
    ).to(DEVICE)

    # Loss / Optim / Scheduler
    crit = nn.HuberLoss(delta=1.0)
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)

    # ---- history for curves ----
    hist = {"train_loss":[], "val_loss":[], "train_mae":[], "val_mae":[]}

    best_val = float("inf"); best_state=None; noimp=0
    printed_batch_info = False
    for ep in range(1, EPOCHS+1):
        # ---- Train ----
        model.train(); tr_loss_sum=0; tr_mae_sum=0; n=0
        # warmup
        for g in opt.param_groups:
            g['lr'] = warmup_lr(ep, LR, WARMUP_EPOCHS)

        for Xb,yb,_ in dl_tr:
            if not printed_batch_info:
                # Xb: (B, P, L, F)  â† ìµœì¢… ëª¨ë¸ ì…ë ¥ í…ì„œ êµ¬ì¡°
                print(f"[Batch] Xb.shape={tuple(Xb.shape)} (B,P,L,F), yb.shape={tuple(yb.shape)}")
                print(f"[Batch] Feature order used -> {feat_names}")
                printed_batch_info = True
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            opt.zero_grad()
            pred = model(Xb)
            loss = crit(pred, yb)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
            bs=yb.size(0)
            tr_loss_sum += loss.item()*bs; n+=bs
            tr_mae_sum  += batch_mae_in_original_units(pred, yb, scaler_y)*bs

        tr_loss = tr_loss_sum / max(1,n)
        tr_mae  = tr_mae_sum  / max(1,n)

        # ---- Validation ----
        model.eval(); va_loss_sum=0; va_mae_sum=0; va_corr_sum = 0; n=0
        with torch.no_grad():
            for Xb,yb,_ in dl_va:
                Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
                pred = model(Xb); loss = crit(pred,yb)
                bs=yb.size(0)
                va_loss_sum += loss.item()*bs; n+=bs
                va_mae_sum  += batch_mae_in_original_units(pred, yb, scaler_y)*bs
                va_corr_sum += batch_corrcoef(pred, yb, scaler_y)*bs
        va_loss = va_loss_sum / max(1,n)
        va_mae  = va_mae_sum  / max(1,n)
        va_corr = va_corr_sum / max(1,n)

        scheduler.step()

        hist["train_loss"].append(tr_loss)
        hist["val_loss"].append(va_loss)
        hist["train_mae"].append(tr_mae)
        hist["val_mae"].append(va_mae)

        print(f"[Epoch {ep:03d}/{EPOCHS}] "
              f"LR={opt.param_groups[0]['lr']:.6f} | "
              f"Loss T/V={tr_loss:.5f}/{va_loss:.5f} | "
              f"MAE  T/V={tr_mae:.5f}/{va_mae:.5f}"
              f"Corr V={va_corr:.3f}")

        if va_loss < best_val - 1e-6:
            best_val = va_loss; noimp=0
            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}
        else:
            noimp += 1
            if noimp >= PATIENCE:
                print(f"Early stopping after {ep} epochs (no improvement {PATIENCE}).")
                break

    if best_state is not None:
        model.load_state_dict({k:v.to(DEVICE) for k,v in best_state.items()})

    # ---- Test & Metrics ----
    model.eval(); preds=[]; trues=[]; starts=[]
    with torch.no_grad():
        for Xb,yb,i0 in dl_te:
            Xb=Xb.to(DEVICE)
            preds.append(model(Xb).detach().cpu().numpy())
            trues.append(yb.numpy())
            starts.append(i0.numpy())
    yhat_sc = np.concatenate(preds,axis=0)
    ytrue_sc= np.concatenate(trues,axis=0)
    starts  = np.concatenate(starts,axis=0)

    # inverse scale (target only)
    yhat  = scaler_y.inverse_transform(yhat_sc.reshape(-1,1)).reshape(-1,PRED_LEN)
    ytrue = scaler_y.inverse_transform(ytrue_sc.reshape(-1,1)).reshape(-1,PRED_LEN)

    mse  = float(np.mean((yhat-ytrue)**2))
    rmse = float(np.sqrt(mse))
    mae  = float(np.mean(np.abs(yhat-ytrue)))
    print("\n=== Final Test Metrics ===")
    print(f"MSE : {mse:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"MAE : {mae:.6f}")

    # =========================
    # Save per-window predictions
    # =========================
    cols_true = [f"true_t+{i}" for i in range(1,PRED_LEN+1)]
    cols_pred = [f"pred_t+{i}" for i in range(1,PRED_LEN+1)]
    out = pd.DataFrame(np.hstack([ytrue, yhat]), columns=cols_true+cols_pred)
    out.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")
    print(f"Saved predictions -> {OUT_CSV}")

    # =========================
    # Plot_1: last window (H-step ahead)
    # =========================
    last_true = ytrue[-1]; last_pred = yhat[-1]
    weeks = np.arange(1, PRED_LEN+1)
    plt.figure(figsize=(10,4))
    plt.plot(weeks, last_true, label="Truth (last window)", linewidth=2)
    plt.plot(weeks, last_pred, label="Prediction (last window)", linewidth=2)
    plt.title("Last Test Window: Truth vs Prediction")
    plt.xlabel("Horizon (weeks ahead)")
    plt.ylabel("ILI per 1,000 Population")
    plt.grid(True); plt.legend()
    plt.tight_layout(); plt.savefig(PLOT_LAST_WINDOW, dpi=150)
    print(f"Saved plot -> {PLOT_LAST_WINDOW}")

    # =========================
    # Plot_2: test reconstruction (val-context included)
    # =========================
    context = y_va_sc[-SEQ_LEN:]                       # í‘œì¤€í™” ì»¨í…ìŠ¤íŠ¸
    y_ct_sc = np.concatenate([context, y_te_sc])       # [SEQ_LEN + test_len]
    # ì…ë ¥ íŠ¹ì§•ë„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨í•´ ì¬êµ¬ì„± í•„ìš” â†’ Xë„ ë™ì¼í•˜ê²Œ ë¶™ì—¬ì„œ ì˜ˆì¸¡
    X_ct_sc = np.concatenate([X_va_sc[-SEQ_LEN:], X_te_sc], axis=0)
    ds_ct = PatchTSTDataset(X_ct_sc, y_ct_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    dl_ct = DataLoader(ds_ct, batch_size=BATCH_SIZE, shuffle=False)

    model.eval(); preds_ct=[]; starts_ct=[]
    with torch.no_grad():
        for Xb, _, i0 in dl_ct:
            Xb = Xb.to(DEVICE)
            preds_ct.append(model(Xb).detach().cpu().numpy())  # (B, H)
            starts_ct.append(i0.numpy())
    yhat_ct_sc = np.concatenate(preds_ct, axis=0)
    starts_ct  = np.concatenate(starts_ct, axis=0)
    yhat_ct = scaler_y.inverse_transform(yhat_ct_sc.reshape(-1,1)).reshape(-1, PRED_LEN)

    test_len = len(y_te)
    recon_sum   = np.zeros(test_len)
    recon_count = np.zeros(test_len)
    h_weights = np.linspace(RECON_W_START, RECON_W_END, PRED_LEN)

    for k, s in enumerate(starts_ct):
        pos0_ct = int(s) + SEQ_LEN   # [context+test] ì¶•
        pos0_te = pos0_ct - SEQ_LEN  # test ì¶•ìœ¼ë¡œ ë³€í™˜
        for j in range(PRED_LEN):
            idx = pos0_te + j
            if 0 <= idx < test_len:
                w = h_weights[j]
                recon_sum[idx]   += yhat_ct[k, j] * w
                recon_count[idx] += w

    recon = np.where(recon_count > 0, recon_sum / np.maximum(1, recon_count), np.nan)

    truth_test = y_te
    x_labels = lab_te
    tick_step = max(1, test_len // 12)
    tick_idx  = list(range(0, test_len, tick_step))
    if tick_idx[-1] != test_len-1:
        tick_idx.append(test_len-1)
    tick_text = [x_labels[i] for i in tick_idx]

    plt.figure(figsize=(12,5))
    plt.plot(range(test_len), truth_test, linewidth=2, label="Truth (test segment)")
    plt.plot(range(test_len), recon,      linewidth=2, label="Prediction (overlap-avg, weighted)")
    plt.title("Test Range: Truth vs Overlap-averaged Prediction (with context)")
    plt.xlabel("Season - Week"); plt.ylabel("ILI per 1,000 Population")
    plt.xticks(tick_idx, tick_text, rotation=45, ha="right")
    plt.grid(True); plt.legend()
    plt.tight_layout(); plt.savefig(PLOT_TEST_RECON, dpi=150)
    print(f"Saved plot -> {PLOT_TEST_RECON}")

    # =========================
    # Plot_3: Train/Val MAE curves
    # =========================
    xs = np.arange(1, len(hist["train_mae"])+1)
    plt.figure(figsize=(10,4))
    plt.plot(xs, hist["train_mae"], linewidth=2, label="Train MAE (original units)")
    plt.plot(xs, hist["val_mae"],   linewidth=2, label="Val MAE (original units)")
    plt.title("Training Curves: MAE per epoch (lower is better)")
    plt.xlabel("Epoch")
    plt.ylabel("MAE (ILI per 1,000)")
    plt.grid(True); plt.legend()
    plt.tight_layout(); plt.savefig(PLOT_MA_CURVES, dpi=150)
    print(f"Saved plot -> {PLOT_MA_CURVES}")


# =========================
# run
# =========================
if __name__ == "__main__":
    print("\n" + "ğŸš€ " * 30)
    print("ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("ğŸš€ " * 30 + "\n")
    
    # API ë˜ëŠ” CSVì—ì„œ ë°ì´í„° ë¡œë“œ
    # í™˜ê²½ë³€ìˆ˜ USE_API=trueë¡œ ì„¤ì •í•˜ë©´ API ì‚¬ìš©, ì•„ë‹ˆë©´ CSV ì‚¬ìš©
    USE_API_MODE = os.getenv('USE_API', 'false').lower() == 'true'
    
    if USE_API_MODE:
        print("=" * 60)
        print("ğŸŒ API ëª¨ë“œ: Pythonì—ì„œ ì§ì ‘ GFID API í˜¸ì¶œ")
        print("=" * 60)
        
        # Pythonì—ì„œ ì§ì ‘ Keycloak ì¸ì¦ í›„ GFID API í˜¸ì¶œ
        df = fetch_data_directly_from_gfid()
        
        print("\n" + "âœ… " * 30)
        print("API ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        print("âœ… " * 30 + "\n")
        
        # ë°ì´í„° í™•ì¸
        print(f"ğŸ“Š DataFrame ì •ë³´:")
        print(f"   - Shape: {df.shape}")
        print(f"   - Columns: {list(df.columns)}")
        print(f"\nì²˜ìŒ 5ê°œ í–‰:")
        print(df.head())
        print(f"\në°ì´í„° íƒ€ì…:")
        print(df.dtypes)
        
        print(f"\nğŸ”§ USE_EXOG = '{USE_EXOG}'  (auto-detects vaccine/resp columns)")
        
        # DataFrameì„ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì „ì²˜ë¦¬
        print("\nğŸ“ˆ ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        X, y, labels, feat_names = load_and_prepare(df=df, use_exog=USE_EXOG)
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   - Data points: {len(y)}")
        print(f"   - Features used ({len(feat_names)}): {feat_names}")
        
    else:
        print("=" * 60)
        print("ğŸ“ CSV ëª¨ë“œ: ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        print("=" * 60)
        
        if CSV_PATH is None:
            raise FileNotFoundError("CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. USE_API=trueë¡œ ì„¤ì •í•˜ê±°ë‚˜ CSV íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”.")
        
        print(f"   - CSV íŒŒì¼: {CSV_PATH.name}")
        print(f"   - Device: {DEVICE}")
        print(f"   - USE_EXOG: '{USE_EXOG}'")
        
        print("\nğŸ“ˆ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
        X, y, labels, feat_names = load_and_prepare(CSV_PATH, USE_EXOG)
        print(f"âœ… CSV ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   - Data points: {len(y)}")
        print(f"   - Features used ({len(feat_names)}): {feat_names}")
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    print("\n" + "ğŸ¯ " * 30)
    print("ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("ğŸ¯ " * 30 + "\n")
    train_and_eval(X, y, labels, feat_names)

    # =========================
# Feature Importance utils
# =========================
def _eval_mae_on_split(model, X_split_sc, y_split_sc, scaler_y, feat_names, 
                       seq_len=SEQ_LEN, pred_len=PRED_LEN, patch_len=PATCH_LEN, stride=STRIDE,
                       batch_size=BATCH_SIZE):
    """í˜„ì¬ ëª¨ë¸ë¡œ í•œ ë¶„í• (va/test) ì„¸íŠ¸ì—ì„œ MAE(ì› ë‹¨ìœ„) ê³„ì‚°"""
    ds = PatchTSTDataset(X_split_sc, y_split_sc, seq_len, pred_len, patch_len, stride)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)
    model.eval()
    mae_sum, n = 0.0, 0
    with torch.no_grad():
        for Xb, yb, _ in dl:
            Xb = Xb.to(DEVICE); yb = yb.to(DEVICE)
            pred = model(Xb)  # (B, H)
            mae_sum += batch_mae_in_original_units(pred, yb, scaler_y) * yb.size(0)
            n += yb.size(0)
    return float(mae_sum / max(1, n))


def compute_feature_importance(model, 
                               X_va_sc, y_va_sc, 
                               X_te_sc=None, y_te_sc=None,
                               scaler_y=None, feat_names=None, 
                               random_state=42):
    """
    í¼ë®¤í…Œì´ì…˜(ì—´ ì„ê¸°) ì¤‘ìš”ë„ì™€ í‰ê·  ëŒ€ì²´(ê·¸ íŠ¹ì§•ì„ í‰ê· ìœ¼ë¡œ ê³ ì •) ì¤‘ìš”ë„ë¥¼ ê³„ì‚°.
    ë°˜í™˜: ì¤‘ìš”ë„ DataFrame (Î”MAEê°€ í´ìˆ˜ë¡ ì¤‘ìš”)
    """
    assert scaler_y is not None and feat_names is not None
    rng = np.random.RandomState(random_state)

    # --- ê¸°ì¤€ì„ (baseline MAE) ---
    baseline_val = _eval_mae_on_split(model, X_va_sc, y_va_sc, scaler_y, feat_names)
    print(f"[FI] Baseline Val MAE: {baseline_val:.6f}")

    baseline_tst = None
    if X_te_sc is not None and y_te_sc is not None:
        baseline_tst = _eval_mae_on_split(model, X_te_sc, y_te_sc, scaler_y, feat_names)
        print(f"[FI] Baseline Test MAE: {baseline_tst:.6f}")

    perm_deltas_val, mean_deltas_val = [], []
    perm_deltas_tst, mean_deltas_tst = [], []

    for j, name in enumerate(feat_names):
        # â‘  í¼ë®¤í…Œì´ì…˜(ì—´ ì„ê¸°)
        Xp = X_va_sc.copy()
        col = Xp[:, j].copy()
        rng.shuffle(col)
        Xp[:, j] = col
        mae_perm_val = _eval_mae_on_split(model, Xp, y_va_sc, scaler_y, feat_names)
        perm_deltas_val.append(mae_perm_val - baseline_val)

        # â‘¡ í‰ê·  ëŒ€ì²´(íŠ¹ì§• ì œê±° íš¨ê³¼)
        Xz = X_va_sc.copy()
        Xz[:, j] = X_va_sc[:, j].mean()
        mae_mean_val = _eval_mae_on_split(model, Xz, y_va_sc, scaler_y, feat_names)
        mean_deltas_val.append(mae_mean_val - baseline_val)

        if X_te_sc is not None and y_te_sc is not None:
            Xp_te = X_te_sc.copy()
            col_te = Xp_te[:, j].copy()
            rng.shuffle(col_te)
            Xp_te[:, j] = col_te
            mae_perm_tst = _eval_mae_on_split(model, Xp_te, y_te_sc, scaler_y, feat_names)
            perm_deltas_tst.append(mae_perm_tst - baseline_tst)

            Xz_te = X_te_sc.copy()
            Xz_te[:, j] = X_te_sc[:, j].mean()
            mae_mean_tst = _eval_mae_on_split(model, Xz_te, y_te_sc, scaler_y, feat_names)
            mean_deltas_tst.append(mae_mean_tst - baseline_tst)

    # DataFrame ìƒì„±
    df_fi = pd.DataFrame({
        "feature": feat_names,
        "perm_delta_val": perm_deltas_val,
        "mean_delta_val": mean_deltas_val,
    })
    if X_te_sc is not None and y_te_sc is not None:
        df_fi["perm_delta_tst"] = perm_deltas_tst
        df_fi["mean_delta_tst"] = mean_deltas_tst

    # í‰ê·  ë¸íƒ€ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    df_fi = df_fi.sort_values("mean_delta_val", ascending=False).reset_index(drop=True)
    return df_fi

def plot_feature_importance(fi_df, out_csv=None, out_png=None):
    """
    Feature Importanceë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    """
    if fi_df is None or len(fi_df) == 0:
        print("No feature importance data to plot.")
        return

    import matplotlib.pyplot as plt

    # CSV ì €ì¥
    if out_csv:
        fi_df.to_csv(out_csv, index=False)
        print(f"Feature Importance saved to {out_csv}")

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # â‘  Permutation Î” (Val)
    axes[0].barh(fi_df["feature"], fi_df["perm_delta_val"], color="steelblue")
    axes[0].set_xlabel("Î”MAE (Permutation, Val)")
    axes[0].set_title("Permutation Feature Importance (Val)")
    axes[0].invert_yaxis()

    # â‘¡ Mean Replacement Î” (Val)
    axes[1].barh(fi_df["feature"], fi_df["mean_delta_val"], color="coral")
    axes[1].set_xlabel("Î”MAE (Mean Replacement, Val)")
    axes[1].set_title("Mean Replacement Feature Importance (Val)")
    axes[1].invert_yaxis()

    plt.tight_layout()

    if out_png:
        plt.savefig(out_png, dpi=150, bbox_inches="tight")
        print(f"Feature Importance plot saved to {out_png}")
    plt.show()


# =========================
# train_and_eval (main)
# =========================
def train_and_eval(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list,
                   compute_fi=False, save_fi=False):
    """
    í†µí•© í•™ìŠµ + í‰ê°€ í•¨ìˆ˜.
    compute_fi=True -> feature importance ê³„ì‚°
    save_fi=True -> CSV/plot ì €ì¥
    """
    torch.manual_seed(SEED); np.random.seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
    print(f"[Config] EPOCHS:{EPOCHS}, BATCH_SIZE:{BATCH_SIZE}, SEQ_LEN:{SEQ_LEN}, PRED_LEN:{PRED_LEN}")
    print(f"[Config] PATCH_LEN:{PATCH_LEN}, STRIDE:{STRIDE}, LR:{LR}, Warmup:{WARMUP_EPOCHS}, Patience:{PATIENCE}")

    N = len(y)
    split_tr = int(0.7*N); split_va = int(0.85*N)
    X_tr, y_tr = X[:split_tr], y[:split_tr]
    X_va, y_va = X[split_tr:split_va], y[split_tr:split_va]
    X_te, y_te = X[split_va:], y[split_va:]

    def get_scaler():
        st = SCALER_TYPE.lower()
        if st=="robust": return RobustScaler()
        if st=="minmax": return MinMaxScaler()
        return StandardScaler()

    scaler_y = get_scaler()
    y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
    y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
    y_te_sc = scaler_y.transform(y_te.reshape(-1,1)).ravel()

    scaler_x = get_scaler()
    X_tr_sc = scaler_x.fit_transform(X_tr)
    X_va_sc = scaler_x.transform(X_va)
    X_te_sc = scaler_x.transform(X_te)

    F = X.shape[1]
    print(f"[Shapes] X_tr:{X_tr.shape}, X_va:{X_va.shape}, X_te:{X_te.shape} | F={F}")
    print(f"[Info] Model input feature order -> {feat_names}")

    ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_va = PatchTSTDataset(X_va_sc, y_va_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_te = PatchTSTDataset(X_te_sc, y_te_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)

    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)

    model = PatchTSTModel(
        in_features=F, patch_len=PATCH_LEN, d_model=D_MODEL, n_heads=N_HEADS,
        n_layers=ENC_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT,
        pred_len=PRED_LEN, head_hidden=HEAD_HIDDEN
    ).to(DEVICE)

    crit = nn.HuberLoss(delta=1.0)
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)

    hist = {"train_loss":[], "val_loss":[], "train_mae":[], "val_mae":[]}

    best_val = float("inf"); best_state=None; noimp=0
    printed_batch_info = False
    for ep in range(1, EPOCHS+1):
        model.train(); tr_loss_sum=0; tr_mae_sum=0; n=0
        for g in opt.param_groups:
            g['lr'] = warmup_lr(ep, LR, WARMUP_EPOCHS)

        for Xb, yb, _ in dl_tr:
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            if not printed_batch_info:
                print(f"[Batch shapes] Xb:{Xb.shape}, yb:{yb.shape}")
                printed_batch_info=True
            opt.zero_grad()
            pred=model(Xb)
            loss=crit(pred,yb)
            loss.backward(); opt.step()

            tr_loss_sum += loss.item()*yb.size(0)
            tr_mae_sum += batch_mae_in_original_units(pred, yb, scaler_y)*yb.size(0)
            n+=yb.size(0)

        tr_loss_avg = tr_loss_sum/max(1,n)
        tr_mae_avg  = tr_mae_sum/max(1,n)

        model.eval(); va_loss_sum=0; va_mae_sum=0; m=0
        with torch.no_grad():
            for Xb,yb,_ in dl_va:
                Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
                pred=model(Xb)
                loss=crit(pred,yb)
                va_loss_sum += loss.item()*yb.size(0)
                va_mae_sum  += batch_mae_in_original_units(pred,yb,scaler_y)*yb.size(0)
                m+=yb.size(0)
        va_loss_avg=va_loss_sum/max(1,m)
        va_mae_avg =va_mae_sum/max(1,m)

        hist["train_loss"].append(tr_loss_avg)
        hist["val_loss"].append(va_loss_avg)
        hist["train_mae"].append(tr_mae_avg)
        hist["val_mae"].append(va_mae_avg)

        if ep<=5 or ep%5==0:
            print(f"Epoch {ep:3d}/{EPOCHS} | TrL:{tr_loss_avg:.6f} TrMAE:{tr_mae_avg:.6f} | VaL:{va_loss_avg:.6f} VaMAE:{va_mae_avg:.6f}")

        if va_mae_avg < best_val:
            best_val = va_mae_avg
            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}
            noimp=0
        else:
            noimp+=1
            if noimp>=PATIENCE:
                print(f"Early stop at epoch {ep} (no improvement for {PATIENCE} epochs)")
                break

        scheduler.step()

    if best_state is not None:
        model.load_state_dict(best_state)
    print(f"Best Val MAE: {best_val:.6f}")

    # Test
    model.eval(); te_mae_sum=0; k=0
    with torch.no_grad():
        for Xb,yb,_ in dl_te:
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            pred=model(Xb)
            te_mae_sum += batch_mae_in_original_units(pred,yb,scaler_y)*yb.size(0)
            k+=yb.size(0)
    te_mae_avg = te_mae_sum/max(1,k)
    print(f"Test MAE (original units): {te_mae_avg:.6f}")

    # Plot curves
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(hist["train_mae"],label="Train MAE")
    plt.plot(hist["val_mae"],label="Val MAE")
    plt.xlabel("Epoch"); plt.ylabel("MAE (original units)"); plt.legend(); plt.title("MAE curves")
    plt.subplot(1,2,2)
    plt.plot(hist["train_loss"],label="Train Loss")
    plt.plot(hist["val_loss"],label="Val Loss")
    plt.xlabel("Epoch"); plt.ylabel("Huber Loss"); plt.legend(); plt.title("Loss curves")
    plt.tight_layout()
    plt.savefig(PLOT_MA_CURVES, dpi=150)
    print(f"MAE/loss curves saved to {PLOT_MA_CURVES}")
    plt.show()

    # Last window
    last_seq_idx = len(y_te_sc) - SEQ_LEN
    if last_seq_idx>=0:
        seq = X_te_sc[last_seq_idx:last_seq_idx+SEQ_LEN]
        seq_t = torch.from_numpy(seq).unsqueeze(0).float().to(DEVICE)
        with torch.no_grad():
            p=model(seq_t).cpu().numpy().ravel()
        p_orig = scaler_y.inverse_transform(p.reshape(-1,1)).ravel()
        y_true_last = scaler_y.inverse_transform(y_te_sc[last_seq_idx+SEQ_LEN:last_seq_idx+SEQ_LEN+PRED_LEN].reshape(-1,1)).ravel()
        plt.figure(figsize=(8,4))
        plt.plot(range(len(y_true_last)), y_true_last, marker='o', label="True")
        plt.plot(range(len(p_orig)), p_orig, marker='x', label="Pred")
        plt.xlabel("Future step (horizon)"); plt.ylabel("ILI")
        plt.title(f"Last window prediction (SEQ_LEN={SEQ_LEN}, PRED_LEN={PRED_LEN})")
        plt.legend(); plt.grid(True, alpha=0.3)
        plt.savefig(PLOT_LAST_WINDOW, dpi=150)
        print(f"Last window plot saved to {PLOT_LAST_WINDOW}")
        plt.show()

    # Test reconstruction
    all_p_te = []
    model.eval()
    with torch.no_grad():
        for Xb,_,_ in dl_te:
            Xb=Xb.to(DEVICE)
            p_b=model(Xb).cpu().numpy()
            all_p_te.append(p_b)
    all_p_te = np.concatenate(all_p_te, axis=0)
    pred_orig = scaler_y.inverse_transform(all_p_te).ravel()
    y_te_orig = scaler_y.inverse_transform(y_te_sc.reshape(-1,1)).ravel()

    plt.figure(figsize=(12,5))
    plt.plot(y_te_orig, label="True", alpha=0.7)
    plt.plot(pred_orig[:len(y_te_orig)], label="Pred", alpha=0.7)
    plt.xlabel("Test set index"); plt.ylabel("ILI")
    plt.title("Test set reconstruction (multi-step predictions)")
    plt.legend(); plt.grid(True, alpha=0.3)
    plt.savefig(PLOT_TEST_RECON, dpi=150)
    print(f"Test reconstruction plot saved to {PLOT_TEST_RECON}")
    plt.show()

    # Feature importance
    fi_df = None
    if compute_fi:
        print("\n[Computing Feature Importance...]")
        fi_df = compute_feature_importance(
            model, X_va_sc, y_va_sc, X_te_sc, y_te_sc,
            scaler_y, feat_names, random_state=SEED
        )
        print("\n[Feature Importance (sorted by mean_delta_val)]")
        print(fi_df.to_string(index=False))

        if save_fi:
            plot_feature_importance(
                fi_df,
                out_csv=str(BASE_DIR / "feature_importance.csv"),
                out_png=str(BASE_DIR / "feature_importance.png")
            )

    # ë°˜í™˜: ì™¸ë¶€ ì…€ì—ì„œ ì¬í™œìš© ê°€ëŠ¥í•˜ë„ë¡
    return model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df

# =========================
# ì‹¤í–‰ë¶€ (ê²°ê³¼ ì¶œë ¥)
# =========================
if __name__ == "__main__":
    model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df = train_and_eval(
        X, y, labels, feat_names,
        compute_fi=True,
        save_fi=True
    )

    print("\n=== [ê²°ê³¼ ìš”ì•½] ===")
    print(f"Feature ê°œìˆ˜: {len(feat_names)}")
    if fi_df is not None:
        print("\n[Top 10 Feature Importance]")
        print(fi_df.head(10).to_string(index=False))
    else:
        print("Feature Importance ê³„ì‚°ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        
