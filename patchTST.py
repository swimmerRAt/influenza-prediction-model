import math
from pathlib import Path
from typing import List, Tuple, Optional
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from dotenv import load_dotenv

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)
import platform
if platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
else:  # Linux
    plt.rcParams['font.family'] = 'NanumGothic'

plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# Optuna for hyperparameter optimization
try:
    import optuna
    from optuna.trial import Trial
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna not installed. Hyperparameter optimization disabled.")
    print("   Install with: pip install optuna")

# PostgreSQL for efficient data loading
from database.db_utils import TimeSeriesDB, load_from_postgres

# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# =========================
print("=" * 60)
print("ğŸ” í™˜ê²½ë³€ìˆ˜ ë¡œë“œ")
print("=" * 60)

# .env íŒŒì¼ ê²½ë¡œ í™•ì¸
env_path = Path.cwd() / '.env'
print(f"1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
print(f"2. .env íŒŒì¼ ê²½ë¡œ: {env_path}")
print(f"3. .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {env_path.exists()}")

# .env íŒŒì¼ ë¡œë“œ
load_result = load_dotenv(env_path, verbose=True, override=True)
print(f"4. .env ë¡œë“œ ê²°ê³¼: {load_result}")
print("=" * 60 + "\n")

# =========================
# Paths & device
# =========================
BASE_DIR = Path.cwd()

# CSV íŒŒì¼ í›„ë³´ ê²½ë¡œ
CANDIDATE_CSVS = [
    BASE_DIR / "data" / "merged" / "merged_influenza_data.csv",
    BASE_DIR / "merged_influenza_data.csv",
    BASE_DIR / "data" / "merged_influenza_data.csv",
]


# =========================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (PostgreSQL)
# =========================
def load_data_from_postgres():
    """
    PostgreSQLì—ì„œ ì¸í”Œë£¨ì—”ì ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    Returns:
        pd.DataFrame: ë¡œë“œëœ ë°ì´í„°
    """
    print("\nğŸ“Š ë°ì´í„° ë¡œë“œ: PostgreSQLì—ì„œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
    try:
        df = load_from_postgres(table_name="influenza_data")
        print(f"âœ… PostgreSQL influenza_data ë¡œë“œ ì™„ë£Œ: {df.shape}")
        return df
    except Exception as e:
        print(f"âŒ PostgreSQL ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def load_weather_data_from_postgres():
    """
    PostgreSQLì—ì„œ ë‚ ì”¨ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    Returns:
        pd.DataFrame: ë¡œë“œëœ ë‚ ì”¨ ë°ì´í„° (year, week, min_temp, max_temp, avg_humidity)
    """
    print("\nğŸŒ¡ï¸  ë‚ ì”¨ ë°ì´í„° ë¡œë“œ: PostgreSQL weather_data í…Œì´ë¸”")
    try:
        db = TimeSeriesDB()
        db.connect()
        df_weather = db.load_data(table_name="weather_data")
        db.close()
        print(f"âœ… PostgreSQL weather_data ë¡œë“œ ì™„ë£Œ: {df_weather.shape}")
        print(f"   - ì»¬ëŸ¼: {list(df_weather.columns)}")
        return df_weather
    except Exception as e:
        print(f"âš ï¸  ë‚ ì”¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"   weather_data í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

def merge_weather_with_influenza(df_influenza, df_weather):
    """
    ì¸í”Œë£¨ì—”ì ë°ì´í„°ì™€ ë‚ ì”¨ ë°ì´í„°ë¥¼ year, week ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    
    Parameters:
        df_influenza: ì¸í”Œë£¨ì—”ì ë°ì´í„°
        df_weather: ë‚ ì”¨ ë°ì´í„°
    
    Returns:
        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°
    """
    print(f"\nğŸ”— ë°ì´í„° ë³‘í•©: influenza_data + weather_data")
    print(f"   - ë³‘í•© ê¸°ì¤€: year, week")
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ í™•ì¸
    df_influenza['year'] = pd.to_numeric(df_influenza['year'], errors='coerce')
    df_influenza['week'] = pd.to_numeric(df_influenza['week'], errors='coerce')
    df_weather['year'] = pd.to_numeric(df_weather['year'], errors='coerce')
    df_weather['week'] = pd.to_numeric(df_weather['week'], errors='coerce')
    
    # LEFT JOIN (influenza_data ê¸°ì¤€)
    df_merged = pd.merge(
        df_influenza,
        df_weather,
        on=['year', 'week'],
        how='left'
    )
    
    print(f"   âœ… ë³‘í•© ì™„ë£Œ:")
    print(f"      - influenza_data í–‰ ìˆ˜: {len(df_influenza)}")
    print(f"      - weather_data í–‰ ìˆ˜: {len(df_weather)}")
    print(f"      - ë³‘í•© í›„ í–‰ ìˆ˜: {len(df_merged)}")
    
    # ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼ í™•ì¸
    new_cols = [c for c in df_weather.columns if c not in df_influenza.columns and c not in ['year', 'week']]
    if new_cols:
        print(f"      - ì¶”ê°€ëœ ë‚ ì”¨ ì»¬ëŸ¼: {new_cols}")
    
    return df_merged

def pick_csv_path():
    for p in CANDIDATE_CSVS:
        if p.exists():
            return p
    raise FileNotFoundError("No input CSV found among:\n" + "\n".join(map(str, CANDIDATE_CSVS)))



def pick_device():
    if torch.cuda.is_available():
        return "cuda"
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

DEVICE = pick_device()
SEED   = 42

print(f"ğŸ–¥ï¸ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {DEVICE}")
print(f"ğŸ² ëœë¤ ì‹œë“œ: {SEED}\n")


# =========================
# Configuration - ëª¨ë“  ì„¤ì •ì„ ì—¬ê¸°ì„œ ê´€ë¦¬
# =========================

class Config:
    """ëª¨ë¸ ì„¤ì • í†µí•© ê´€ë¦¬"""
    
    # ===== Optuna ìµœì í™” ì„¤ì • =====
    USE_OPTUNA = False       # Optuna ìµœì í™” ì‹¤í–‰
    N_TRIALS = 50          # Optuna ìµœì í™” ì‹œë„ íšŸìˆ˜
    OPTUNA_TIMEOUT = None   # ìµœì í™” ì‹œê°„ ì œí•œ (ì´ˆ), Noneì´ë©´ ë¬´ì œí•œ
    
    # Optuna ìµœì í™” ë²”ìœ„ (USE_OPTUNA=Trueì¼ ë•Œ ì‚¬ìš©)
    OPTUNA_SEARCH_SPACE = {
        'd_model': [64, 128, 256],  # n_headsì˜ ë°°ìˆ˜ë¡œ ì„¤ì •
        'n_heads': [2, 4, 8, 16],       # Attention head ê°œìˆ˜
        'enc_layers': (2, 8),       # Encoder ë ˆì´ì–´ ê°œìˆ˜ (ë²”ìœ„ í™•ì¥)
        'ff_dim': [64, 96, 128, 192, 256, 384, 512],  # Feed-forward ì°¨ì› (ë” ë§ì€ ê°’ ì¶”ê°€)
        'dropout': (0.05, 0.5),                       # Dropout ë¹„ìœ¨ (ë²”ìœ„ í™•ì¥)
        'lr': (1e-6, 1e-2),                           # Learning rate (ë²”ìœ„ í™•ì¥, log scale)
        'weight_decay': (1e-6, 1e-2),                 # Weight decay (ë²”ìœ„ í™•ì¥, log scale)
        'batch_size': [16, 32, 48, 64, 96, 128],      # Batch size (ë” ì„¸ë°€í•œ ê°’ ì¶”ê°€)
        'seq_len': (8, 30),       # Input sequence length (ì„¸ë°€í™”)
        # pred_lenì€ Config.PRED_LEN ì‚¬ìš© (Optunaì—ì„œ ì œì™¸)
        'patch_len': [2, 3, 4, 5, 6],                 # Patch length (ë²”ìœ„ í™•ì¥)
    }
    
    # ===== ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’) =====
    # Optunaë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ ë˜ëŠ” ìµœì í™” í›„ ê³ ì •ê°’ìœ¼ë¡œ ì‚¬ìš©
    EPOCHS = 200
    BATCH_SIZE = 64
    SEQ_LEN = 16            # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê³¼ê±° ëª‡ ì£¼)
    PRED_LEN = 4            # ì˜ˆì¸¡ ê¸¸ì´ (ë¯¸ë˜ ëª‡ ì£¼) â€” ê¸°ë³¸: 4ì£¼(í•œ ë‹¬)
    PATCH_LEN = 4           # CNN íŒ¨ì¹˜ ê¸¸ì´
    STRIDE = 1              # íŒ¨ì¹˜ ìŠ¤íŠ¸ë¼ì´ë“œ
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜
    D_MODEL = 128           # ëª¨ë¸ ì°¨ì› (4ì˜ ë°°ìˆ˜ í•„ìˆ˜)
    N_HEADS = 2             # Attention head ê°œìˆ˜
    ENC_LAYERS = 4          # Encoder ë ˆì´ì–´ ê°œìˆ˜
    FF_DIM = 128            # Feed-forward ì°¨ì›
    DROPOUT = 0.3           # Dropout ë¹„ìœ¨
    HEAD_HIDDEN = [64, 64]  # Prediction head hidden layers
    
    # ===== í•™ìŠµ ì„¤ì • =====
    LR = 5e-4               # Learning rate
    WEIGHT_DECAY = 5e-4     # Weight decay (L2 regularization)
    PATIENCE = 60           # Early stopping patience
    WARMUP_EPOCHS = 30      # Learning rate warmup epochs
    
    # ===== Loss í•¨ìˆ˜ ì„¤ì • =====
    PEAK_THRESHOLD_QUANTILE = 0.85  # í”¼í¬ ê¸°ì¤€ (ìƒìœ„ 15% - ë” ë†’ì€ í”¼í¬ë§Œ ì§‘ì¤‘)
    PEAK_WEIGHT_ALPHA = 12.0        # í”¼í¬ êµ¬ê°„ ê°€ì¤‘ì¹˜ (8.0 â†’ 12.0ìœ¼ë¡œ ìƒí–¥, peak ì–¸ë”ìŠˆíŒ… ê°ì†Œ)
    AMPLITUDE_WEIGHT_BETA = 0.6     # ì§„í­ ë³´ì¡´ í•­ ê°€ì¤‘ì¹˜ (0.3 â†’ 0.6, 3~4ì£¼ í›„ ì˜ˆì¸¡ê°’ ìƒí–¥)
    
    # Horizon Weighting (ì˜ˆì¸¡ êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜)
    HORIZON_WEIGHT_MODE = "exponential"  # "exponential", "tail_boost", "uniform"
    HORIZON_EXP_SCALE = 2.0              # exponential ëª¨ë“œ ìŠ¤ì¼€ì¼ (1.2 â†’ 2.0)
    HORIZON_TAIL_BOOST = 2.5             # tail_boost ëª¨ë“œ: ë’¤ìª½ ê°€ì¤‘ì¹˜ ë°°ìˆ˜
    HORIZON_TAIL_COUNT = 2               # tail_boost ëª¨ë“œ: ë’¤ìª½ ëª‡ ê°œ
    
    # ===== ë°ì´í„° ì„¤ì • =====
    TRAIN_RATIO = 0.7       # Train ë°ì´í„° ë¹„ìœ¨
    VAL_RATIO = 0.15        # Validation ë°ì´í„° ë¹„ìœ¨ (Test = 1 - TRAIN - VAL)
    SCALER_TYPE = "robust"  # Scaler íƒ€ì…: "standard", "robust", "minmax"
    
    # Log ë³€í™˜ ì„¤ì • (í”¼í¬ ì˜ˆì¸¡ í–¥ìƒ)
    USE_LOG_TRANSFORM = True  # íƒ€ê²Ÿ ë³€ìˆ˜ì— log(1+x) ë³€í™˜ ì ìš©
    LOG_EPSILON = 0.000001         # log(x + epsilon)ì˜ epsilon ê°’
    
    # ì™¸ìƒ íŠ¹ì§• ì‚¬ìš© ëª¨ë“œ
    # "auto": ìë™ ê°ì§€, "none": ì‚¬ìš© ì•ˆí•¨, "vax": ë°±ì‹ ë¥ ë§Œ, 
    # "resp": í˜¸í¡ê¸°ì§€ìˆ˜ë§Œ, "both": ë‘˜ ë‹¤, "all": ëª¨ë“  íŠ¹ì§•
    USE_EXOG = "all"
    INCLUDE_SEASONAL_FEATS = True  # week_sin í¬í•¨ ì—¬ë¶€
    
    # ===== ì—°ë ¹ëŒ€ë³„ ë™í•™ ì„¤ì • =====
    USE_AGE_GROUP_DYNAMICS = False  # ì–´ë¦°ì´ ì§‘ë‹¨ ILIë¥¼ ì™¸ìƒ ë³€ìˆ˜ë¡œ ì‚¬ìš© (í˜„ì¬ ë¹„í™œì„±í™”)
    # ì£¼ì˜: "0-6ì„¸"ëŠ” ILI ë°ì´í„°ê°€ ì—†ìŒ! "0ì„¸"ì™€ "1-6ì„¸"ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŒ
    LEAD_AGE_GROUPS = ["0ì„¸", "1-6ì„¸", "7-12ì„¸"]  # ì„ í–‰ ì§€í‘œ ì—°ë ¹ëŒ€ (ìœ í–‰ì´ ë¨¼ì € ì‹œì‘)
    
    # ===== í”¼ì²˜ ì œì™¸ ì„¤ì • =====
    EXCLUDE_HOSPITALIZATION = True  # hospitalization í”¼ì²˜ ì œì™¸ ì—¬ë¶€
    
    # ===== ì¼ë³„ ë°ì´í„° ë³€í™˜ ì„¤ì • =====
    USE_DAILY_DATA = True              # ì£¼ì°¨ë³„ â†’ ì¼ë³„ ë°ì´í„° ë³€í™˜ ì—¬ë¶€
    DAILY_INTERP_METHOD = "linear"     # ì¼ë³„ ë°ì´í„° ë³´ê°„ : "gaussian" ë˜ëŠ” "linear"
    GAUSSIAN_STD = 1.0                 # ë°”ìš°ì‹œì•ˆ ì»¤ë„ í‘œì¤€í¸ì°¨
    DAILY_SEQ_LEN = 112                # ì¼ë³„ ì…ë ¥ ê¸¸ì´ (ì•½ 16ì£¼)
    DAILY_PRED_LEN = 28                # ì¼ë³„ ì˜ˆì¸¡ ê¸¸ì´ (ì•½ 4ì£¼)
    
    # ===== íŠ¸ë Œë“œ ë°ì´í„° ì„¤ì • (Google, Naver, Twitter) =====
    # TODO: APIê°€ ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜í•˜ëŠ” ë¬¸ì œ í•´ê²° í›„ Trueë¡œ ë³€ê²½
    USE_TRENDS_DATA = False  # íŠ¸ë Œë“œ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ (í˜„ì¬ ë¹„í™œì„±í™”)
    TRENDS_DB_NAME = "trends"  # PostgreSQL íŠ¸ë Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
    TRENDS_TABLE_NAME = "trends_data"  # íŠ¸ë Œë“œ ë°ì´í„° í…Œì´ë¸” ì´ë¦„
    
    # ===== ì¶œë ¥ ì„¤ì • =====
    OUT_CSV = str(BASE_DIR / "ili_predictions.csv")
    PLOT_LAST_WINDOW = str(BASE_DIR / "plot_last_window.png")
    PLOT_TEST_RECON = str(BASE_DIR / "results.png")
    PLOT_MA_CURVES = str(BASE_DIR / "plot_ma_curves.png")
    BEST_PARAMS_JSON = str(BASE_DIR / "best_hyperparameters.json")
    
    # ===== ê¸°íƒ€ ì„¤ì • =====
    RECON_W_START = 2.0     # Overlap ì¬êµ¬ì„± ì‹œì‘ ê°€ì¤‘ì¹˜
    RECON_W_END = 0.5       # Overlap ì¬êµ¬ì„± ë ê°€ì¤‘ì¹˜

# ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì • (í•˜ìœ„ í˜¸í™˜ì„±)
USE_OPTUNA = Config.USE_OPTUNA
N_TRIALS = Config.N_TRIALS

EPOCHS = Config.EPOCHS
BATCH_SIZE = Config.BATCH_SIZE
SEQ_LEN = Config.SEQ_LEN
PRED_LEN = Config.PRED_LEN
PATCH_LEN = Config.PATCH_LEN
STRIDE = Config.STRIDE

D_MODEL = Config.D_MODEL
N_HEADS = Config.N_HEADS
ENC_LAYERS = Config.ENC_LAYERS
FF_DIM = Config.FF_DIM
DROPOUT = Config.DROPOUT
HEAD_HIDDEN = Config.HEAD_HIDDEN

LR = Config.LR
WEIGHT_DECAY = Config.WEIGHT_DECAY
PATIENCE = Config.PATIENCE
WARMUP_EPOCHS = Config.WARMUP_EPOCHS

SCALER_TYPE = Config.SCALER_TYPE
USE_EXOG = Config.USE_EXOG
INCLUDE_SEASONAL_FEATS = Config.INCLUDE_SEASONAL_FEATS

OUT_CSV = Config.OUT_CSV
PLOT_LAST_WINDOW = Config.PLOT_LAST_WINDOW
PLOT_TEST_RECON = Config.PLOT_TEST_RECON
PLOT_MA_CURVES = Config.PLOT_MA_CURVES

RECON_W_START = Config.RECON_W_START
RECON_W_END = Config.RECON_W_END

# =========================
# utils
# =========================
from datetime import date

def _iso_weeks_in_year(y: int) -> int:
    # ISO ë‹¬ë ¥ì˜ ë§ˆì§€ë§‰ ì£¼ ë²ˆí˜¸(52 ë˜ëŠ” 53)
    return date(y, 12, 28).isocalendar().week

def weekly_to_daily_interp_gaussian(
    df: pd.DataFrame,
    season_col: str = "season_norm",
    week_col: str = "week",
    target_col: str = "ili",
    method: str = "gaussian",
    gaussian_std: float = 1.0,
) -> pd.DataFrame:
    """
    ì£¼ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì¼ ë‹¨ìœ„ë¡œ í™•ì¥(ë°”ìš°ì‹œì•ˆ ë˜ëŠ” ì„ í˜•ë³´ê°„).
    
    Parameters:
        df: ì£¼ì°¨ë³„ ë°ì´í„°í”„ë ˆì„
        season_col: ì‹œì¦Œ ì»¬ëŸ¼ëª…
        week_col: ì£¼ì°¨ ì»¬ëŸ¼ëª…
        target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        method: ë³´ê°„ ë°©ë²• ("gaussian" ë˜ëŠ” "linear")
        gaussian_std: ë°”ìš°ì‹œì•ˆ ì»¤ë„ í‘œì¤€í¸ì°¨ (method="gaussian"ì¼ ë•Œ)
        
    Returns:
        date ì»¬ëŸ¼ í¬í•¨í•œ ì¼ ë‹¨ìœ„ DF
    """
    df = df.copy()
    df.columns = df.columns.str.replace("\ufeff", "", regex=True).str.strip()

    # --- ì‹œì¦Œ/ì£¼ì°¨ í™•ë³´ ---
    has_season = season_col in df.columns
    has_week   = week_col in df.columns

    if not (has_season and has_week):
        # labelì—ì„œ ì‹œì¦Œ/ì£¼ì°¨ ì¶”ì¶œ ì‹œë„: "2024-2025 season - W29"
        if "label" in df.columns:
            import re
            def _parse_label(lbl):
                m = re.search(r"(\d{4}-\d{4}).*W\s*([0-9]+)", str(lbl))
                if m:
                    return m.group(1), int(m.group(2))
                return None
            parsed = df["label"].map(_parse_label)
            if not has_season:
                df[season_col] = [p[0] if p else np.nan for p in parsed]
                has_season = True
            if not has_week:
                df[week_col] = [p[1] if p else np.nan for p in parsed]
                has_week = True

    # ìµœí›„ì˜ ìˆ˜ë‹¨: season_normì´ ì—†ìœ¼ë©´ ë‹¨ì¼ ì‹œì¦Œìœ¼ë¡œ, week ì—†ìœ¼ë©´ 1..N
    if not has_season:
        # ì²« í–‰ì˜ ì—°ë„ë¥¼ ì°¾ì•„ ëŒ€ì²´ ì‹œì¦Œëª… ë§Œë“¤ê¸°
        # ì—†ìœ¼ë©´ "0000-0001"
        first_year = None
        if "date" in df.columns:
            try:
                first_year = pd.to_datetime(df["date"]).dt.year.min()
            except Exception:
                pass
        if first_year is None:
            first_year = pd.Timestamp.today().year
        df[season_col] = f"{first_year}-{first_year+1}"
        has_season = True

    if not has_week:
        df[week_col] = np.arange(1, len(df) + 1, dtype=int)
        has_week = True

    # ìˆ«ìí™”
    df[week_col] = pd.to_numeric(df[week_col], errors="coerce")
    # ì‹œì¦Œ ë¬¸ìì—´ ì •ê·œí™”
    def _norm_season_text_local(s: str) -> str:
        ss = str(s).replace("ì ˆê¸°", "")
        import re
        m = re.search(r"(\d{4})\s*-\s*(\d{4})", ss)
        return f"{m.group(1)}-{m.group(2)}" if m else ss.strip()
    df[season_col] = df[season_col].astype(str).map(_norm_season_text_local)

    # --- ISO ì£¼ ì‹œì‘ì¼ ì‚°ì¶œ (ì‹œì¦Œ ê·œì¹™ ë°˜ì˜) ---
    week_starts = []
    for _, row in df.iterrows():
        season = str(row[season_col])
        try:
            y0 = int(season.split("-")[0])
        except Exception:
            y0 = pd.Timestamp.today().year
        wk = int(row[week_col]) if not pd.isna(row[week_col]) else 1
        iso_year = y0 if wk >= 36 else (y0 + 1)
        # í•´ë‹¹ ISOë…„ì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì£¼ ë„˜ì§€ ì•Šë„ë¡ ë³´ì •
        wk = min(max(1, wk), _iso_weeks_in_year(iso_year))
        # ì›”ìš”ì¼(1) ê¸°ì¤€ ì£¼ ì‹œì‘ì¼
        week_starts.append(pd.Timestamp.fromisocalendar(iso_year, wk, 1))
    df["week_start"] = week_starts

    # --- ì¤‘ë³µ week_start ì²˜ë¦¬: ìˆ˜ì¹˜=mean, ë¹„ìˆ˜ì¹˜=first ---
    if df["week_start"].duplicated().any():
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        agg = {c: "mean" for c in num_cols}
        # ë¹„ìˆ˜ì¹˜ ì»¬ëŸ¼(ë¼ë²¨/ì‹œì¦Œ ë“±)ì€ ì²« ê°’ ìœ ì§€
        for c in df.columns:
            if c not in num_cols and c != "week_start":
                agg[c] = "first"
        df = df.groupby("week_start", as_index=False).agg(agg)

    # --- ì¼ ë‹¨ìœ„ ë¦¬ìƒ˜í”Œ ---
    df = df.set_index("week_start").sort_index()
    df_daily = df.resample("D").asfreq()

    # ìˆ˜ì¹˜í˜• ë³´ê°„
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if method.lower() == "gaussian":
        # ğŸ”´ ë°”ìš°ì‹œì•ˆ ë³´ê°„ë²• (Gaussian Interpolation)
        from scipy.ndimage import gaussian_filter1d
        
        for c in num_cols:
            # ì›ë³¸ ì£¼ì°¨ë³„ ë°ì´í„°
            valid_mask = df[c].notna()
            if valid_mask.sum() < 2:
                # ë°ì´í„°ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ì„ í˜•ë³´ê°„
                df_daily[c] = df_daily[c].interpolate(method="linear", limit_direction="both")
                continue
            
            # ë¨¼ì € ì„ í˜•ë³´ê°„ìœ¼ë¡œ NaN ì±„ìš°ê¸°
            temp = df_daily[c].interpolate(method="linear", limit_direction="both")
            
            # ë°”ìš°ì‹œì•ˆ í•„í„° ì ìš© (í‰í™œ íš¨ê³¼)
            if temp.notna().sum() > 0:
                values = temp.fillna(temp.mean()).values
                smoothed = gaussian_filter1d(values, sigma=gaussian_std)
                df_daily[c] = smoothed
            else:
                df_daily[c] = temp
    else:
        # ì„ í˜•ë³´ê°„ (ê¸°ì¡´ ë°©ì‹)
        for c in num_cols:
            df_daily[c] = df_daily[c].interpolate(method="linear", limit_direction="both")

    # ë²”ì£¼í˜•ì€ ì•ë’¤ ì±„ì›€
    cat_cols = [c for c in df.columns if c not in num_cols]
    for c in cat_cols:
        df_daily[c] = df_daily[c].ffill().bfill()

    # ê²°ê³¼
    out = df_daily.reset_index().rename(columns={"week_start": "date"})
    # dateëŠ” datetimeìœ¼ë¡œ ê°•ì œ
    out["date"] = pd.to_datetime(out["date"])
    
    print(f"\nâœ… ì¼ë³„ ë°ì´í„° ë³€í™˜ ì™„ë£Œ ({method.upper()} ë³´ê°„ë²•):")
    print(f"   - ì…ë ¥: {len(df)} ì£¼(week)")
    print(f"   - ì¶œë ¥: {len(out)} ì¼(day) â†’ {len(out)/7:.1f}ë°° í™•ëŒ€")
    print(f"   - ë‚ ì§œ ë²”ìœ„: {out['date'].min().date()} ~ {out['date'].max().date()}")
    
    return out
    
def set_seed(seed=42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)



def make_splits(n: int, train_ratio=None, val_ratio=None):
    """ë°ì´í„° ë¶„í•  (train/val/test)"""
    if train_ratio is None:
        train_ratio = Config.TRAIN_RATIO
    if val_ratio is None:
        val_ratio = Config.VAL_RATIO
    n_train = int(n * train_ratio)
    n_val   = int(n * val_ratio)
    return (0, n_train), (n_train, n_train+n_val), (n_train+n_val, n)

class LogTransformScaler:
    """
    Log ë³€í™˜ì„ ì ìš©í•˜ëŠ” Scaler
    í”¼í¬ ì˜ˆì¸¡ í–¥ìƒì„ ìœ„í•´ log(1+x) ë³€í™˜ í›„ ìŠ¤ì¼€ì¼ë§
    """
    def __init__(self, base_scaler=None, epsilon=1.0):
        self.base_scaler = base_scaler or RobustScaler()
        self.epsilon = epsilon  # log(x + epsilon)
        
    def fit(self, X):
        # Log ë³€í™˜ í›„ scaler fit
        X_log = np.log(X + self.epsilon)
        self.base_scaler.fit(X_log)
        return self
    
    def transform(self, X):
        # Log ë³€í™˜ í›„ scaler transform
        X_log = np.log(X + self.epsilon)
        return self.base_scaler.transform(X_log)
    
    def fit_transform(self, X):
        return self.fit(X).transform(X)
    
    def inverse_transform(self, X_scaled):
        # scaler inverse í›„ exp ë³€í™˜
        X_log = self.base_scaler.inverse_transform(X_scaled)
        return np.exp(X_log) - self.epsilon


def get_scaler(name=None, for_target=False):
    """
    Scaler ìƒì„± í•¨ìˆ˜
    
    Args:
        name: scaler íƒ€ì… ("robust", "minmax", "standard")
        for_target: Trueì´ë©´ íƒ€ê²Ÿ ë³€ìˆ˜ìš© (Log ë³€í™˜ ì ìš© ê°€ëŠ¥), Falseì´ë©´ í”¼ì²˜ìš©
    """
    s = (name or Config.SCALER_TYPE).lower()
    
    # Log ë³€í™˜ì€ íƒ€ê²Ÿ ë³€ìˆ˜ì—ë§Œ ì ìš©
    if for_target and Config.USE_LOG_TRANSFORM:
        # Log ë³€í™˜ + ê¸°ë³¸ scaler
        if s == "robust":
            base = RobustScaler()
        elif s == "minmax":
            base = MinMaxScaler()
        else:
            base = StandardScaler()
        return LogTransformScaler(base_scaler=base, epsilon=Config.LOG_EPSILON)
    else:
        # ê¸°ì¡´ scaler (í”¼ì²˜ ë˜ëŠ” Log ë³€í™˜ ë¯¸ì‚¬ìš©)
        if s == "robust":
            return RobustScaler()
        elif s == "minmax":
            return MinMaxScaler()
        else:
            return StandardScaler()

def _norm_season_text(s: str) -> str:
    ss = str(s).replace("ì ˆê¸°", "")
    import re
    m = re.search(r"(\d{4})\s*-\s*(\d{4})", ss)
    return f"{m.group(1)}-{m.group(2)}" if m else ss.strip()


# =========================
# ì—°ë ¹ëŒ€ ë§¤í•‘ ë° ë°ì´í„° ë¡œë“œ ìœ í‹¸ë¦¬í‹°
# =========================

# ì—°ë ¹ëŒ€ ê·¸ë£¹ ì •ì˜ (ë°ì´í„°ì…‹ë§ˆë‹¤ ì—°ë ¹ëŒ€ í‘œê¸°ê°€ ë‹¤ë¦„)
# ì£¼ì˜: '0-6ì„¸'ëŠ” í•©ê³„ ì—°ë ¹ëŒ€ë¡œ ILI ë°ì´í„°ê°€ ì—†ìŒ! '0ì„¸'ì™€ '1-6ì„¸'ë¥¼ ê°ê° ì‚¬ìš©í•´ì•¼ í•¨
AGE_GROUP_MAPPING = {
    # í‘œì¤€í™”ëœ ì—°ë ¹ëŒ€ ì´ë¦„ -> ê° ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì´ë¦„ë“¤
    '0ì„¸': ['0ì„¸'],           # ì˜ì•„ - ILI ìˆìŒ (ì„ í–‰ ì§€í‘œ)
    '1-6ì„¸': ['1-6ì„¸'],       # ìœ ì•„ - ILI ìˆìŒ (ì„ í–‰ ì§€í‘œ)
    '0-6ì„¸': ['0-6ì„¸'],       # í•©ê³„ ì—°ë ¹ëŒ€ - ILI ì—†ìŒ! (ì‚¬ìš© ë¶ˆê°€)
    '7-12ì„¸': ['7-12ì„¸'],     # ì´ˆë“±í•™ìƒ - ILI ìˆìŒ (ì„ í–‰ ì§€í‘œ)
    '13-18ì„¸': ['13-18ì„¸'],
    '19-49ì„¸': ['19-49ì„¸'],
    '50-64ì„¸': ['50-64ì„¸'],
    '65ì„¸ì´ìƒ': ['65ì„¸ì´ìƒ', '65ì„¸ ì´ìƒ'],
    '65-69ì„¸': ['65-69ì„¸'],
    '70-74ì„¸': ['70-74ì„¸'],
    '75ì„¸ì´ìƒ': ['75ì„¸ ì´ìƒ', '75ì„¸ì´ìƒ'],
}

# ì—­ë°©í–¥ ë§¤í•‘: ë°ì´í„°ì…‹ì˜ ì—°ë ¹ëŒ€ -> í‘œì¤€í™”ëœ ì—°ë ¹ëŒ€
def normalize_age_group(age_str: str) -> str:
    """ë°ì´í„°ì…‹ì˜ ì—°ë ¹ëŒ€ í‘œê¸°ë¥¼ í‘œì¤€í™”"""
    for standard, variants in AGE_GROUP_MAPPING.items():
        if age_str in variants:
            return standard
    return age_str  # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜


# =========================
# ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦ í•¨ìˆ˜
# =========================
def validate_data_sources(
    age_group: str = "19-49ì„¸",
    data_dir: str = "data/before",
    merged_csv_path: str = "merged_influenza_data.csv",
    verbose: bool = True
) -> dict:
    """
    merged_influenza_data.csvì™€ ì›ë³¸ CSV(data/before)ì—ì„œ 
    ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í•„í„°ë§í•œ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì¼ê´€ì„± ê²€ì¦
    
    Parameters:
        age_group: ë¹„êµí•  ì—°ë ¹ëŒ€
        data_dir: ì›ë³¸ CSV ë””ë ‰í† ë¦¬
        merged_csv_path: ë³‘í•©ëœ CSV íŒŒì¼ ê²½ë¡œ
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        dict: ë¹„êµ ê²°ê³¼ {'match': bool, 'details': {...}}
    """
    from pathlib import Path
    
    if verbose:
        print(f"\n{'='*70}")
        print(f"ğŸ” ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦: {age_group}")
        print(f"{'='*70}")
    
    results = {
        'age_group': age_group,
        'match': False,
        'details': {}
    }
    
    # ===== 1. merged_influenza_data.csvì—ì„œ í•„í„°ë§ =====
    merged_path = Path(merged_csv_path)
    if not merged_path.exists():
        if verbose:
            print(f"   âš ï¸ {merged_csv_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        results['details']['merged_error'] = 'File not found'
        return results
    
    try:
        df_merged_all = pd.read_csv(merged_csv_path)
        
        # ì—°ë ¹ëŒ€ ë³€í˜• ëª©ë¡
        age_variants = AGE_GROUP_MAPPING.get(age_group, [age_group])
        
        # í•„í„°ë§ (merged CSVëŠ” age_group ì»¬ëŸ¼ ì‚¬ìš©)
        mask = df_merged_all['age_group'].isin(age_variants)
        df_merged = df_merged_all[mask].copy()
        
        # ì—¬ëŸ¬ ë³€í˜•ì´ ìˆëŠ” ì—°ë ¹ëŒ€ëŠ” year, week ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™” í•„ìš”
        # 0-6ì„¸: 0ì„¸ + 1-6ì„¸, 65ì„¸ì´ìƒ: 65ì„¸ì´ìƒ + 65ì„¸ ì´ìƒ ë“±
        if len(age_variants) > 1 and len(df_merged) > 0:
            # ì¤‘ë³µ (year, week) ì¡°í•©ì´ ìˆëŠ”ì§€ í™•ì¸
            dup_count = df_merged.duplicated(subset=['year', 'week'], keep=False).sum()
            if dup_count > 0:
                # year, week ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
                agg_dict = {}
                for col in df_merged.columns:
                    if col in ['year', 'week', 'age_group', 'subtype']:
                        continue  # ê·¸ë£¹í™”/ë¬¸ìì—´ ì»¬ëŸ¼ ì œì™¸
                    elif col in ['hospitalization', 'emergency_patients']:
                        agg_dict[col] = 'sum'  # ì…ì›/ì‘ê¸‰ì‹¤ì€ í•©ì‚°
                    elif df_merged[col].dtype in ['float64', 'int64']:
                        agg_dict[col] = 'mean'  # ìˆ«ìí˜•ë§Œ í‰ê· 
                
                df_merged = df_merged.groupby(['year', 'week'], as_index=False).agg(agg_dict)
                df_merged['age_group'] = age_group
        
        # ì •ë ¬
        df_merged = df_merged.sort_values(['year', 'week']).reset_index(drop=True)
        
        if verbose:
            print(f"\nğŸ“Š ì†ŒìŠ¤ 1: merged_influenza_data.csv")
            print(f"   - í•„í„° ì¡°ê±´: age_group in {age_variants}")
            print(f"   - ê²°ê³¼ í–‰ ìˆ˜: {len(df_merged)}")
            print(f"   - ì—°ë„ ë²”ìœ„: {df_merged['year'].min():.0f} ~ {df_merged['year'].max():.0f}")
            print(f"   - ILI ë²”ìœ„: {df_merged['ili'].min():.2f} ~ {df_merged['ili'].max():.2f}" if df_merged['ili'].notna().any() else "   - ILI: ëª¨ë‘ ê²°ì¸¡")
        
        results['details']['merged'] = {
            'rows': len(df_merged),
            'year_range': (int(df_merged['year'].min()), int(df_merged['year'].max())),
            'ili_range': (float(df_merged['ili'].min()), float(df_merged['ili'].max())) if df_merged['ili'].notna().any() else None,
            'nulls': int(df_merged.isnull().sum().sum())
        }
        
    except Exception as e:
        if verbose:
            print(f"   âŒ merged CSV ë¡œë“œ ì˜¤ë¥˜: {e}")
        results['details']['merged_error'] = str(e)
        return results
    
    # ===== 2. ì›ë³¸ CSV(data/before)ì—ì„œ í•„í„°ë§ =====
    try:
        df_raw = load_raw_data_by_age_group(data_dir=data_dir, age_group=age_group)
        df_raw = df_raw.sort_values(['year', 'week']).reset_index(drop=True)
        
        if verbose:
            print(f"\nğŸ“Š ì†ŒìŠ¤ 2: data/before (ì›ë³¸ CSV)")
            print(f"   - í•¨ìˆ˜: load_raw_data_by_age_group('{age_group}')")
            print(f"   - ê²°ê³¼ í–‰ ìˆ˜: {len(df_raw)}")
            print(f"   - ì—°ë„ ë²”ìœ„: {df_raw['year'].min():.0f} ~ {df_raw['year'].max():.0f}")
            print(f"   - ILI ë²”ìœ„: {df_raw['ili'].min():.2f} ~ {df_raw['ili'].max():.2f}" if df_raw['ili'].notna().any() else "   - ILI: ëª¨ë‘ ê²°ì¸¡")
        
        results['details']['raw'] = {
            'rows': len(df_raw),
            'year_range': (int(df_raw['year'].min()), int(df_raw['year'].max())),
            'ili_range': (float(df_raw['ili'].min()), float(df_raw['ili'].max())) if df_raw['ili'].notna().any() else None,
            'nulls': int(df_raw.isnull().sum().sum())
        }
        
    except Exception as e:
        if verbose:
            print(f"   âŒ ì›ë³¸ CSV ë¡œë“œ ì˜¤ë¥˜: {e}")
        results['details']['raw_error'] = str(e)
        return results
    
    # ===== 3. ë¹„êµ =====
    if verbose:
        print(f"\nğŸ“Š ë¹„êµ ê²°ê³¼:")
    
    # í–‰ ìˆ˜ ë¹„êµ
    row_match = len(df_merged) == len(df_raw)
    if verbose:
        print(f"   - í–‰ ìˆ˜ ì¼ì¹˜: {'âœ…' if row_match else 'âŒ'} (merged: {len(df_merged)}, raw: {len(df_raw)})")
    
    # ILI ê°’ ë¹„êµ (ê³µí†µ year/week ê¸°ì¤€)
    common_keys = set(zip(df_merged['year'], df_merged['week'])) & set(zip(df_raw['year'], df_raw['week']))
    
    if common_keys:
        # ê³µí†µ í‚¤ë¡œ ë³‘í•©
        df_merged_subset = df_merged[df_merged.apply(lambda r: (r['year'], r['week']) in common_keys, axis=1)].copy()
        df_raw_subset = df_raw[df_raw.apply(lambda r: (r['year'], r['week']) in common_keys, axis=1)].copy()
        
        df_merged_subset = df_merged_subset.sort_values(['year', 'week']).reset_index(drop=True)
        df_raw_subset = df_raw_subset.sort_values(['year', 'week']).reset_index(drop=True)
        
        # ILI ë¹„êµ
        ili_merged = df_merged_subset['ili'].fillna(0).values
        ili_raw = df_raw_subset['ili'].fillna(0).values
        
        if len(ili_merged) == len(ili_raw):
            ili_diff = np.abs(ili_merged - ili_raw)
            ili_match = np.allclose(ili_merged, ili_raw, rtol=1e-5, atol=1e-8, equal_nan=True)
            ili_max_diff = ili_diff.max()
            ili_mean_diff = ili_diff.mean()
            
            if verbose:
                print(f"   - ILI ê°’ ì¼ì¹˜: {'âœ…' if ili_match else 'âš ï¸'} (ìµœëŒ€ ì°¨ì´: {ili_max_diff:.6f}, í‰ê·  ì°¨ì´: {ili_mean_diff:.6f})")
            
            results['details']['ili_comparison'] = {
                'match': bool(ili_match),
                'max_diff': float(ili_max_diff),
                'mean_diff': float(ili_mean_diff)
            }
        else:
            if verbose:
                print(f"   - ILI ë¹„êµ ë¶ˆê°€: í–‰ ìˆ˜ ë¶ˆì¼ì¹˜")
            ili_match = False
    else:
        if verbose:
            print(f"   - ê³µí†µ í‚¤ ì—†ìŒ")
        ili_match = False
    
    # ì „ì²´ ì¼ì¹˜ ì—¬ë¶€
    results['match'] = row_match and (ili_match if common_keys else False)
    
    if verbose:
        print(f"\n   ğŸ“‹ ìµœì¢… ê²°ê³¼: {'âœ… ì¼ì¹˜' if results['match'] else 'âš ï¸ ë¶ˆì¼ì¹˜ (ì°¨ì´ ìˆìŒ)'}")
        print(f"{'='*70}")
    
    return results


def validate_all_age_groups(
    data_dir: str = "data/before",
    merged_csv_path: str = "merged_influenza_data.csv"
) -> dict:
    """
    ëª¨ë“  ì£¼ìš” ì—°ë ¹ëŒ€ì— ëŒ€í•´ ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦ ì‹¤í–‰
    
    Returns:
        dict: ì—°ë ¹ëŒ€ë³„ ê²€ì¦ ê²°ê³¼
    """
    print(f"\n{'ğŸ”¬ '*20}")
    print("ëª¨ë“  ì—°ë ¹ëŒ€ ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦")
    print(f"{'ğŸ”¬ '*20}")
    
    age_groups = ['0-6ì„¸', '7-12ì„¸', '13-18ì„¸', '19-49ì„¸', '50-64ì„¸', '65ì„¸ì´ìƒ']
    all_results = {}
    
    for age in age_groups:
        result = validate_data_sources(
            age_group=age,
            data_dir=data_dir,
            merged_csv_path=merged_csv_path,
            verbose=True
        )
        all_results[age] = result
    
    # ìš”ì•½
    print(f"\n{'='*70}")
    print("ğŸ“‹ ì „ì²´ ê²€ì¦ ìš”ì•½")
    print(f"{'='*70}")
    
    for age, result in all_results.items():
        status = 'âœ…' if result['match'] else 'âš ï¸'
        details = result.get('details', {})
        merged_rows = details.get('merged', {}).get('rows', 'N/A')
        raw_rows = details.get('raw', {}).get('rows', 'N/A')
        print(f"   {status} {age}: merged={merged_rows}í–‰, raw={raw_rows}í–‰")
    
    return all_results


def load_raw_data_by_age_group(
    data_dir: str = "data/before",
    age_group: str = "19-49ì„¸"
) -> pd.DataFrame:
    """
    íŠ¹ì • ì—°ë ¹ëŒ€ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ data/before ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ
    PostgreSQLì„ ê±°ì¹˜ì§€ ì•Šê³  ì›ë³¸ CSVì—ì„œ ì§ì ‘ ë¡œë“œ
    
    Parameters:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        age_group: ì„ íƒí•  ì—°ë ¹ëŒ€ (ì˜ˆ: '19-49ì„¸', '65ì„¸ì´ìƒ')
    
    Returns:
        pd.DataFrame: í•´ë‹¹ ì—°ë ¹ëŒ€ì˜ ë³‘í•©ëœ ë°ì´í„°
    """
    from pathlib import Path
    
    data_path = Path(data_dir)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ ì—°ë ¹ëŒ€ë³„ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {age_group}")
    print(f"{'='*60}")
    
    # ì—°ë ¹ëŒ€ ë³€í˜• ëª©ë¡
    age_variants = AGE_GROUP_MAPPING.get(age_group, [age_group])
    print(f"   - ê²€ìƒ‰í•  ì—°ë ¹ëŒ€ ë³€í˜•: {age_variants}")
    
    # ë°ì´í„°ì…‹ë³„ ë¡œë“œ
    # has_age: True = ì—°ë ¹ëŒ€ í•„í„°ë§ í•„ìˆ˜, False = ì „êµ­ ë°ì´í„° (ì—°ë ¹ëŒ€ ì—†ìŒ)
    # fallback_to_avg: True = ì—°ë ¹ëŒ€ ë°ì´í„° ì—†ìœ¼ë©´ ì „êµ­ í‰ê·  ì‚¬ìš©
    datasets = {
        'ds_0101': {'col': 'ì˜ì‚¬í™˜ì ë¶„ìœ¨', 'target': 'ili', 'has_age': True, 'fallback_to_avg': False},
        'ds_0103': {'col': 'ì…ì›í™˜ì ìˆ˜', 'target': 'hospitalization_confirmed', 'has_age': True, 'fallback_to_avg': False},
        'ds_0104': {'col': 'ì…ì›í™˜ì ìˆ˜', 'target': 'hospitalization_suspected', 'has_age': True, 'fallback_to_avg': False},
        'ds_0106': {'col': 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ', 'target': 'detection_rate', 'has_age': True, 'fallback_to_avg': False},
        'ds_0108': {'col': 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ', 'target': 'detection_rate_alt', 'has_age': True, 'fallback_to_avg': False},
        'ds_0109': {'col': 'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì', 'target': 'emergency_patients', 'has_age': True, 'fallback_to_avg': False},
        'ds_0110': {'col': 'ì˜ˆë°©ì ‘ì¢…ë¥ ', 'target': 'vaccine_rate', 'has_age': True, 'fallback_to_avg': True},
    }
    
    all_data = {}
    
    for dsid, info in datasets.items():
        ds_num = dsid.replace('ds_', '')
        pattern = f"flu-{ds_num}-*.csv"
        files = list(data_path.glob(pattern))
        
        if not files:
            continue
        
        dfs = []
        for f in sorted(files):
            try:
                df = pd.read_csv(f)
                dfs.append(df)
            except Exception as e:
                print(f"   âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({f.name}): {e}")
        
        if not dfs:
            continue
        
        df_combined = pd.concat(dfs, ignore_index=True)
        
        # ì—°ë ¹ëŒ€ í•„í„°ë§
        if info['has_age'] and 'ì—°ë ¹ëŒ€' in df_combined.columns:
            # í•´ë‹¹ ì—°ë ¹ëŒ€ë§Œ í•„í„°ë§
            mask = df_combined['ì—°ë ¹ëŒ€'].isin(age_variants)
            df_filtered = df_combined[mask].copy()
            
            # ì—°ë ¹ëŒ€ ë°ì´í„°ê°€ ì—†ê³  fallback_to_avgê°€ Trueì¸ ê²½ìš° ì „êµ­ í‰ê·  ì‚¬ìš©
            if df_filtered.empty and info.get('fallback_to_avg', False):
                print(f"   - {dsid}: ì—°ë ¹ëŒ€ '{age_group}' ë°ì´í„° ì—†ìŒ â†’ ì „êµ­ í‰ê·  ì‚¬ìš©")
                
                # ì»¬ëŸ¼ í‘œì¤€í™”
                df_combined = df_combined.rename(columns={
                    'ì—°ë„': 'year',
                    'ì£¼ì°¨': 'week',
                    info['col']: info['target']
                })
                
                # ìˆ˜ì¹˜í˜• ë³€í™˜
                df_combined[info['target']] = pd.to_numeric(df_combined[info['target']], errors='coerce')
                
                # ì£¼ì°¨ë³„ ì „êµ­ í‰ê·  ê³„ì‚°
                df_filtered = df_combined.groupby(['year', 'week'], as_index=False)[info['target']].mean()
                
                all_data[info['target']] = df_filtered
                print(f"   - {dsid} ({info['target']}): {len(df_filtered)}í–‰ ë¡œë“œ (ì „êµ­ í‰ê· )")
                continue
            
            if df_filtered.empty:
                print(f"   - {dsid}: ì—°ë ¹ëŒ€ '{age_group}' ë°ì´í„° ì—†ìŒ")
                continue
            
            # ì»¬ëŸ¼ í‘œì¤€í™”
            df_filtered = df_filtered.rename(columns={
                'ì—°ë„': 'year',
                'ì£¼ì°¨': 'week',
                'ì—°ë ¹ëŒ€': 'age_group',
                info['col']: info['target']
            })
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            cols = ['year', 'week', info['target']]
            df_filtered = df_filtered[cols].copy()
            
            # ì—¬ëŸ¬ ì—°ë ¹ëŒ€ ë³€í˜•ì´ ìˆì„ ê²½ìš° (ì˜ˆ: 0-6ì„¸ = 0ì„¸ + 1-6ì„¸) í•©ì‚°
            if len(age_variants) > 1:
                # ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜
                df_filtered[info['target']] = pd.to_numeric(df_filtered[info['target']], errors='coerce')
                # year, week ê¸°ì¤€ìœ¼ë¡œ í•©ì‚° (ì…ì›í™˜ì, ì‘ê¸‰ì‹¤) ë˜ëŠ” í‰ê·  (ILI, ê²€ì¶œë¥ )
                if info['target'] in ['hospitalization_confirmed', 'hospitalization_suspected', 'emergency_patients']:
                    df_filtered = df_filtered.groupby(['year', 'week'], as_index=False)[info['target']].sum()
                else:
                    df_filtered = df_filtered.groupby(['year', 'week'], as_index=False)[info['target']].mean()
            
            all_data[info['target']] = df_filtered
            print(f"   - {dsid} ({info['target']}): {len(df_filtered)}í–‰ ë¡œë“œ")
    
    if not all_data:
        print(f"\nâš ï¸ ì—°ë ¹ëŒ€ '{age_group}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ëª¨ë“  ë°ì´í„° ë³‘í•© (year, week ê¸°ì¤€)
    print(f"\nğŸ“Š ë°ì´í„° ë³‘í•© ì¤‘...")
    
    # ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
    result_df = None
    for target_name, df in all_data.items():
        if result_df is None:
            result_df = df.copy()
        else:
            # year, week ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
            result_df = pd.merge(result_df, df, on=['year', 'week'], how='outer')
    
    # ì •ë ¬
    result_df = result_df.sort_values(['year', 'week']).reset_index(drop=True)
    
    # hospitalization í•©ì‚° (í™•ì§„ + ì˜ì‹¬)
    if 'hospitalization_confirmed' in result_df.columns or 'hospitalization_suspected' in result_df.columns:
        confirmed = result_df.get('hospitalization_confirmed', 0).fillna(0)
        suspected = result_df.get('hospitalization_suspected', 0).fillna(0)
        result_df['hospitalization'] = confirmed + suspected
        
        # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
        for col in ['hospitalization_confirmed', 'hospitalization_suspected']:
            if col in result_df.columns:
                result_df = result_df.drop(columns=[col])
    
    # detection_rate í†µí•© (ds_0106ê³¼ ds_0108 ì¤‘ í•˜ë‚˜ ì„ íƒ)
    if 'detection_rate' in result_df.columns and 'detection_rate_alt' in result_df.columns:
        # ìš°ì„  ds_0106 ì‚¬ìš©, ì—†ìœ¼ë©´ ds_0108
        result_df['detection_rate'] = result_df['detection_rate'].fillna(result_df['detection_rate_alt'])
        result_df = result_df.drop(columns=['detection_rate_alt'])
    elif 'detection_rate_alt' in result_df.columns:
        result_df = result_df.rename(columns={'detection_rate_alt': 'detection_rate'})
    
    # ì—°ë ¹ëŒ€ ì»¬ëŸ¼ ì¶”ê°€
    result_df['age_group'] = age_group
    
    print(f"\nâœ… ì—°ë ¹ëŒ€ '{age_group}' ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   - ì´ {len(result_df)}í–‰")
    print(f"   - ì»¬ëŸ¼: {list(result_df.columns)}")
    print(f"   - ì—°ë„ ë²”ìœ„: {result_df['year'].min():.0f} ~ {result_df['year'].max():.0f}")
    print(f"   - ì£¼ì°¨ ë²”ìœ„: {result_df['week'].min():.0f} ~ {result_df['week'].max():.0f}")
    
    return result_df


def get_available_age_groups(data_dir: str = "data/before") -> dict:
    """
    data/before ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ë ¹ëŒ€ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        dict: ë°ì´í„°ì…‹ë³„ ì—°ë ¹ëŒ€ ëª©ë¡
    """
    from pathlib import Path
    
    data_path = Path(data_dir)
    result = {}
    
    # ì£¼ìš” ë°ì´í„°ì…‹ í™•ì¸
    datasets = ['0101', '0103', '0106', '0108', '0109', '0110']
    
    for ds in datasets:
        pattern = f"flu-{ds}-*.csv"
        files = list(data_path.glob(pattern))
        
        if not files:
            continue
        
        age_groups = set()
        for f in files:
            try:
                df = pd.read_csv(f)
                if 'ì—°ë ¹ëŒ€' in df.columns:
                    age_groups.update(df['ì—°ë ¹ëŒ€'].dropna().unique())
            except:
                pass
        
        if age_groups:
            result[f'ds_{ds}'] = sorted(list(age_groups))
    
    return result


# =========================
# ì•„í˜•ë³„ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ds_0107)
# =========================
def load_subtype_data(data_dir: str = "data/before", subtype: str = "A") -> pd.DataFrame:
    """
    ds_0107 ë°ì´í„°ì—ì„œ íŠ¹ì • ì•„í˜•(A/B)ì˜ ê²€ì¶œë¥  ë°ì´í„°ë¥¼ ë¡œë“œ
    
    Parameters:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        subtype: ì•„í˜• ('A', 'B', ë˜ëŠ” 'all')
    
    Returns:
        pd.DataFrame: ì•„í˜•ë³„ ê²€ì¶œë¥  ë°ì´í„° (ì—°ë„, ì£¼ì°¨, ê²€ì¶œë¥ )
    """
    from pathlib import Path
    
    data_path = Path(data_dir)
    flu_0107_files = list(data_path.glob("flu-0107-*.csv"))
    
    if not flu_0107_files:
        print(f"âš ï¸ ds_0107 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return pd.DataFrame()
    
    print(f"\nğŸ“Š ì•„í˜•ë³„ ê²€ì¶œë¥  ë°ì´í„° ë¡œë“œ (ds_0107)")
    print(f"   - ë°œê²¬ëœ íŒŒì¼: {len(flu_0107_files)}ê°œ")
    print(f"   - ì„ íƒëœ ì•„í˜•: {subtype}")
    
    all_dfs = []
    for filepath in sorted(flu_0107_files):
        try:
            df = pd.read_csv(filepath)
            all_dfs.append(df)
        except Exception as e:
            print(f"   âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({filepath.name}): {e}")
    
    if not all_dfs:
        return pd.DataFrame()
    
    df_combined = pd.concat(all_dfs, ignore_index=True)
    
    # ì»¬ëŸ¼ëª… ë§¤í•‘
    col_map = {
        'ì—°ë„': 'year',
        'ì£¼ì°¨': 'week',
        'ì•„í˜•': 'subtype',
        'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate'
    }
    df_combined = df_combined.rename(columns=col_map)
    
    # 'ê²€ì¶œë¥ ' í–‰ ì œê±° (ì „ì²´ ê²€ì¶œë¥ )
    if 'subtype' in df_combined.columns:
        df_combined = df_combined[df_combined['subtype'] != 'ê²€ì¶œë¥ '].copy()
    
    # ì•„í˜• í•„í„°ë§
    if subtype.upper() != 'ALL':
        df_combined = df_combined[df_combined['subtype'] == subtype.upper()].copy()
    
    # ì •ë ¬
    df_combined = df_combined.sort_values(['year', 'week']).reset_index(drop=True)
    
    print(f"   - ìµœì¢… ë°ì´í„°: {len(df_combined)}í–‰")
    print(f"   - ì—°ë„ ë²”ìœ„: {df_combined['year'].min()} ~ {df_combined['year'].max()}")
    print(f"   - ì•„í˜•ë³„ ë¶„í¬: {df_combined['subtype'].value_counts().to_dict() if 'subtype' in df_combined.columns else 'N/A'}")
    
    return df_combined


def prepare_subtype_data(
    subtype: str = "A",
    data_dir: str = "data/before"
) -> Tuple[np.ndarray, np.ndarray, list, list]:
    """
    ì•„í˜•ë³„(A/B) ê²€ì¶œë¥  ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
    ds_0107 ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì•„í˜•ì˜ ê²€ì¶œë¥  ì‹œê³„ì—´ ì˜ˆì¸¡
    
    Parameters:
        subtype: ì•„í˜• ('A' ë˜ëŠ” 'B')
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        X: (N, F) features
        y: (N,) target (ê²€ì¶œë¥ )
        labels: list[str] for plotting
        feat_names: list[str] feature names
    """
    # ì•„í˜•ë³„ ë°ì´í„° ë¡œë“œ
    df = load_subtype_data(data_dir=data_dir, subtype=subtype)
    
    if df.empty:
        raise ValueError(f"ì•„í˜• '{subtype}' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ“Š ì•„í˜•ë³„ ê²€ì¶œë¥  ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„")
    print(f"   - ì„ íƒëœ ì•„í˜•: {subtype}")
    print(f"   - ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ")
    
    # ê³„ì ˆì„± í”¼ì²˜ ì¶”ê°€
    df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)
    
    # season_norm ë¼ë²¨ ìƒì„±
    df['season_norm'] = df.apply(
        lambda row: f"{int(row['year'])}-{int(row['year'])+1}" if row['week'] >= 36 
                   else f"{int(row['year'])-1}-{int(row['year'])}",
        axis=1
    )
    
    # í”¼ì²˜ êµ¬ì„±: ê²€ì¶œë¥  + ê³„ì ˆì„±
    feat_names = ['detection_rate', 'week_sin']
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.dropna(subset=['detection_rate'])
    
    X = df[feat_names].to_numpy(dtype=float)
    y = df['detection_rate'].to_numpy(dtype=float)
    labels = (df['season_norm'].astype(str) + f" ({subtype}) - W" + df['week'].astype(int).astype(str)).tolist()
    
    print(f"\nâœ… ì•„í˜•ë³„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"   - X shape: {X.shape}")
    print(f"   - y shape: {y.shape}")
    print(f"   - Features: {feat_names}")
    
    return X, y, labels, feat_names


# =========================
# ì—°ë ¹ëŒ€ë³„ ë°ì´í„° ì¤€ë¹„ (ì›ë³¸ CSVì—ì„œ ì§ì ‘ ë¡œë“œ)
# =========================
def load_and_prepare_by_age(
    age_group: str = "19-49ì„¸",
    data_dir: str = "data/before",
    use_exog: str = "all"
) -> Tuple[np.ndarray, np.ndarray, list, list]:
    """
    íŠ¹ì • ì—°ë ¹ëŒ€ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ ì§ì ‘ ë¡œë“œí•˜ì—¬ ëª¨ë¸ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬
    PostgreSQLì„ ê±°ì¹˜ì§€ ì•Šê³  data/beforeì—ì„œ ì§ì ‘ ë¡œë“œ
    
    Parameters:
        age_group: ì—°ë ¹ëŒ€ (ì˜ˆ: '19-49ì„¸', '65ì„¸ì´ìƒ', '0-6ì„¸')
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        use_exog: ì™¸ìƒë³€ìˆ˜ ì‚¬ìš© ëª¨ë“œ ('all', 'vaccine', 'resp', 'none', 'auto')
    
    Returns:
        X: (N, F) features
        y: (N,) target (ILI)
        labels: list[str] for plotting
        feat_names: list[str] feature names
    """
    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    df = load_raw_data_by_age_group(data_dir=data_dir, age_group=age_group)
    
    if df.empty:
        raise ValueError(f"ì—°ë ¹ëŒ€ '{age_group}' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ILI ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    if 'ili' not in df.columns:
        raise ValueError(f"ì—°ë ¹ëŒ€ '{age_group}'ì— ILI ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ===== ë‚ ì”¨ ë°ì´í„° ë³‘í•© (PostgreSQL) =====
    print(f"\nğŸŒ¡ï¸  ë‚ ì”¨ ë°ì´í„° ë³‘í•© ì‹œë„...")
    try:
        df_weather = load_weather_data_from_postgres()
        if df_weather is not None and not df_weather.empty:
            df = merge_weather_with_influenza(df, df_weather)
            
            # ë³‘í•© ì„±ê³µ í™•ì¸
            weather_cols_merged = [c for c in ['min_temp', 'max_temp', 'avg_humidity'] if c in df.columns]
            print(f"\n   âœ… ë‚ ì”¨ ë°ì´í„° ë³‘í•© ì„±ê³µ!")
            print(f"      - ë³‘í•© í›„ Shape: {df.shape}")
            print(f"      - ì¶”ê°€ëœ ë‚ ì”¨ ì»¬ëŸ¼: {weather_cols_merged}")
            print(f"      - ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë‚ ì”¨ ë°ì´í„°ê°€ ëª¨ë¸ì— ì ìš©ë©ë‹ˆë‹¤.")
        else:
            print(f"   âš ï¸  ë‚ ì”¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¸í”Œë£¨ì—”ì ë°ì´í„°ë§Œìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"   âš ï¸  ë‚ ì”¨ ë°ì´í„° ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"   ì¸í”Œë£¨ì—”ì ë°ì´í„°ë§Œìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    print(f"\nğŸ“Š ì—°ë ¹ëŒ€ë³„ ë°ì´í„° ì „ì²˜ë¦¬: {age_group}")
    
    # ===== ì¼ë³„ ë°ì´í„° ë³€í™˜ (ë°”ìš°ì‹œì•ˆ ë³´ê°„) =====
    if Config.USE_DAILY_DATA:
        print(f"\nğŸ”„ ì£¼ì°¨ë³„ â†’ ì¼ë³„ ë°ì´í„° ë³€í™˜ ì‹œì‘...")
        print(f"   - ë³´ê°„ ë°©ë²•: {Config.DAILY_INTERP_METHOD.upper()}")
        print(f"   - ë°”ìš°ì‹œì•ˆ í‘œì¤€í¸ì°¨: {Config.GAUSSIAN_STD}")
        
        # season_norm ìƒì„± (ë¨¼ì €)
        df['season_norm'] = df.apply(
            lambda row: f"{int(row['year'])}-{int(row['year'])+1}" if row['week'] >= 36 
                       else f"{int(row['year'])-1}-{int(row['year'])}",
            axis=1
        )
        
        # ì •ë ¬ (ë³€í™˜ ì „)
        df = df.sort_values(['year', 'week']).reset_index(drop=True)
        
        # ì¼ë³„ ë³€í™˜
        df = weekly_to_daily_interp_gaussian(
            df,
            season_col="season_norm",
            week_col="week",
            target_col="ili",
            method=Config.DAILY_INTERP_METHOD,
            gaussian_std=Config.GAUSSIAN_STD
        )
        
        # SEQ_LEN, PRED_LENì„ ì¼ë³„ë¡œ ì—…ë°ì´íŠ¸
        global SEQ_LEN, PRED_LEN
        SEQ_LEN = Config.DAILY_SEQ_LEN
        PRED_LEN = Config.DAILY_PRED_LEN
        
        print(f"   âœ… ì¼ë³„ ë°ì´í„° ë³€í™˜ ì™„ë£Œ!")
        print(f"   - ìƒˆë¡œìš´ ì…ë ¥ ê¸¸ì´ (SEQ_LEN): {SEQ_LEN}ì¼")
        print(f"   - ìƒˆë¡œìš´ ì˜ˆì¸¡ ê¸¸ì´ (PRED_LEN): {PRED_LEN}ì¼")
        print(f"   - ë³€í™˜ í›„ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(df)}")
    else:
        # ì •ë ¬ (ì¼ë³„ ë³€í™˜ ë¯¸ì‚¬ìš©)
        df = df.sort_values(['year', 'week']).reset_index(drop=True)
    
    # ===== íŒ¬ë°ë¯¹ ê¸°ê°„ ì²˜ë¦¬ =====
    pandemic_mask = (
        ((df['year'] == 2020) & (df['week'] >= 14)) |
        ((df['year'] == 2021)) |
        ((df['year'] == 2022) & (df['week'] <= 22))
    )
    
    pandemic_count = pandemic_mask.sum()
    print(f"   - íŒ¬ë°ë¯¹ ê¸°ê°„ ë°ì´í„°: {pandemic_count}í–‰")
    
    # íŒ¬ë°ë¯¹ ê¸°ê°„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in ['ili', 'hospitalization', 'detection_rate', 'emergency_patients']:
        if col in df.columns:
            df.loc[pandemic_mask, col] = np.nan
    
    # ===== ê³„ì ˆì„± íŒ¨í„´ ê¸°ë°˜ ë³´ê°„ =====
    if df['ili'].isna().sum() > 0:
        print(f"   - ILI ê²°ì¸¡ì¹˜ ë³´ê°„ ì¤‘...")
        
        # íŒ¬ë°ë¯¹ ì´ì „ ë°ì´í„°ë¡œ ì£¼ì°¨ë³„ í‰ê·  ê³„ì‚°
        pre_pandemic = df[(df['year'] >= 2017) & (df['year'] <= 2019) & df['ili'].notna()]
        
        if not pre_pandemic.empty:
            weekly_pattern = pre_pandemic.groupby('week')['ili'].mean()
            
            for idx in df[df['ili'].isna()].index:
                week = int(df.loc[idx, 'week'])
                if week in weekly_pattern.index:
                    df.loc[idx, 'ili'] = weekly_pattern[week]
    
    # ë‹¤ë¥¸ ì»¬ëŸ¼ë„ ë³´ê°„
    for col in ['hospitalization', 'detection_rate', 'emergency_patients']:
        if col in df.columns and df[col].isna().sum() > 0:
            pre_pandemic = df[(df['year'] >= 2017) & (df['year'] <= 2019) & df[col].notna()]
            if not pre_pandemic.empty:
                weekly_pattern = pre_pandemic.groupby('week')[col].mean()
                for idx in df[df[col].isna()].index:
                    week = int(df.loc[idx, 'week'])
                    if week in weekly_pattern.index:
                        df.loc[idx, col] = weekly_pattern[week]
    
    # ===== ê³„ì ˆì„± í”¼ì²˜ ì¶”ê°€ =====
    df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)
    
    # season_norm ë¼ë²¨ ìƒì„±
    df['season_norm'] = df.apply(
        lambda row: f"{int(row['year'])}-{int(row['year'])+1}" if row['week'] >= 36 
                   else f"{int(row['year'])-1}-{int(row['year'])}",
        axis=1
    )
    
    # ===== ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€ (ì–´ë¦°ì´ ì§‘ë‹¨ ILI) =====
    if Config.USE_AGE_GROUP_DYNAMICS and age_group not in Config.LEAD_AGE_GROUPS:
        print(f"\nğŸ”— ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€ ì¤‘...")
        for lead_age in Config.LEAD_AGE_GROUPS:
            try:
                lead_df = load_raw_data_by_age_group(data_dir=data_dir, age_group=lead_age)
                if not lead_df.empty and 'ili' in lead_df.columns:
                    lead_df = lead_df.sort_values(['year', 'week']).reset_index(drop=True)
                    # year, week ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                    lead_ili = lead_df[['year', 'week', 'ili']].copy()
                    lead_ili = lead_ili.rename(columns={'ili': f'ili_{lead_age.replace("-", "_").replace("ì„¸", "")}'})
                    df = df.merge(lead_ili, on=['year', 'week'], how='left')
                    print(f"   âœ… {lead_age} ILI ì¶”ê°€: ili_{lead_age.replace('-', '_').replace('ì„¸', '')}")
            except Exception as e:
                print(f"   âš ï¸  {lead_age} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ===== íŠ¸ë Œë“œ ë°ì´í„° ë³‘í•© (PostgreSQL trends DB) =====
    if Config.USE_TRENDS_DATA:
        print(f"\nğŸ” íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ ì¤‘ (PostgreSQL {Config.TRENDS_DB_NAME} DB)...")
        try:
            from database.db_utils import load_trends_from_postgres
            trends_df = load_trends_from_postgres(
                table_name=Config.TRENDS_TABLE_NAME,
                db_name=Config.TRENDS_DB_NAME
            )
            if not trends_df.empty and 'year' in trends_df.columns and 'week' in trends_df.columns:
                df = df.merge(trends_df, on=['year', 'week'], how='left')
                # Trends ì»¬ëŸ¼ëª… í™•ì¸ (google_, naver_, twitter_ ì ‘ë‘ì‚¬)
                trends_cols = [c for c in trends_df.columns if c not in ['year', 'week']]
                print(f"   âœ… íŠ¸ë Œë“œ í”¼ì²˜ ì¶”ê°€: {len(trends_cols)}ê°œ ì»¬ëŸ¼")
                print(f"      (Google: {len([c for c in trends_cols if c.startswith('google_')])}ê°œ, "
                      f"Naver: {len([c for c in trends_cols if c.startswith('naver_')])}ê°œ, "
                      f"Twitter: {len([c for c in trends_cols if c.startswith('twitter_')])}ê°œ)")
                # ê²°ì¸¡ì¹˜ 0ìœ¼ë¡œ ì±„ì›€ (ê²€ìƒ‰ëŸ‰/ì–¸ê¸‰ëŸ‰ ì—†ìŒ = 0)
                for col in trends_cols:
                    if col in df.columns:
                        df[col] = df[col].fillna(0)
            else:
                print(f"   âš ï¸  íŠ¸ë Œë“œ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ year, week ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"   âš ï¸  íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ğŸ’¡ ë¨¼ì € 'python database/update_trends_database.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    # ===== í”¼ì²˜ ì„ íƒ =====
    # ê¸°ë³¸ í”¼ì²˜: ILI (íƒ€ê²Ÿ)
    chosen = ['ili']
    
    # ì™¸ìƒë³€ìˆ˜ ì„¤ì •
    has_hosp = 'hospitalization' in df.columns and df['hospitalization'].notna().any()
    has_detection = 'detection_rate' in df.columns and df['detection_rate'].notna().any()
    has_emergency = 'emergency_patients' in df.columns and df['emergency_patients'].notna().any()
    has_vaccine = 'vaccine_rate' in df.columns and df['vaccine_rate'].notna().any()
    
    # hospitalization ì œì™¸ ì„¤ì • í™•ì¸
    exclude_hosp = getattr(Config, 'EXCLUDE_HOSPITALIZATION', False)
    if exclude_hosp:
        has_hosp = False
        print("   âš ï¸ hospitalization í”¼ì²˜ ì œì™¸ë¨ (Config.EXCLUDE_HOSPITALIZATION=True)")
    
    if use_exog in ('all', 'auto'):
        if has_hosp:
            chosen.append('hospitalization')
        if has_detection:
            chosen.append('detection_rate')
        if has_emergency:
            chosen.append('emergency_patients')
        if has_vaccine:
            chosen.append('vaccine_rate')
    elif use_exog == 'vaccine' and has_vaccine:
        chosen.append('vaccine_rate')
    elif use_exog == 'resp':
        if has_hosp:
            chosen.append('hospitalization')
        if has_detection:
            chosen.append('detection_rate')
    
    # ê³„ì ˆì„± í”¼ì²˜ ì¶”ê°€
    if INCLUDE_SEASONAL_FEATS:
        chosen.append('week_sin')
    
    # ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€
    if Config.USE_AGE_GROUP_DYNAMICS and age_group not in Config.LEAD_AGE_GROUPS:
        for lead_age in Config.LEAD_AGE_GROUPS:
            col_name = f'ili_{lead_age.replace("-", "_").replace("ì„¸", "")}'
            if col_name in df.columns and df[col_name].notna().any():
                chosen.append(col_name)
                print(f"   âœ… ì„ í–‰ ì§€í‘œ ì¶”ê°€: {col_name}")
    
    # íŠ¸ë Œë“œ í”¼ì²˜ ì¶”ê°€ (google_, naver_, twitter_ ì ‘ë‘ì‚¬ë¡œ ìë™ ê°ì§€)
    if Config.USE_TRENDS_DATA:
        trends_cols = [c for c in df.columns if c.startswith(('google_', 'naver_', 'twitter_'))]
        for col in trends_cols:
            if col in df.columns and df[col].notna().any():
                chosen.append(col)
        if trends_cols:
            print(f"   âœ… íŠ¸ë Œë“œ í”¼ì²˜ {len(trends_cols)}ê°œ ì¶”ê°€")
    
    # ğŸŒ¡ï¸ ë‚ ì”¨ í”¼ì²˜ ì¶”ê°€ (PostgreSQL weather_data)
    weather_cols = ['min_temp', 'max_temp', 'avg_humidity']  # weather_data í…Œì´ë¸”ì˜ ì»¬ëŸ¼
    added_weather_cols = []
    for col in weather_cols:
        if col in df.columns and df[col].notna().any():
            added_weather_cols.append(col)
            chosen.append(col)
    
    if added_weather_cols:
        print(f"\nğŸŒ¡ï¸  ë‚ ì”¨ í”¼ì²˜ ëª¨ë¸ì— ì ìš©:")
        print(f"   âœ… PostgreSQL weather_data í…Œì´ë¸”ì—ì„œ ê°€ì ¸ì˜¨ {len(added_weather_cols)}ê°œ í”¼ì²˜ ì¶”ê°€")
        print(f"      - {added_weather_cols}")
        # ê° ë‚ ì”¨ í”¼ì²˜ì˜ í†µê³„ ì¶œë ¥
        for col in added_weather_cols:
            data = df[col].dropna()
            if len(data) > 0:
                print(f"      â€¢ {col}: í‰ê·  {data.mean():.2f}, í‘œì¤€í¸ì°¨ {data.std():.2f}")
    else:
        print(f"\nâš ï¸  ë‚ ì”¨ í”¼ì²˜ ì—†ìŒ (weather_data í…Œì´ë¸” í™•ì¸ í•„ìš”)")
    
    print(f"   - ì„ íƒëœ í”¼ì²˜: {chosen}")
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in chosen:
        if col in df.columns:
            df[col] = df[col].fillna(0)
    
    # ILIê°€ ì—†ëŠ” í–‰ ì œê±°
    df = df[df['ili'].notna()].copy()
    
    # X, y êµ¬ì„±
    feat_names = chosen[:]
    X = df[feat_names].to_numpy(dtype=float)
    y = df['ili'].to_numpy(dtype=float)
    
    # Labels ìƒì„± (ì¼ë³„ ë°ì´í„°ì¸ì§€ ì£¼ì°¨ë³„ ë°ì´í„°ì¸ì§€ êµ¬ë¶„)
    if Config.USE_DAILY_DATA and 'date' in df.columns:
        # ì¼ë³„ ë°ì´í„°: date ì»¬ëŸ¼ ì‚¬ìš©
        labels = (df['season_norm'].astype(str) + f" ({age_group}) - " + df['date'].astype(str)).tolist()
    else:
        # ì£¼ì°¨ë³„ ë°ì´í„°: week ì»¬ëŸ¼ ì‚¬ìš©
        labels = (df['season_norm'].astype(str) + f" ({age_group}) - W" + df['week'].astype(int).astype(str)).tolist()
    
    print(f"\nâœ… ì—°ë ¹ëŒ€ '{age_group}' ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"   - X shape: {X.shape}")
    print(f"   - y shape: {y.shape}")
    print(f"   - Features: {feat_names}")
    print(f"   - ILI ë²”ìœ„: [{y.min():.2f}, {y.max():.2f}]")
    
    return X, y, labels, feat_names


# =========================
# data loader (multivariate-ready) - PostgreSQL ë²„ì „
# =========================
def load_and_prepare(
    df: pd.DataFrame, 
    use_exog: str = "auto",
    age_group: Optional[str] = None,
    subtype: Optional[str] = None
) -> Tuple[np.ndarray, np.ndarray, list, list]:
    """
    PostgreSQL ë˜ëŠ” CSV ë°ì´í„°ë¥¼ PatchTST ëª¨ë¸ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬
    
    Returns:
        X: (N, F) features (first column should be target variable)
        y: (N,) target (ì˜ì‚¬í™˜ì ë¶„ìœ¨)
        labels: list[str] for plotting ticks
        used_feat_names: list[str] feature column names (len=F)
    
    Parameters:
        df: PostgreSQL ë˜ëŠ” APIì—ì„œ ê°€ì ¸ì˜¨ DataFrame
        use_exog: ì™¸ìƒë³€ìˆ˜ ì‚¬ìš© ëª¨ë“œ
        age_group: íŠ¹ì • ì—°ë ¹ëŒ€ ì„ íƒ (ì˜ˆ: '19-49ì„¸', '65ì„¸ì´ìƒ', Noneì´ë©´ ìë™ ì„ íƒ)
        subtype: ì•„í˜• í•„í„°ë§ ('A', 'B', Noneì´ë©´ ìš°ì„¸ ì•„í˜• ì‚¬ìš©)
    """
    if df is None:
        raise ValueError("dfëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
    
    df = df.copy()
    
    print(f"\nğŸ“Š ì›ë³¸ ë°ì´í„° êµ¬ì¡°:")
    print(f"   - Shape: {df.shape}")
    print(f"   - Columns: {list(df.columns)}")
    
    # ===== ë‚ ì”¨ ë°ì´í„° ë³‘í•© (PostgreSQL) =====
    print(f"\nğŸŒ¡ï¸  ë‚ ì”¨ ë°ì´í„° ë³‘í•© ì‹œë„...")
    try:
        df_weather = load_weather_data_from_postgres()
        if df_weather is not None and not df_weather.empty:
            df = merge_weather_with_influenza(df, df_weather)
            
            # ë³‘í•© ì„±ê³µ í™•ì¸
            weather_cols_merged = [c for c in ['min_temp', 'max_temp', 'avg_humidity'] if c in df.columns]
            print(f"\n   âœ… ë‚ ì”¨ ë°ì´í„° ë³‘í•© ì„±ê³µ!")
            print(f"      - ë³‘í•© í›„ Shape: {df.shape}")
            print(f"      - ì¶”ê°€ëœ ë‚ ì”¨ ì»¬ëŸ¼: {weather_cols_merged}")
            print(f"      - ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜¨ ë‚ ì”¨ ë°ì´í„°ê°€ ëª¨ë¸ì— ì ìš©ë©ë‹ˆë‹¤.")
        else:
            print(f"   âš ï¸  ë‚ ì”¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¸í”Œë£¨ì—”ì ë°ì´í„°ë§Œìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"   âš ï¸  ë‚ ì”¨ ë°ì´í„° ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"   ì¸í”Œë£¨ì—”ì ë°ì´í„°ë§Œìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ===== PostgreSQL ë°ì´í„° í˜•ì‹ ê°ì§€ ë° ì²˜ë¦¬ =====
    is_postgres_format = all(col in df.columns for col in ['year', 'week', 'age_group'])
    
    if is_postgres_format:
        print(f"\nğŸ” PostgreSQL ë°ì´í„° í˜•ì‹ ê°ì§€ë¨ - ì—°ë ¹ëŒ€ë³„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        # ===== íŒ¬ë°ë¯¹ ê¸°ê°„ ë°ì´í„° ì²˜ë¦¬: ê³„ì ˆì„± íŒ¨í„´ ê¸°ë°˜ ë³´ê°„ =====
        print(f"\nğŸ’¡ íŒ¬ë°ë¯¹ ê¸°ê°„ ë°ì´í„° ì²˜ë¦¬ ì „ëµ: ê³„ì ˆì„± íŒ¨í„´ ê¸°ë°˜ ë³´ê°„")
        print(f"   - íŒ¬ë°ë¯¹ ê¸°ê°„ (2020-W14 ~ 2022-W22)ì„ ê²°ì¸¡ì¹˜ë¡œ í‘œì‹œ")
        print(f"   - ê³¼ê±° ê³„ì ˆ íŒ¨í„´(2017-2019)ìœ¼ë¡œ ë³´ê°„í•˜ì—¬ ì‹œê³„ì—´ ì—°ì†ì„± ìœ ì§€")
        
        before_count = len(df)
        
        # íŒ¬ë°ë¯¹ ê¸°ê°„ ë§ˆìŠ¤í¬ ìƒì„±
        pandemic_mask = (
            ((df['year'] == 2020) & (df['week'] >= 14)) |
            ((df['year'] == 2021)) |
            ((df['year'] == 2022) & (df['week'] <= 22))
        )
        
        pandemic_count = pandemic_mask.sum()
        print(f"\n   ğŸ“Š íŒ¬ë°ë¯¹ ê¸°ê°„ ë°ì´í„°: {pandemic_count:,}í–‰ ({pandemic_count/before_count*100:.1f}%)")
        
        # íŒ¬ë°ë¯¹ ê¸°ê°„ì˜ ILI ê°’ì„ NaNìœ¼ë¡œ ì„¤ì • (ê²°ì¸¡ì¹˜ í‘œì‹œ)
        # ë‚˜ì¤‘ì— ì—°ë ¹ëŒ€ë³„ë¡œ ì²˜ë¦¬í•œ í›„ ë³´ê°„í•  ê²ƒì„
        import numpy as np
        df.loc[pandemic_mask, 'ili'] = np.nan
        
        # ë‹¤ë¥¸ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë„ íŒ¬ë°ë¯¹ ê¸°ê°„ ë™ì•ˆ NaN ì²˜ë¦¬
        numeric_cols_to_mask = ['hospitalization', 'detection_rate', 'emergency_patients']
        for col in numeric_cols_to_mask:
            if col in df.columns:
                df.loc[pandemic_mask, col] = np.nan
        
        print(f"   âœ… íŒ¬ë°ë¯¹ ê¸°ê°„ ë°ì´í„°ë¥¼ ê²°ì¸¡ì¹˜(NaN)ë¡œ í‘œì‹œ ì™„ë£Œ")
        print(f"   â­ï¸  ì—°ë ¹ëŒ€ë³„ í•„í„°ë§ í›„ ë³´ê°„ ì²˜ë¦¬ ì˜ˆì •")
        
        # ì—°ë ¹ëŒ€ë³„ ë°ì´í„° í™•ì¸
        age_groups = df['age_group'].unique()
        print(f"\n   - ê³ ìœ  ì—°ë ¹ëŒ€: {len(age_groups)}ê°œ")
        print(f"   - ì—°ë ¹ëŒ€ ëª©ë¡: {sorted(age_groups)[:5]}...")
        
        # ì—°ë ¹ëŒ€ ì„ íƒ: íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •ëœ ê²½ìš° ìš°ì„  ì‚¬ìš©
        target_age_group = None
        
        if age_group is not None:
            # ì‚¬ìš©ì ì§€ì • ì—°ë ¹ëŒ€
            if age_group in age_groups:
                target_age_group = age_group
                print(f"   - ì‚¬ìš©ì ì§€ì • ì—°ë ¹ëŒ€ ì‚¬ìš©: '{age_group}'")
            else:
                print(f"   âš ï¸ ì§€ì •ëœ ì—°ë ¹ëŒ€ '{age_group}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   â„¹ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ë ¹ëŒ€: {sorted(age_groups)}")
        
        if target_age_group is None:
            # ìë™ ì„ íƒ: ë°ì´í„°ê°€ ê°€ì¥ í’ë¶€í•œ ì—°ë ¹ëŒ€
            # ìš°ì„ ìˆœìœ„: 19-49ì„¸ (ê°€ì¥ ì¼ë°˜ì ) > 65ì„¸ì´ìƒ > 65ì„¸ ì´ìƒ > 0-6ì„¸
            candidate_age_groups = ['19-49ì„¸', '65ì„¸ì´ìƒ', '65ì„¸ ì´ìƒ', '0-6ì„¸']
            
            for candidate in candidate_age_groups:
                if candidate in age_groups:
                    # í•´ë‹¹ ì—°ë ¹ëŒ€ì˜ ë°ì´í„° í’ˆì§ˆ í™•ì¸
                    temp_df = df[df['age_group'] == candidate].copy()
                    valid_ili = temp_df['ili'].notna().sum()
                    if valid_ili > 100:  # ìµœì†Œ 100ê°œ ì´ìƒì˜ ìœ íš¨ ë°ì´í„°
                        target_age_group = candidate
                        break
        
        if target_age_group and target_age_group in age_groups:
            print(f"   - '{target_age_group}' ì—°ë ¹ëŒ€ ë°ì´í„° ì‚¬ìš©")
            df_age = df[df['age_group'] == target_age_group].copy()
            
            # ì •ë ¬ (ë³´ê°„ ì „ í•„ìˆ˜)
            df_age = df_age.sort_values(['year', 'week']).reset_index(drop=True)
            
            # ===== íŒ¬ë°ë¯¹ ê¸°ê°„ ê²°ì¸¡ì¹˜ ë³´ê°„: ê³„ì ˆì„± íŒ¨í„´ ê¸°ë°˜ =====
            print(f"\n   ğŸ”§ íŒ¬ë°ë¯¹ ê¸°ê°„ ê²°ì¸¡ì¹˜ ë³´ê°„ ì‹œì‘ (ê³„ì ˆì„± íŒ¨í„´ ê¸°ë°˜)...")
            
            # ILI ë³´ê°„ ì „ ê²°ì¸¡ì¹˜ ê°œìˆ˜
            ili_nan_before = df_age['ili'].isna().sum()
            print(f"      - ILI ê²°ì¸¡ì¹˜: {ili_nan_before}ê°œ")
            
            if ili_nan_before > 0:
                # âœ… ê³„ì ˆì„± íŒ¨í„´ ê¸°ë°˜ ë³´ê°„ (Seasonal Pattern Interpolation)
                print(f"      - ë³´ê°„ ë°©ë²•: ê³¼ê±° ê³„ì ˆì„± íŒ¨í„´ (2017-2019 ê¸°ì¤€)")
                
                # 1ï¸âƒ£ íŒ¬ë°ë¯¹ ì´ì „ ê¸°ê°„ì˜ ì£¼ì°¨ë³„ í‰ê·  íŒ¨í„´ ê³„ì‚°
                pre_pandemic_mask = (df_age['year'] >= 2017) & (df_age['year'] <= 2019)
                df_pre_pandemic = df_age[pre_pandemic_mask & df_age['ili'].notna()].copy()
                
                # ì£¼ì°¨ë³„ í‰ê·  ILI ê³„ì‚°
                weekly_pattern = df_pre_pandemic.groupby('week')['ili'].mean()
                print(f"      - ì°¸ì¡° ë°ì´í„°: 2017-2019ë…„ ({len(df_pre_pandemic)}í–‰)")
                print(f"      - ì£¼ì°¨ë³„ íŒ¨í„´: {len(weekly_pattern)}ê°œ ì£¼ì°¨")
                
                # 2ï¸âƒ£ íŒ¬ë°ë¯¹ ê¸°ê°„(NaN) ë°ì´í„°ë¥¼ ì£¼ì°¨ë³„ í‰ê·  íŒ¨í„´ìœ¼ë¡œ ëŒ€ì²´
                pandemic_nan_mask = df_age['ili'].isna()
                for idx in df_age[pandemic_nan_mask].index:
                    week_num = df_age.loc[idx, 'week']
                    # í•´ë‹¹ ì£¼ì°¨ì˜ ê³¼ê±° í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                    if week_num in weekly_pattern.index:
                        df_age.loc[idx, 'ili'] = weekly_pattern[week_num]
                    else:
                        # í˜¹ì‹œ ì£¼ì°¨ê°€ ì—†ìœ¼ë©´ ì „ì²´ í‰ê·  ì‚¬ìš©
                        df_age.loc[idx, 'ili'] = weekly_pattern.mean()
                
                ili_nan_after = df_age['ili'].isna().sum()
                filled_count = ili_nan_before - ili_nan_after
                print(f"      âœ… ILI ë³´ê°„ ì™„ë£Œ: {filled_count}ê°œ ì±„ì›Œì§ (ê³„ì ˆ íŒ¨í„´ ê¸°ë°˜)")
                
                # 3ï¸âƒ£ ìŒìˆ˜ ê°’ ì œê±° (ILIëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•¨)
                negative_count = (df_age['ili'] < 0).sum()
                if negative_count > 0:
                    print(f"      âš ï¸ ìŒìˆ˜ ê°’ {negative_count}ê°œ ë°œê²¬ - 0ìœ¼ë¡œ ëŒ€ì²´")
                    df_age.loc[df_age['ili'] < 0, 'ili'] = 0
            
            # 4ï¸âƒ£ ë‹¤ë¥¸ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë„ ê³„ì ˆì„± íŒ¨í„´ìœ¼ë¡œ ë³´ê°„
            numeric_cols_to_interpolate = ['hospitalization', 'detection_rate', 'emergency_patients']
            for col in numeric_cols_to_interpolate:
                if col in df_age.columns:
                    nan_count = df_age[col].isna().sum()
                    if nan_count > 0:
                        # ê³¼ê±° ì£¼ì°¨ë³„ í‰ê·  íŒ¨í„´ ê³„ì‚°
                        pre_pandemic_mask = (df_age['year'] >= 2017) & (df_age['year'] <= 2019)
                        df_pre_pandemic = df_age[pre_pandemic_mask & df_age[col].notna()].copy()
                        
                        if len(df_pre_pandemic) > 0:
                            weekly_pattern = df_pre_pandemic.groupby('week')[col].mean()
                            
                            # íŒ¬ë°ë¯¹ ê¸°ê°„ ê²°ì¸¡ì¹˜ë¥¼ íŒ¨í„´ìœ¼ë¡œ ëŒ€ì²´
                            col_nan_mask = df_age[col].isna()
                            for idx in df_age[col_nan_mask].index:
                                week_num = df_age.loc[idx, 'week']
                                if week_num in weekly_pattern.index:
                                    df_age.loc[idx, col] = weekly_pattern[week_num]
                                else:
                                    df_age.loc[idx, col] = weekly_pattern.mean()
                            
                            # ìŒìˆ˜ ê°’ ì œê±°
                            if (df_age[col] < 0).sum() > 0:
                                df_age.loc[df_age[col] < 0, col] = 0
                            
                            print(f"      âœ… {col} ë³´ê°„ ì™„ë£Œ (ê³„ì ˆ íŒ¨í„´ ê¸°ë°˜)")
                        else:
                            # ì°¸ì¡° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ medianìœ¼ë¡œ ëŒ€ì²´
                            median_val = df_age[col].median()
                            if pd.notna(median_val):
                                df_age[col] = df_age[col].fillna(median_val)
                            print(f"      âš ï¸ {col} ë³´ê°„: ì°¸ì¡° ë°ì´í„° ë¶€ì¡± - median ì‚¬ìš©")
            
            print(f"\n   âœ… íŒ¬ë°ë¯¹ ê¸°ê°„ ë³´ê°„ ì™„ë£Œ - ì‹œê³„ì—´ ì—°ì†ì„± ìœ ì§€")
            print(f"   ğŸ“Š ìµœì¢… ë°ì´í„°: {len(df_age)}í–‰")
            
            # ì˜ˆë°©ì ‘ì¢…ë¥ ì´ ëª¨ë‘ NaNì¸ ê²½ìš° ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
            if df_age['vaccine_rate'].notna().sum() == 0:
                print(f"   - '{target_age_group}' ì—°ë ¹ëŒ€ì— ì˜ˆë°©ì ‘ì¢…ë¥  ë°ì´í„° ì—†ìŒ - ì „ì²´ í‰ê·  ì‚¬ìš©")
                # ì—°ë„/ì£¼ì°¨ë³„ë¡œ ì „ì²´ ì—°ë ¹ëŒ€ì˜ ì˜ˆë°©ì ‘ì¢…ë¥  í‰ê·  ê³„ì‚°
                vaccine_avg = df.groupby(['year', 'week'], as_index=False)['vaccine_rate'].mean()
                vaccine_avg = vaccine_avg.rename(columns={'vaccine_rate': 'vaccine_rate_avg'})
                df_age = df_age.merge(vaccine_avg, on=['year', 'week'], how='left')
                df_age['vaccine_rate'] = df_age['vaccine_rate_avg']
                df_age = df_age.drop(columns=['vaccine_rate_avg'])
            
            df = df_age
        else:
            # ì ì ˆí•œ ë‹¨ì¼ ì—°ë ¹ëŒ€ê°€ ì—†ìœ¼ë©´ ì—°ë„/ì£¼ì°¨ë³„ í‰ê·  ì‚¬ìš©
            print(f"   - ì—°ë„/ì£¼ì°¨ë³„ ì „ì²´ ì—°ë ¹ëŒ€ í‰ê·  ì‚¬ìš©")
            numeric_cols = ['ili', 'hospitalization', 'detection_rate', 'vaccine_rate', 'emergency_patients']
            agg_dict = {col: 'mean' for col in numeric_cols if col in df.columns}
            agg_dict['subtype'] = 'first'  # ì•„í˜•ì€ ì²« ê°’ ì‚¬ìš©
            
            df = df.groupby(['year', 'week'], as_index=False).agg(agg_dict)
        
        # ì •ë ¬
        df = df.sort_values(['year', 'week']).reset_index(drop=True)
        
        # season_norm ìƒì„± (week 36 ì´ìƒì€ í˜„ì¬ ì—°ë„ ì‹œì¦Œ, ë¯¸ë§Œì€ ë‹¤ìŒ ì—°ë„ ì‹œì¦Œ)
        df['season_norm'] = df.apply(
            lambda row: f"{int(row['year'])}-{int(row['year'])+1}" if row['week'] >= 36 
                       else f"{int(row['year'])-1}-{int(row['year'])}",
            axis=1
        )
        
        print(f"\nâœ… PostgreSQL ë°ì´í„° ë³€í™˜ ì™„ë£Œ:")
        print(f"   - ë³€í™˜ í›„ Shape: {df.shape}")
        print(f"   - ì—°ë„ ë²”ìœ„: {df['year'].min():.0f} ~ {df['year'].max():.0f}")
        print(f"   - ì£¼ì°¨ ë²”ìœ„: {df['week'].min():.0f} ~ {df['week'].max():.0f}")
        print(f"   - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(df)}")
    
    # ===== ì¼ë³„ ë°ì´í„° ë³€í™˜ (ë°”ìš°ì‹œì•ˆ ë³´ê°„) =====
    if Config.USE_DAILY_DATA:
        print(f"\nğŸ”„ ì£¼ì°¨ë³„ â†’ ì¼ë³„ ë°ì´í„° ë³€í™˜ ì‹œì‘...")
        print(f"   - ë³´ê°„ ë°©ë²•: {Config.DAILY_INTERP_METHOD.upper()}")
        print(f"   - ë°”ìš°ì‹œì•ˆ í‘œì¤€í¸ì°¨: {Config.GAUSSIAN_STD}")
        
        # season_norm ìƒì„± (ì•„ì§ ì—†ìœ¼ë©´)
        if 'season_norm' not in df.columns and {'year', 'week'}.issubset(df.columns):
            df['season_norm'] = df.apply(
                lambda row: f"{int(row['year'])}-{int(row['year'])+1}" if row['week'] >= 36 
                           else f"{int(row['year'])-1}-{int(row['year'])}",
                axis=1
            )
        
        # ì¼ë³„ ë³€í™˜
        df = weekly_to_daily_interp_gaussian(
            df,
            season_col="season_norm",
            week_col="week",
            target_col="ili",
            method=Config.DAILY_INTERP_METHOD,
            gaussian_std=Config.GAUSSIAN_STD
        )
        
        # SEQ_LEN, PRED_LENì„ ì¼ë³„ë¡œ ì—…ë°ì´íŠ¸
        global SEQ_LEN, PRED_LEN
        SEQ_LEN = Config.DAILY_SEQ_LEN
        PRED_LEN = Config.DAILY_PRED_LEN
        
        print(f"   âœ… ì¼ë³„ ë°ì´í„° ë³€í™˜ ì™„ë£Œ!")
        print(f"   - ìƒˆë¡œìš´ ì…ë ¥ ê¸¸ì´ (SEQ_LEN): {SEQ_LEN}ì¼")
        print(f"   - ìƒˆë¡œìš´ ì˜ˆì¸¡ ê¸¸ì´ (PRED_LEN): {PRED_LEN}ì¼")
    
    # âš ï¸  ì •ë ¬: year, weekë§Œ ì‚¬ìš© (season_norm ì •ë ¬ ì œê±°)
    # season_norm ê¸°ì¤€ ì •ë ¬ì€ ì‹œê°„ ìˆœì„œë¥¼ íŒŒê´´í•¨ (week 1ì´ week 36ë³´ë‹¤ ì•ìœ¼ë¡œ ê°)
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date").reset_index(drop=True)
    elif {"year", "week"}.issubset(df.columns):
        # year, weekë§Œ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ìˆœì„œ ìœ ì§€
        df["year"] = pd.to_numeric(df["year"], errors="coerce")
        df["week"] = pd.to_numeric(df["week"], errors="coerce")
        df = df.sort_values(["year", "week"]).reset_index(drop=True)
        print(f"   - ì •ë ¬: year, week ê¸°ì¤€ (ì‹œê°„ ìˆœì„œ ìœ ì§€)")
        
        # ğŸ”´ ì¤‘ë³µ ì œê±°: ê°™ì€ (year, week) ì¡°í•©ì´ ì—¬ëŸ¬ ê°œ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë§Œ ìœ ì§€
        before_len = len(df)
        df = df.drop_duplicates(subset=["year", "week"], keep="first")
        after_len = len(df)
        if before_len != after_len:
            print(f"   âš ï¸ ì¤‘ë³µ {before_len - after_len}ê°œ ì œê±°ë¨ (ë™ì¼ year/week)")
    elif "label" in df.columns:
        df = df.sort_values(["label"]).reset_index(drop=True)

    # íƒ€ê¹ƒ ë³€ìˆ˜ í™•ì¸
    if "ili" not in df.columns:
        raise ValueError("ë°ì´í„°ì— 'ili' (ì˜ì‚¬í™˜ì ë¶„ìœ¨) ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    df["ili"] = pd.to_numeric(df["ili"], errors="coerce")
    if df["ili"].isna().any():
        print(f"   âš ï¸ 'ili' ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ {df['ili'].isna().sum()}ê°œ ë°œê²¬ - ë³´ê°„ ì²˜ë¦¬")
        df["ili"] = df["ili"].interpolate(method="linear", limit_direction="both").fillna(df["ili"].median())
    
    # --- Seasonality feature ì¶”ê°€ ---
    if "week" in df.columns:
        df["week_sin"] = np.sin(2 * np.pi * df["week"] / 52.0)
    else:
        df["week_sin"] = 0.0
    
    # --- ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€ (PostgreSQL ë²„ì „) ---
    if Config.USE_AGE_GROUP_DYNAMICS and age_group and age_group not in Config.LEAD_AGE_GROUPS:
        print(f"\nğŸ”— ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€ ì¤‘ (PostgreSQL)...")
        # í˜„ì¬ dfëŠ” í•„í„°ë§ëœ ì—°ë ¹ëŒ€ë§Œ ìˆìœ¼ë¯€ë¡œ, ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” merged CSVì—ì„œ ì§ì ‘ ë¡œë“œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        try:
            csv_path = pick_csv_path()
            full_df = pd.read_csv(csv_path)
            for lead_age in Config.LEAD_AGE_GROUPS:
                lead_data = full_df[full_df['age_group'] == lead_age].copy()
                if not lead_data.empty and 'ili' in lead_data.columns:
                    lead_data = lead_data.sort_values(['year', 'week']).reset_index(drop=True)
                    lead_ili = lead_data[['year', 'week', 'ili']].copy()
                    col_name = f'ili_{lead_age.replace("-", "_").replace("ì„¸", "")}'
                    lead_ili = lead_ili.rename(columns={'ili': col_name})
                    df = df.merge(lead_ili, on=['year', 'week'], how='left')
                    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                    if col_name in df.columns:
                        df[col_name] = df[col_name].fillna(0)
                    print(f"   âœ… {lead_age} ILI ì¶”ê°€: {col_name}")
        except Exception as e:
            print(f"   âš ï¸  ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    # --- íŠ¸ë Œë“œ ë°ì´í„° ë³‘í•© (PostgreSQL trends DB) ---
    if Config.USE_TRENDS_DATA:
        print(f"\nğŸ” íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ ì¤‘ (PostgreSQL {Config.TRENDS_DB_NAME} DB)...")
        try:
            from database.db_utils import load_trends_from_postgres
            trends_df = load_trends_from_postgres(
                table_name=Config.TRENDS_TABLE_NAME,
                db_name=Config.TRENDS_DB_NAME
            )
            if not trends_df.empty and 'year' in trends_df.columns and 'week' in trends_df.columns:
                df = df.merge(trends_df, on=['year', 'week'], how='left')
                trends_cols = [c for c in trends_df.columns if c not in ['year', 'week']]
                print(f"   âœ… íŠ¸ë Œë“œ í”¼ì²˜ ì¶”ê°€: {len(trends_cols)}ê°œ ì»¬ëŸ¼")
                for col in trends_cols:
                    if col in df.columns:
                        df[col] = df[col].fillna(0)
            else:
                print(f"   âš ï¸  íŠ¸ë Œë“œ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ year, week ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"   âš ï¸  íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ğŸ’¡ ë¨¼ì € 'python database/update_database.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    # --- Alias ë§¤í•‘ ---
    if "hospitalization" in df.columns and "respiratory_index" not in df.columns:
        df["respiratory_index"] = df["hospitalization"]
    if "case_count" in df.columns and "respiratory_index" not in df.columns:
        df["respiratory_index"] = df["case_count"]

    # ê¸°í›„ í”¼ì²˜ í›„ë³´
    climate_feats = []
    if "wx_week_avg_temp" in df.columns:     climate_feats.append("wx_week_avg_temp")
    if "wx_week_avg_rain" in df.columns:     climate_feats.append("wx_week_avg_rain")
    if "wx_week_avg_humidity" in df.columns: climate_feats.append("wx_week_avg_humidity")
    if "detection_rate" in df.columns:       climate_feats.append("detection_rate")  # PostgreSQL íŠ¹ì„±

    # ì™¸ìƒ í›„ë³´ ì¡´ì¬ ì—¬ë¶€
    has_vax  = "vaccine_rate" in df.columns
    has_resp = "respiratory_index" in df.columns or "hospitalization" in df.columns


    # ëª¨ë“  column_mapping ë‚´ë¶€ëª…ì„ featureë¡œ ê°•ì œ í¬í•¨
    column_mapping = {
        'ì—°ë„': 'year',
        'ì£¼ì°¨': 'week',
        'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
        'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate',
        'ì…ì›í™˜ì ìˆ˜': 'hospitalization',
        'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
        'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients',
        'ì•„í˜•': 'subtype'
    }
    
    # hospitalization ì œì™¸ ì„¤ì • í™•ì¸
    exclude_hosp = getattr(Config, 'EXCLUDE_HOSPITALIZATION', False)
    
    # weekëŠ” week_sinìœ¼ë¡œ ëŒ€ì²´, ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
    chosen = []
    for v in column_mapping.values():
        if v == "week":
            chosen.append("week_sin")
        elif v == "hospitalization" and exclude_hosp:
            # hospitalization ì œì™¸
            continue
        else:
            chosen.append(v)
    # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ë³´ì¡´
    chosen = [x for i, x in enumerate(chosen) if x not in chosen[:i]]
    
    if exclude_hosp:
        print("   âš ï¸ hospitalization í”¼ì²˜ ì œì™¸ë¨ (Config.EXCLUDE_HOSPITALIZATION=True)")

    # ìˆ«ìí™” & ë³´ê°„
    for c in chosen:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            if df[c].isna().any():
                # ì„ í˜• ë³´ê°„ í›„ medianìœ¼ë¡œ ë‚¨ì€ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
                df[c] = df[c].interpolate(method="linear", limit_direction="both")
                # ì—¬ì „íˆ NaNì´ ìˆìœ¼ë©´ median ì‚¬ìš© (medianë„ NaNì´ë©´ 0 ì‚¬ìš©)
                median_val = df[c].median()
                if pd.isna(median_val):
                    median_val = 0.0
                df[c] = df[c].fillna(median_val)
        else:
            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
            print(f"   âš ï¸ ì»¬ëŸ¼ '{c}'ê°€ ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
            df[c] = 0.0

    # ë¼ë²¨
    if "label" in df.columns and df["label"].notna().any():
        labels = df["label"].astype(str).tolist()
    elif {"season_norm","week"}.issubset(df.columns):
        labels = (df["season_norm"].astype(str) + " season - W" + df["week"].astype(int).astype(str)).tolist()
    else:
        labels = [f"idx_{i}" for i in range(len(df))]

    # X, y êµ¬ì„±
    feat_names = chosen[:]
    if INCLUDE_SEASONAL_FEATS and "week_sin" in df.columns:
        feat_names.append("week_sin")
    
    # ì—°ë ¹ëŒ€ë³„ ë™í•™ í”¼ì²˜ ì¶”ê°€ (PostgreSQL ë²„ì „)
    if Config.USE_AGE_GROUP_DYNAMICS and age_group and age_group not in Config.LEAD_AGE_GROUPS:
        for lead_age in Config.LEAD_AGE_GROUPS:
            col_name = f'ili_{lead_age.replace("-", "_").replace("ì„¸", "")}'
            if col_name in df.columns:
                feat_names.append(col_name)
                print(f"   âœ… ì„ í–‰ ì§€í‘œ í”¼ì²˜ ì¶”ê°€: {col_name}")
    
    # íŠ¸ë Œë“œ í”¼ì²˜ ì¶”ê°€ (PostgreSQL ë²„ì „)
    if Config.USE_TRENDS_DATA:
        trends_cols = [c for c in df.columns if c.startswith(('google_', 'naver_', 'twitter_'))]
        for col in trends_cols:
            if col in df.columns:
                feat_names.append(col)
        if trends_cols:
            print(f"   âœ… íŠ¸ë Œë“œ í”¼ì²˜ {len(trends_cols)}ê°œ ì¶”ê°€")

    # ğŸŒ¡ï¸ ë‚ ì”¨ í”¼ì²˜ ì¶”ê°€ (PostgreSQL weather_data)
    weather_cols = ['min_temp', 'max_temp', 'avg_humidity']  # weather_data í…Œì´ë¸”ì˜ ì»¬ëŸ¼
    added_weather_cols = []
    for col in weather_cols:
        if col in df.columns:
            added_weather_cols.append(col)
            feat_names.append(col)
    
    if added_weather_cols:
        print(f"\nğŸŒ¡ï¸  ë‚ ì”¨ í”¼ì²˜ ëª¨ë¸ì— ì ìš©:")
        print(f"   âœ… PostgreSQL weather_data í…Œì´ë¸”ì—ì„œ ê°€ì ¸ì˜¨ {len(added_weather_cols)}ê°œ í”¼ì²˜ ì¶”ê°€")
        print(f"      - {added_weather_cols}")
        # ê° ë‚ ì”¨ í”¼ì²˜ì˜ í†µê³„ ì¶œë ¥
        for col in added_weather_cols:
            data = df[col].dropna()
            if len(data) > 0:
                print(f"      â€¢ {col}: í‰ê·  {data.mean():.2f}, í‘œì¤€í¸ì°¨ {data.std():.2f}")
    else:
        print(f"\nâš ï¸  ë‚ ì”¨ í”¼ì²˜ ì—†ìŒ (weather_data í…Œì´ë¸” í™•ì¸ í•„ìš”)")
    
    # ì„ íƒëœ ì…ë ¥ í”¼ì²˜ ë¡œê·¸
    print(f"\n[Data] Exogenous detected -> vaccine_rate: {has_vax} | respiratory/hospitalization: {has_resp} | climate_feats: {climate_feats}")
    print(f"[Data] Selected feature columns (order) -> {feat_names}")

    X = df[feat_names].to_numpy(dtype=float)
    y = df["ili"].to_numpy(dtype=float)
    
    # ğŸ” vaccine_rate ì§„ë‹¨
    if 'vaccine_rate' in feat_names:
        vax_idx = feat_names.index('vaccine_rate')
        vax_data = X[:, vax_idx]
        print(f"\nğŸ”¬ vaccine_rate ë°ì´í„° ë¶„ì„:")
        print(f"   - ë²”ìœ„: [{vax_data.min():.4f}, {vax_data.max():.4f}]")
        print(f"   - í‰ê· : {vax_data.mean():.4f}, í‘œì¤€í¸ì°¨: {vax_data.std():.4f}")
        print(f"   - ë³€ë™ê³„ìˆ˜(CV): {vax_data.std()/vax_data.mean():.4f}")
        print(f"   - 0ì¸ ê°’: {(vax_data == 0).sum()}ê°œ / {len(vax_data)}ê°œ")
        print(f"   - ìƒê´€ê³„ìˆ˜ (vaccine_rate vs ili): {np.corrcoef(vax_data, y)[0,1]:.4f}")
    
    print(f"\nâœ… ìµœì¢… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"   - X shape: {X.shape}")
    print(f"   - y shape: {y.shape}")
    print(f"   - Features: {len(feat_names)}")
    
    return X, y, labels, feat_names

# =========================
# Loss Function
# =========================
class PeakAwareLoss(nn.Module):
    """
    ê³ ì • ê¸°ì¤€ Peak + ì§„í­ ë³´ì¡´ + Horizon Weighting Loss
    
    íŠ¹ì§•:
    1. Peak êµ¬ê°„(ìƒìœ„ quantile)ì— ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš©
    2. ì§„í­ ë³´ì¡´ í•­ìœ¼ë¡œ peak flattening ë°©ì§€
    3. Horizon weighting: ì˜ˆì¸¡ êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜ (í”¼í¬ê°€ ì£¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í›„ë°˜ë¶€ ê°•ì¡°)
    4. MAE ê¸°ë°˜ìœ¼ë¡œ outlierì— robust
    """
    def __init__(self, peak_quantile=0.9, alpha=4.0, beta=0.3, 
                 pred_len=4, horizon_mode="exponential", 
                 horizon_exp_scale=1.2, horizon_tail_boost=2.5, horizon_tail_count=2):
        super().__init__()
        self.peak_quantile = peak_quantile
        self.alpha = alpha  # í”¼í¬ ê°€ì¤‘ì¹˜
        self.beta = beta    # ì§„í­ ë³´ì¡´ ê°€ì¤‘ì¹˜
        self.mae = nn.L1Loss(reduction="none")
        
        # ğŸ”´ Horizon Weighting ê³„ì‚°
        h_weights = self._compute_horizon_weights(
            pred_len, horizon_mode, horizon_exp_scale, 
            horizon_tail_boost, horizon_tail_count
        )
        # tensorë¡œ ë³€í™˜í•˜ì—¬ ë“±ë¡ (í•™ìŠµë˜ì§€ ì•ŠëŠ” ë²„í¼)
        self.register_buffer('horizon_weights', torch.from_numpy(h_weights).float())
        
        print(f"[Loss] Horizon weights ({horizon_mode}): {h_weights}")
    
    def _compute_horizon_weights(self, pred_len, mode, exp_scale, tail_boost, tail_count):
        """ì˜ˆì¸¡ êµ¬ê°„ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        if mode == "exponential":
            # ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ (ë’¤ë¡œ ê°ˆìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¦ê°€)
            h_weights = np.exp(np.linspace(0, exp_scale, pred_len))
        elif mode == "tail_boost":
            # ë’¤ìª½ Nê°œë§Œ ë¶€ìŠ¤íŠ¸
            h_weights = np.ones(pred_len)
            h_weights[-tail_count:] *= tail_boost
        else:  # uniform
            h_weights = np.ones(pred_len)
        
        # ì •ê·œí™” (í•©ì´ pred_lenì´ ë˜ë„ë¡)
        h_weights = h_weights / h_weights.sum() * pred_len
        return h_weights
    
    def forward(self, pred, target):
        """
        Args:
            pred: (B, H) ì˜ˆì¸¡ê°’
            target: (B, H) ì‹¤ì œê°’
        Returns:
            loss: scalar
        """
        # Base MAE
        base_loss = self.mae(pred, target)  # (B, H)
        
        # ğŸ”´ í”¼í¬ êµ¬ê°„ ê°€ì¤‘ (ë°°ì¹˜ë³„ ë™ì  threshold)
        with torch.no_grad():
            peak_threshold = torch.quantile(target, self.peak_quantile)
            peak_mask = target >= peak_threshold
            weights = torch.ones_like(target)
            weights[peak_mask] = self.alpha
        
        # ğŸ”´ Horizon weighting ì ìš©
        # horizon_weights: (H,) -> (1, H)ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        horizon_w = self.horizon_weights.view(1, -1)  # (1, H)
        weighted_mae = (base_loss * weights * horizon_w).mean()
        
        # ğŸ”´ ì§„í­ ë³´ì¡´ í•­ (peak flattening ë°©ì§€)
        # ê° ë°°ì¹˜ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ê°’ ì°¨ì´ë¥¼ íŒ¨ë„í‹°ë¡œ ì¶”ê°€
        pred_max = pred.max(dim=1).values    # (B,)
        target_max = target.max(dim=1).values  # (B,)
        amp_loss = torch.abs(pred_max - target_max).mean()
        
        # ì´ ì†ì‹¤
        total_loss = weighted_mae + self.beta * amp_loss
        
        return total_loss

# =========================
# dataset
# =========================
class PatchTSTDataset(Dataset):
    """Multivariate X (N,F) + y (N,) -> (patchified) windows."""
    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len:int, pred_len:int, patch_len:int, stride:int):
        assert len(X) == len(y)
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)
        self.seq_len, self.pred_len = seq_len, pred_len
        self.patch_len, self.stride = patch_len, stride
        max_start = len(self.y) - (seq_len + pred_len)
        self.indices = list(range(max(0, max_start + 1)))

    def __len__(self): return len(self.indices)

    def __getitem__(self, idx):
        i = self.indices[idx]
        seq_X = self.X[i:i+self.seq_len, :]                      # (L, F)
        tgt_y = self.y[i+self.seq_len:i+self.seq_len+self.pred_len]  # (H,)

        # patchify along time axis
        patches = []
        pos = 0
        while pos + self.patch_len <= self.seq_len:
            patches.append(seq_X[pos:pos+self.patch_len, :])     # (patch_len, F)
            pos += self.stride
        X_patch = np.stack(patches, axis=0)                      # (P, patch_len, F)
        return torch.from_numpy(X_patch).float(), torch.from_numpy(tgt_y).float(), i

# =========================
# model (Multi-Scale CNN + TokenConvMixer + PatchTST + AttnPool)
# =========================
class MultiScaleCNNPatchEmbed(nn.Module):
    """
    (B, P, L, F) -> [ê° íŒ¨ì¹˜] ë©€í‹°ìŠ¤ì¼€ì¼ Conv1d ë¶„ê¸°(k=2/3/5, ë˜ í•˜ë‚˜ëŠ” dilation=2) â†’ GAP â†’ (B, P, D)
    - ë¶„ê¸° 4ê°œ ì¶œë ¥ concat â†’ D_MODEL
    - íŒ¨ì¹˜ ë‚´ë¶€ì˜ ê¸‰ê²©/ì™„ë§Œ/ì”ì§„ë™ íŒ¨í„´ì„ ë™ì‹œì— í¬ì°©
    """
    def __init__(self, in_features: int, patch_len: int, d_model: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % 4 == 0, "d_modelì€ 4ì˜ ë°°ìˆ˜ê°€ ë˜ì–´ì•¼ ë©€í‹°ìŠ¤ì¼€ì¼ ë¶„ê¸° í•©ì‚°ì´ ë§ìŠµë‹ˆë‹¤."
        out_ch = d_model // 4
    # ì»¤ë„ í¬ê¸°ë¥¼ patch_lenì— ë¹„ë¡€í•˜ê²Œ ì„¤ì •
        self.b2 = nn.Conv1d(in_features, out_ch, kernel_size=1, padding=0)
        self.b3 = nn.Conv1d(in_features, out_ch, kernel_size=3, padding=1)
        self.b5 = nn.Conv1d(in_features, out_ch, kernel_size=5, padding=2)
        self.bd = nn.Conv1d(in_features, out_ch, kernel_size=3, padding=2, dilation=2)

        self.bn   = nn.BatchNorm1d(d_model)
        self.act  = nn.GELU()
        self.pool = nn.AdaptiveAvgPool1d(1)   # (B*P, D, L) â†’ (B*P, D, 1)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        # x: (B, P, L, F)
        B, P, L, F = x.shape
        x = x.view(B*P, L, F).permute(0, 2, 1)        # (B*P, F, L)

        z = torch.cat([self.b2(x), self.b3(x), self.b5(x), self.bd(x)], dim=1)  # (B*P, D, L)
        z = self.act(self.bn(z))
        z = self.pool(z).squeeze(-1)                  # (B*P, D)
        z = self.drop(z)
        return z.view(B, P, -1)                       # (B, P, D)

class TokenConvMixer(nn.Module):
    """
    íŒ¨ì¹˜ í† í° ê°„(P ì¶•) ë¡œì»¬ ì—°ì†ì„± ê°•í™”: DepthwiseConv1d(P-ì¶•) + PointwiseConv1d
    ì…ë ¥/ì¶œë ¥: (B, P, D)
    """
    def __init__(self, d_model: int, dropout: float = 0.1):
        super().__init__()
        self.dw = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)
        self.pw = nn.Conv1d(d_model, d_model, kernel_size=1)
        self.bn = nn.BatchNorm1d(d_model)
        self.act = nn.GELU()
        self.drop = nn.Dropout(dropout)

    def forward(self, z):              # (B, P, D)
        y = z.permute(0, 2, 1)         # (B, D, P)
        y = self.dw(y)
        y = self.pw(y)
        y = self.bn(y)
        y = self.act(y)
        y = self.drop(y)
        y = y.permute(0, 2, 1)         # (B, P, D)
        return z + y                   # Residual

class PositionalEncoding(nn.Module):
    def __init__(self, d_model:int, max_len:int=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).float().unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))
        pe[:,0::2] = torch.sin(position*div)
        if d_model % 2 == 1:
            pe[:,1::2] = torch.cos(position*div)[:, :pe[:,1::2].shape[1]]
        else:
            pe[:,1::2] = torch.cos(position*div)
        self.register_buffer("pe", pe.unsqueeze(0))
    def forward(self, x):
        P = x.size(1)
        return x + self.pe[:, :P, :]

class AttnPool(nn.Module):
    """Learnable-query attention pooling over patch tokens."""
    def __init__(self, d_model:int):
        super().__init__()
        self.q = nn.Parameter(torch.randn(1, 1, d_model))
        self.proj = nn.Linear(d_model, d_model, bias=False)
    def forward(self, z):           # z: (B, P, D)
        B,P,D = z.shape
        q = self.q.expand(B, -1, -1)                       # (B,1,D)
        k = self.proj(z)                                   # (B,P,D)
        attn = torch.softmax((q @ k.transpose(1,2)) / (D**0.5), dim=-1)  # (B,1,P)
        pooled = attn @ z                                  # (B,1,D)
        return pooled.squeeze(1)                           # (B,D)

class PatchTSTModel(nn.Module):
    def __init__(self, in_features:int, patch_len:int, d_model:int, n_heads:int,
                 n_layers:int, ff_dim:int, dropout:float, pred_len:int, head_hidden:List[int]):
        super().__init__()
        # â‘  ë©€í‹°ìŠ¤ì¼€ì¼ CNN íŒ¨ì¹˜ ì„ë² ë”©
        self.embed = MultiScaleCNNPatchEmbed(in_features, patch_len, d_model, dropout=dropout*0.5)
        # â‘¡ íŒ¨ì¹˜ í† í° ê°„ ë¡œì»¬ ì—°ì†ì„± ë¯¹ì„œ
        self.mixer = nn.Sequential(
            TokenConvMixer(d_model, dropout=dropout),
            TokenConvMixer(d_model, dropout=dropout),
        )
        # â‘¢ PatchTST ì¸ì½”ë”
        self.posenc = PositionalEncoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim,
            dropout=dropout, batch_first=True, activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.pool = AttnPool(d_model)

        # â‘£ Dual-head ì˜ˆì¸¡: Trend + Peak
        # head_hidden: MLP layers for feature extraction
        mlp_shared, in_dim = [], d_model
        for h in head_hidden[:2]:
            mlp_shared += [nn.Linear(in_dim, h), nn.GELU(), nn.Dropout(dropout)]
            in_dim = h
        self.shared_mlp = nn.Sequential(*mlp_shared) if mlp_shared else nn.Identity()
        
        # Dual heads
        self.head_trend = nn.Linear(in_dim, pred_len)  # ê¸°ë³¸ íŠ¸ë Œë“œ
        self.head_peak = nn.Linear(in_dim, pred_len)   # í”¼í¬ ë³´ì • (ì–‘ìˆ˜ë§Œ)

    def forward(self, x):
        # x: (B, P, L, F)
        z = self.embed(x)      # (B,P,D)
        z = self.mixer(z)      # (B,P,D)
        z = self.posenc(z)
        z = self.encoder(z)
        z = self.pool(z)       # (B,D)
        
        # Shared MLP
        z = self.shared_mlp(z)  # (B, hidden_dim)
        
        # Dual-head prediction with adaptive gating
        trend = self.head_trend(z)              # (B, H) - ê¸°ë³¸ íŠ¸ë Œë“œ
        peak = torch.relu(self.head_peak(z))    # (B, H) - í”¼í¬ ë³´ì • (ì–‘ìˆ˜ë§Œ)
        
        # trendê°€ í´ ë•Œ peak ì˜í–¥ ì¦ê°€ (sigmoid gating)
        return trend + peak * torch.sigmoid(trend)  # (B,H) - ìµœì¢… ì˜ˆì¸¡

    def correlation_loss(pred, true):
    # pred, true: (B, H)
        pred = pred - pred.mean(dim=1, keepdim=True)
        true = true - true.mean(dim=1, keepdim=True)
        corr = (pred * true).sum(dim=1) / (
            (pred.norm(dim=1) * true.norm(dim=1)) + 1e-6
        )
        return 1 - corr.mean()
    # =========================
# helpers
# =========================
def warmup_lr(ep:int, base_lr:float, warmup_epochs:int):
    if ep <= warmup_epochs:
        return base_lr * (ep / max(1, warmup_epochs))
    return base_lr

def batch_mae_in_original_units(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    """
    Compute MAE in original units for single-step or multi-step prediction.

    pred_b: (B,) or (B,1) or (B,H)
    y_b:    (B,H) or (B,)
    """
    # move to numpy
    p = pred_b.detach().cpu().numpy()
    t = y_b.detach().cpu().numpy()

    # ensure 2D
    if p.ndim == 1:
        p = p[:, None]          # (B,1)
    if t.ndim == 1:
        t = t[:, None]          # (B,1)

    # if prediction is single-step but target is multi-step, broadcast
    if p.shape[1] == 1 and t.shape[1] > 1:
        p = np.repeat(p, t.shape[1], axis=1)

    # flatten to (B*H, 1)
    p = p.reshape(-1, 1)
    t = t.reshape(-1, 1)

    # inverse scaling
    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)

    return float(np.mean(np.abs(p_orig - t_orig)))

def batch_rmse_in_original_units(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    """
    Compute RMSE in original units for single-step or multi-step prediction.
    """
    p = pred_b.detach().cpu().numpy()
    t = y_b.detach().cpu().numpy()

    if p.ndim == 1:
        p = p[:, None]
    if t.ndim == 1:
        t = t[:, None]

    if p.shape[1] == 1 and t.shape[1] > 1:
        p = np.repeat(p, t.shape[1], axis=1)

    p = p.reshape(-1, 1)
    t = t.reshape(-1, 1)

    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)

    return float(np.sqrt(np.mean((p_orig - t_orig)**2)))

def batch_mse_in_original_units(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    """
    Compute MSE in original units for single-step or multi-step prediction.
    """
    p = pred_b.detach().cpu().numpy()
    t = y_b.detach().cpu().numpy()

    if p.ndim == 1:
        p = p[:, None]
    if t.ndim == 1:
        t = t[:, None]

    if p.shape[1] == 1 and t.shape[1] > 1:
        p = np.repeat(p, t.shape[1], axis=1)

    p = p.reshape(-1, 1)
    t = t.reshape(-1, 1)

    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)

    return float(np.mean((p_orig - t_orig)**2))

def batch_corrcoef(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    """
    Pearson correlation coefficient (batch í‰ê· )
    pred_b, y_b: (B, H)
    """
    p = pred_b.detach().cpu().numpy().reshape(-1, 1)
    t = y_b.detach().cpu().numpy().reshape(-1, 1)
    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)

    if np.std(p_orig) < 1e-6 or np.std(t_orig) < 1e-6:
        return 0.0
    return float(np.corrcoef(p_orig, t_orig)[0,1])

# =========================
# train & evaluate
# =========================
def train_and_eval(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list):
    """
    X: (N,F), y: (N,), feat_names: ['ili', 'vaccine_rate', 'respiratory_index'] ë“±
    """
    set_seed(SEED)
    (s0,e0),(s1,e1),(s2,e2) = make_splits(len(y))
    X_tr, X_va, X_te = X[s0:e0], X[s1:e1], X[s2:e2]
    y_tr, y_va, y_te = y[s0:e0], y[s1:e1], y[s2:e2]
    lab_tr, lab_va, lab_te = labels[s0:e0], labels[s1:e1], labels[s2:e2]

    # ==== ë°ì´í„° ë¶„í•  ì§„ë‹¨ ====
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´:")
    print(f"   Train: {lab_tr[0]} ~ {lab_tr[-1]} ({len(y_tr)}ê°œ)")
    print(f"   Val:   {lab_va[0]} ~ {lab_va[-1]} ({len(y_va)}ê°œ)")
    print(f"   Test:  {lab_te[0]} ~ {lab_te[-1]} ({len(y_te)}ê°œ)")
    print(f"   Train y ë²”ìœ„: [{y_tr.min():.2f}, {y_tr.max():.2f}], í‰ê· : {y_tr.mean():.2f}")
    print(f"   Val   y ë²”ìœ„: [{y_va.min():.2f}, {y_va.max():.2f}], í‰ê· : {y_va.mean():.2f}")
    print(f"   Test  y ë²”ìœ„: [{y_te.min():.2f}, {y_te.max():.2f}], í‰ê· : {y_te.mean():.2f}")

    # ==== Scaling ====
    # Target scaler (íƒ€ê²Ÿ: Log ë³€í™˜ ì ìš©)
    scaler_y = get_scaler(for_target=True)
    y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
    y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
    y_te_sc = scaler_y.transform(y_te.reshape(-1,1)).ravel()

    # Feature scaler (í”¼ì²˜: Log ë³€í™˜ ë¯¸ì ìš©)
    scaler_x = get_scaler(for_target=False)
    X_tr_sc = scaler_x.fit_transform(X_tr)
    X_va_sc = scaler_x.transform(X_va)
    X_te_sc = scaler_x.transform(X_te)

    F = X.shape[1]
    print(f"[Shapes] X_tr:{X_tr.shape}, X_va:{X_va.shape}, X_te:{X_te.shape} | F={F}")
    print(f"[Info] Model input feature order -> {feat_names}")

    ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_va = PatchTSTDataset(X_va_sc, y_va_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_te = PatchTSTDataset(X_te_sc, y_te_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)

    # drop_last=False ë¡œ ë³€ê²½(ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ í•™ìŠµ ë°°ì¹˜ ë³´ì¥)
    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)

    model = PatchTSTModel(
        in_features=F, patch_len=PATCH_LEN, d_model=D_MODEL, n_heads=N_HEADS,
        n_layers=ENC_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT,
        pred_len=PRED_LEN, head_hidden=HEAD_HIDDEN
    ).to(DEVICE)

    # Loss / Optim / Scheduler
    crit = PeakAwareLoss(
        peak_quantile=Config.PEAK_THRESHOLD_QUANTILE,
        alpha=Config.PEAK_WEIGHT_ALPHA,
        beta=Config.AMPLITUDE_WEIGHT_BETA,
        pred_len=PRED_LEN,
        horizon_mode=Config.HORIZON_WEIGHT_MODE,
        horizon_exp_scale=Config.HORIZON_EXP_SCALE,
        horizon_tail_boost=Config.HORIZON_TAIL_BOOST,
        horizon_tail_count=Config.HORIZON_TAIL_COUNT
    ).to(DEVICE)
    
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)

    # ---- history for curves ----
    hist = {"train_loss":[], "val_loss":[], "train_mae":[], "val_mae":[]}

    best_val = float("inf"); best_state=None; noimp=0
    printed_batch_info = False
    for ep in range(1, EPOCHS+1):
        # ---- Train ----
        model.train(); tr_loss_sum=0; tr_mae_sum=0; n=0
        # warmup
        for g in opt.param_groups:
            g['lr'] = warmup_lr(ep, LR, WARMUP_EPOCHS)

        for Xb,yb,_ in dl_tr:
            if not printed_batch_info:
                # Xb: (B, P, L, F)  â† ìµœì¢… ëª¨ë¸ ì…ë ¥ í…ì„œ êµ¬ì¡°
                print(f"[Batch] Xb.shape={tuple(Xb.shape)} (B,P,L,F), yb.shape={tuple(yb.shape)}")
                print(f"[Batch] Feature order used -> {feat_names}")
                printed_batch_info = True
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            opt.zero_grad()
            pred = model(Xb)
            loss = crit(pred, yb)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
            bs=yb.size(0)
            tr_loss_sum += loss.item()*bs; n+=bs
            tr_mae_sum  += batch_mae_in_original_units(pred, yb, scaler_y)*bs

        tr_loss = tr_loss_sum / max(1,n)
        tr_mae  = tr_mae_sum  / max(1,n)

        # ---- Validation ----
        model.eval(); va_loss_sum=0; va_mae_sum=0; va_corr_sum = 0; n=0
        with torch.no_grad():
            for Xb,yb,_ in dl_va:
                Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
                pred = model(Xb); loss = crit(pred, yb)
                bs=yb.size(0)
                va_loss_sum += loss.item()*bs; n+=bs
                va_mae_sum  += batch_mae_in_original_units(pred, yb, scaler_y)*bs
                va_corr_sum += batch_corrcoef(pred, yb, scaler_y)*bs
        va_loss = va_loss_sum / max(1,n)
        va_mae  = va_mae_sum  / max(1,n)
        va_corr = va_corr_sum / max(1,n)

        scheduler.step()

        hist["train_loss"].append(tr_loss)
        hist["val_loss"].append(va_loss)
        hist["train_mae"].append(tr_mae)
        hist["val_mae"].append(va_mae)

        print(f"[Epoch {ep:03d}/{EPOCHS}] "
              f"LR={opt.param_groups[0]['lr']:.6f} | "
              f"Loss T/V={tr_loss:.5f}/{va_loss:.5f} | "
              f"MAE  T/V={tr_mae:.5f}/{va_mae:.5f}"
              f"Corr V={va_corr:.3f}")

        if va_loss < best_val - 1e-6:
            best_val = va_loss; noimp=0
            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}
        else:
            noimp += 1
            if noimp >= PATIENCE:
                print(f"Early stopping after {ep} epochs (no improvement {PATIENCE}).")
                break

    if best_state is not None:
        model.load_state_dict({k:v.to(DEVICE) for k,v in best_state.items()})

    # ---- Test & Metrics ----
    model.eval(); preds=[]; trues=[]; starts=[]
    with torch.no_grad():
        for Xb,yb,i0 in dl_te:
            Xb=Xb.to(DEVICE)
            preds.append(model(Xb).detach().cpu().numpy())
            trues.append(yb.numpy())
            starts.append(i0.numpy())
    yhat_sc = np.concatenate(preds,axis=0)
    ytrue_sc= np.concatenate(trues,axis=0)
    starts  = np.concatenate(starts,axis=0)

    # inverse scale (target only)
    yhat  = scaler_y.inverse_transform(yhat_sc.reshape(-1,1)).reshape(-1,PRED_LEN)
    ytrue = scaler_y.inverse_transform(ytrue_sc.reshape(-1,1)).reshape(-1,PRED_LEN)

    # ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ê³„ì‚°
    mae  = float(np.mean(np.abs(yhat-ytrue)))
    mse  = float(np.mean((yhat-ytrue)**2))
    rmse = float(np.sqrt(mse))
    
    print("\n" + "="*60)
    print("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    print(f"MAE  (Mean Absolute Error):      {mae:.6f}")
    print(f"MSE  (Mean Squared Error):       {mse:.6f}")
    print(f"RMSE (Root Mean Squared Error):  {rmse:.6f}")
    print("="*60)

    # =========================
    # Save per-window predictions
    # =========================
    cols_true = [f"true_t+{i}" for i in range(1,PRED_LEN+1)]
    cols_pred = [f"pred_t+{i}" for i in range(1,PRED_LEN+1)]
    out = pd.DataFrame(np.hstack([ytrue, yhat]), columns=cols_true+cols_pred)
    out.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")
    print(f"Saved predictions -> {OUT_CSV}")

    # =========================
    # Plot_1: last window (H-step ahead)
    # =========================
    last_true = ytrue[-1]; last_pred = yhat[-1]
    weeks = np.arange(1, PRED_LEN+1)
    plt.figure(figsize=(10,4))
    plt.plot(weeks, last_true, label="Truth (last window)", linewidth=2)
    plt.plot(weeks, last_pred, label="Prediction (last window)", linewidth=2)
    plt.title("Last Test Window: Truth vs Prediction")
    plt.xlabel("Horizon (weeks ahead)")
    plt.ylabel("ILI per 1,000 Population")
    plt.grid(True); plt.legend()
    plt.tight_layout()
    plt.savefig(PLOT_LAST_WINDOW, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight', format='png', pad_inches=0.1)
    print(f"Saved plot -> {PLOT_LAST_WINDOW}")

    # =========================
    # Plot_2: test reconstruction (val-context included)
    # =========================
    context = y_va_sc[-SEQ_LEN:]                       # í‘œì¤€í™” ì»¨í…ìŠ¤íŠ¸
    y_ct_sc = np.concatenate([context, y_te_sc])       # [SEQ_LEN + test_len]
    # ì…ë ¥ íŠ¹ì§•ë„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨í•´ ì¬êµ¬ì„± í•„ìš” â†’ Xë„ ë™ì¼í•˜ê²Œ ë¶™ì—¬ì„œ ì˜ˆì¸¡
    X_ct_sc = np.concatenate([X_va_sc[-SEQ_LEN:], X_te_sc], axis=0)
    ds_ct = PatchTSTDataset(X_ct_sc, y_ct_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    dl_ct = DataLoader(ds_ct, batch_size=BATCH_SIZE, shuffle=False)

    model.eval(); preds_ct=[]; starts_ct=[]
    with torch.no_grad():
        for Xb, _, i0 in dl_ct:
            Xb = Xb.to(DEVICE)
            preds_ct.append(model(Xb).detach().cpu().numpy())  # (B, H)
            starts_ct.append(i0.numpy())
    yhat_ct_sc = np.concatenate(preds_ct, axis=0)
    starts_ct  = np.concatenate(starts_ct, axis=0)
    yhat_ct = scaler_y.inverse_transform(yhat_ct_sc.reshape(-1,1)).reshape(-1, PRED_LEN)

    test_len = len(y_te)
    recon_sum   = np.zeros(test_len)
    recon_count = np.zeros(test_len)
    h_weights = np.linspace(RECON_W_START, RECON_W_END, PRED_LEN)

    for k, s in enumerate(starts_ct):
        pos0_ct = int(s) + SEQ_LEN   # [context+test] ì¶•
        pos0_te = pos0_ct - SEQ_LEN  # test ì¶•ìœ¼ë¡œ ë³€í™˜
        for j in range(PRED_LEN):
            idx = pos0_te + j
            if 0 <= idx < test_len:
                w = h_weights[j]
                recon_sum[idx]   += yhat_ct[k, j] * w
                recon_count[idx] += w

    recon = np.where(recon_count > 0, recon_sum / np.maximum(1, recon_count), np.nan)

    truth_test = y_te
    x_labels = lab_te
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš© (NaN ì œê±°)
    valid_mask = ~np.isnan(truth_test) & ~np.isnan(recon)
    valid_indices = np.where(valid_mask)[0]
    
    if len(valid_indices) < len(truth_test):
        print(f"\nâš ï¸ Warning: {len(truth_test) - len(valid_indices)} NaN values removed from test reconstruction")
        truth_test = truth_test[valid_mask]
        recon = recon[valid_mask]
        x_labels = [x_labels[i] for i in valid_indices]
    
    test_len = len(truth_test)
    
    # ë””ë²„ê·¸: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²”ìœ„ ì¶œë ¥
    print(f"\n=== Test Segment Info ===")
    print(f"Test length: {test_len}")
    print(f"First label: {x_labels[0]}")
    print(f"Last label: {x_labels[-1]}")
    print(f"Truth range: [{truth_test.min():.2f}, {truth_test.max():.2f}]")
    print(f"Truth mean: {truth_test.mean():.2f}")
    print(f"Prediction range: [{np.nanmin(recon):.2f}, {np.nanmax(recon):.2f}]")
    print(f"Prediction mean: {np.nanmean(recon):.2f}")
    
    # Xì¶• ë¼ë²¨ì„ ë” ìì£¼ í‘œì‹œ (ê°„ê²© ì¡°ì •)
    tick_step = max(1, test_len // 20)  # ì•½ 20ê°œ ë¼ë²¨ í‘œì‹œ
    tick_idx  = list(range(0, test_len, tick_step))
    if tick_idx[-1] != test_len-1:
        tick_idx.append(test_len-1)
    tick_text = [x_labels[i] for i in tick_idx]

    plt.figure(figsize=(18,6))  # ê·¸ë˜í”„ í¬ê¸° í™•ëŒ€
    plt.plot(range(test_len), truth_test, linewidth=2.5, marker='o', markersize=3, 
             label=f"Truth (test segment, n={test_len})", color='darkblue')
    plt.plot(range(test_len), recon, linewidth=2.5, marker='s', markersize=3,
             label="Prediction (overlap-avg, weighted)", color='darkorange')
    plt.title(f"Test Range: Truth vs Prediction | {x_labels[0]} ~ {x_labels[-1]}", 
              fontsize=14, fontweight='bold')
    plt.xlabel("Season - Week", fontsize=12)
    plt.ylabel("ILI per 1,000 Population", fontsize=12)
    plt.xticks(tick_idx, tick_text, rotation=45, ha="right", fontsize=9)
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.legend(fontsize=11, loc='upper left')
    
    # Yì¶• ë²”ìœ„ ëª…ì‹œì  ì„¤ì • (ì´ìƒê°’ ë°©ì§€)
    y_min = min(truth_test.min(), np.nanmin(recon))
    y_max = max(truth_test.max(), np.nanmax(recon))
    plt.ylim(y_min * 0.95, y_max * 1.05)
    
    plt.tight_layout()
    plt.savefig(PLOT_TEST_RECON, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight', format='png', pad_inches=0.1)
    print(f"Saved plot -> {PLOT_TEST_RECON}")

    # =========================
    # Plot_3: Train/Val MAE curves
    # =========================
    xs = np.arange(1, len(hist["train_mae"])+1)
    plt.figure(figsize=(10,4))
    plt.plot(xs, hist["train_mae"], linewidth=2, label="Train MAE (original units)")
    plt.plot(xs, hist["val_mae"],   linewidth=2, label="Val MAE (original units)")
    plt.title("Training Curves: MAE per epoch (lower is better)")
    plt.xlabel("Epoch")
    plt.ylabel("MAE (ILI per 1,000)")
    plt.grid(True); plt.legend()
    plt.tight_layout()
    plt.savefig(PLOT_MA_CURVES, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight', format='png', pad_inches=0.1)
    print(f"Saved plot -> {PLOT_MA_CURVES}")


    # =========================
# Feature Importance utils
# =========================
def _eval_mse_on_split(model, X_split_sc, y_split_sc, scaler_y, feat_names,
                       batch_size=BATCH_SIZE):
    """
    Feature Importanceìš© MSE ê³„ì‚° (Perturbation-Based Method)
    """
    model.eval()

    # ì‹¤ì œ ëª¨ë¸ì˜ pred_lenì„ ì‚¬ìš© (Dual-head êµ¬ì¡°ì—ì„œ head_trend ì‚¬ìš©)
    pred_len = model.head_trend.out_features
    seq_len  = SEQ_LEN
    patch_len = PATCH_LEN
    stride = STRIDE

    ds = PatchTSTDataset(
        X_split_sc, y_split_sc,
        seq_len=seq_len,
        pred_len=pred_len,
        patch_len=patch_len,
        stride=stride
    )
    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)

    mse_sum, n = 0.0, 0
    with torch.no_grad():
        for Xb, yb, _ in dl:
            Xb = Xb.to(DEVICE)
            yb = yb.to(DEVICE)

            pred = model(Xb)  # (B, H_model)

            # pred / yb shape mismatch ë°©ì§€
            H = pred.shape[1]
            yb = yb[:, :H]

            # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            pred_np = pred.cpu().numpy()
            yb_np = yb.cpu().numpy()
            
            # scaler_yê°€ 2D ì…ë ¥ì„ ìš”êµ¬í•˜ë©´ reshape
            pred_orig = scaler_y.inverse_transform(pred_np.reshape(-1, 1)).flatten()[:H]
            yb_orig = scaler_y.inverse_transform(yb_np.reshape(-1, 1)).flatten()[:H]
            
            # MSE ê³„ì‚°
            mse = np.mean((pred_orig - yb_orig) ** 2)
            mse_sum += mse * yb.size(0)
            n += yb.size(0)

    return float(mse_sum / max(1, n))


def compute_feature_importance(model, 
                               X_va_sc, y_va_sc, 
                               X_te_sc=None, y_te_sc=None,
                               scaler_y=None, feat_names=None, 
                               random_state=42):
    """
    Perturbation-Based Feature Importance ê³„ì‚°
    
    ë°©ë²•:
    1. ê° ë³€ìˆ˜ë¥¼ ë§ˆìŠ¤í‚¹(í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´)
    2. MSE ì¦ê°€ëŸ‰ ì¸¡ì •: Importance(i) = MSE_masked(i) - MSE_original
    3. ì¤‘ìš”ë„ ì •ê·œí™”
    
    Note: 'ili'ëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ì´ë¯€ë¡œ Feature Importance ê³„ì‚°ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.
    """
    assert scaler_y is not None and feat_names is not None

    # --- 'ili' ì œì™¸: íƒ€ê²Ÿ ë³€ìˆ˜ëŠ” feature importance ê³„ì‚°ì—ì„œ ì œì™¸ ---
    feat_indices = [i for i, name in enumerate(feat_names) if name != 'ili']
    filtered_feat_names = [feat_names[i] for i in feat_indices]
    
    if len(filtered_feat_names) < len(feat_names):
        print(f"[FI] 'ili' íŠ¹ì§• ì œì™¸ë¨ (íƒ€ê²Ÿ ë³€ìˆ˜)")
        print(f"[FI] Feature Importance ê³„ì‚° ëŒ€ìƒ: {len(filtered_feat_names)}ê°œ íŠ¹ì§•")

    # --- Step 1: Baseline MSE (ì›ë³¸ ë°ì´í„°) ---
    print(f"[FI] Computing Baseline MSE...")
    mse_original_val = _eval_mse_on_split(model, X_va_sc, y_va_sc, scaler_y, feat_names)
    print(f"[FI] Baseline Val MSE: {mse_original_val:.6f}")

    mse_original_tst = None
    if X_te_sc is not None and y_te_sc is not None:
        mse_original_tst = _eval_mse_on_split(model, X_te_sc, y_te_sc, scaler_y, feat_names)
        print(f"[FI] Baseline Test MSE: {mse_original_tst:.6f}")

    # --- Step 2: ê° ë³€ìˆ˜ë¥¼ ë§ˆìŠ¤í‚¹í•˜ì—¬ MSE ì¦ê°€ëŸ‰ ì¸¡ì • ---
    print(f"[FI] Computing Perturbation Importance...")
    importance_val = []
    importance_tst = []

    for j in feat_indices:
        name = feat_names[j]
        
        # Validation set: í•´ë‹¹ í”¼ì²˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        X_masked_val = X_va_sc.copy()
        X_masked_val[:, j] = X_va_sc[:, j].mean()
        
        mse_masked_val = _eval_mse_on_split(model, X_masked_val, y_va_sc, scaler_y, feat_names)
        delta_mse_val = mse_masked_val - mse_original_val
        importance_val.append(delta_mse_val)
        
        print(f"  - {name}: Î”MSE={delta_mse_val:.6f}")

        # Test set (optional)
        if X_te_sc is not None and y_te_sc is not None:
            X_masked_tst = X_te_sc.copy()
            X_masked_tst[:, j] = X_te_sc[:, j].mean()
            
            mse_masked_tst = _eval_mse_on_split(model, X_masked_tst, y_te_sc, scaler_y, feat_names)
            delta_mse_tst = mse_masked_tst - mse_original_tst
            importance_tst.append(delta_mse_tst)

    # --- Step 3: Normalization ---
    importance_val = np.array(importance_val)
    sum_importance_val = np.abs(importance_val).sum()
    if sum_importance_val > 0:
        importance_norm_val = importance_val / sum_importance_val
    else:
        importance_norm_val = np.zeros_like(importance_val)

    importance_norm_tst = None
    if importance_tst:
        importance_tst = np.array(importance_tst)
        sum_importance_tst = np.abs(importance_tst).sum()
        if sum_importance_tst > 0:
            importance_norm_tst = importance_tst / sum_importance_tst
        else:
            importance_norm_tst = np.zeros_like(importance_tst)

    # --- DataFrame ìƒì„± ---
    column_mapping = {
        'ì—°ë„': 'year',
        'ì£¼ì°¨': 'week',
        'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
        'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate',
        'ì…ì›í™˜ì ìˆ˜': 'hospitalization',
        'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
        'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients',
        'ì•„í˜•': 'subtype'
    }
    inv_colmap = {v: k for k, v in column_mapping.items()}

    feature_disp = [f"{f} ({inv_colmap[f]})" if f in inv_colmap else f for f in filtered_feat_names]

    df_fi = pd.DataFrame({
        "feature": feature_disp,
        "importance_raw_val": importance_val,
        "importance_norm_val": importance_norm_val,
    })
    
    if importance_norm_tst is not None:
        df_fi["importance_raw_tst"] = importance_tst
        df_fi["importance_norm_tst"] = importance_norm_tst

    # Raw importance ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    df_fi = df_fi.sort_values("importance_raw_val", ascending=False).reset_index(drop=True)
    
    print(f"\n[FI] Feature Importance Calculation Complete!")
    return df_fi

def plot_feature_importance(fi_df, out_csv=None, out_png=None):
    """
    Perturbation-Based Feature Importanceë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    """
    if fi_df is None or len(fi_df) == 0:
        print("No feature importance data to plot.")
        return

    import matplotlib.pyplot as plt

    # CSV ì €ì¥
    if out_csv:
        fi_df.to_csv(out_csv, index=False)
        print(f"Feature Importance saved to {out_csv}")

    # ì‹œê°í™” (2ê°œ ì„œë¸Œí”Œë¡¯: Raw & Normalized)
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # â‘  Raw Importance (Î”MSE)
    axes[0].barh(fi_df["feature"], fi_df["importance_raw_val"], color="steelblue")
    axes[0].set_xlabel("Î”MSE (MSE_masked - MSE_original)")
    axes[0].set_title("Perturbation-Based Importance (Raw)")
    axes[0].invert_yaxis()
    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=0.8)

    # â‘¡ Normalized Importance
    axes[1].barh(fi_df["feature"], fi_df["importance_norm_val"], color="coral")
    axes[1].set_xlabel("Normalized Importance")
    axes[1].set_title("Perturbation-Based Importance (Normalized)")
    axes[1].invert_yaxis()

    plt.tight_layout()

    if out_png:
        plt.savefig(out_png, dpi=150, bbox_inches="tight")
        print(f"Feature Importance plot saved to {out_png}")
    plt.show()


# =========================
# Hyperparameter Management
# =========================
def get_default_hyperparameters() -> dict:
    """
    Config í´ë˜ìŠ¤ì˜ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
    
    Returns:
        ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° dict
    """
    return {
        'd_model': Config.D_MODEL,
        'n_heads': Config.N_HEADS,
        'enc_layers': Config.ENC_LAYERS,
        'ff_dim': Config.FF_DIM,
        'dropout': Config.DROPOUT,
        'lr': Config.LR,
        'weight_decay': Config.WEIGHT_DECAY,
        'batch_size': Config.BATCH_SIZE,
        'seq_len': Config.SEQ_LEN if not Config.USE_DAILY_DATA else Config.DAILY_SEQ_LEN,
        'patch_len': Config.PATCH_LEN
    }


def load_best_hyperparameters(json_path: str = "best_hyperparameters.json") -> Optional[dict]:
    """
    ì €ì¥ëœ best_hyperparameters.jsonì—ì„œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ
    
    Args:
        json_path: JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        í•˜ì´í¼íŒŒë¼ë¯¸í„° dict ë˜ëŠ” None (íŒŒì¼ ì—†ìŒ)
    """
    import json
    import os
    
    if not os.path.exists(json_path):
        return None
    
    try:
        with open(json_path, 'r') as f:
            params = json.load(f)
        
        print(f"\n{'='*70}")
        print(f"âœ… ì €ì¥ëœ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì„±ê³µ: {json_path}")
        print(f"{'='*70}")
        print(f"ğŸ“Š ë¡œë“œëœ íŒŒë¼ë¯¸í„°:")
        for key, value in params.items():
            print(f"   - {key}: {value}")
        print(f"{'='*70}\n")
        
        return params
    except Exception as e:
        print(f"âš ï¸  JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({json_path}): {e}")
        return None


def save_best_hyperparameters(params: dict, json_path: str = "best_hyperparameters.json") -> bool:
    """
    ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ì— ì €ì¥
    
    Args:
        params: í•˜ì´í¼íŒŒë¼ë¯¸í„° dict
        json_path: ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    import json
    
    try:
        with open(json_path, 'w') as f:
            json.dump(params, f, indent=2)
        print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì„±ê³µ: {json_path}")
        return True
    except Exception as e:
        print(f"âŒ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# =========================
# Optuna Optimization
# =========================
def optimize_hyperparameters(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list,
                            n_trials: int = 50):
    """
    Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    
    Args:
        X: ì…ë ¥ íŠ¹ì§• (N, F)
        y: íƒ€ê²Ÿ ë³€ìˆ˜ (N,)
        labels: ì‹œê°„ ë¼ë²¨
        feat_names: íŠ¹ì§• ì´ë¦„
        n_trials: ìµœì í™” ì‹œë„ íšŸìˆ˜
        
    Returns:
        best_params: ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° dict
    """
    if not OPTUNA_AVAILABLE:
        raise ImportError("Optuna is not installed. Install with: pip install optuna")
    
    print("\n" + "=" * 70)
    print("ğŸ” Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
    if Config.USE_DAILY_DATA:
        print(f"   ğŸ“… ì¼ë³„ ë°ì´í„° ëª¨ë“œ (SEQ_LEN={SEQ_LEN}, PRED_LEN={PRED_LEN})")
        print(f"   âš™ï¸  ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” ê³ ì •ê°’ ì‚¬ìš© (í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì œì™¸)")
    else:
        print(f"   ğŸ“† ì£¼ì°¨ë³„ ë°ì´í„° ëª¨ë“œ (SEQ_LEN={SEQ_LEN}, PRED_LEN={PRED_LEN})")
        print(f"   âš™ï¸  ì‹œí€€ìŠ¤ ê¸¸ì´ë„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ëŒ€ìƒ")
    print("=" * 70)
    
    def objective(trial: Trial) -> float:
        """Optuna objective function - validation MAEë¥¼ ìµœì†Œí™”"""
        
        # Trial ì‹œì‘ ì•Œë¦¼
        if Config.USE_DAILY_DATA and trial.number == 0:
            print(f"\n   ğŸ’¡ Trial {trial.number}: ì¼ë³„ ë°ì´í„° ëª¨ë“œë¡œ í•™ìŠµ ì‹œì‘ (seq_len=112 ê³ ì •)")
        
        # Configì—ì„œ íƒìƒ‰ ê³µê°„ ê°€ì ¸ì˜¤ê¸°
        search_space = Config.OPTUNA_SEARCH_SPACE
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§: search_spaceì— í‚¤ê°€ ì—†ìœ¼ë©´ Configì˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        params = {}
        params['d_model'] = trial.suggest_categorical('d_model', search_space['d_model'])
        params['n_heads'] = trial.suggest_categorical('n_heads', search_space['n_heads'])
        params['enc_layers'] = trial.suggest_int('enc_layers', *search_space['enc_layers'])
        params['ff_dim'] = trial.suggest_categorical('ff_dim', search_space['ff_dim'])
        params['dropout'] = trial.suggest_float('dropout', *search_space['dropout'])
        params['lr'] = trial.suggest_float('lr', *search_space['lr'], log=True)
        params['weight_decay'] = trial.suggest_float('weight_decay', *search_space['weight_decay'], log=True)
        params['batch_size'] = trial.suggest_categorical('batch_size', search_space['batch_size'])

        # seq_len / pred_len: ì¼ë³„ ë°ì´í„°ì¼ ë•ŒëŠ” ê³ ì •ê°’ ì‚¬ìš©
        if Config.USE_DAILY_DATA:
            # ì¼ë³„ ë°ì´í„°: SEQ_LEN, PRED_LEN ê³ ì • (í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì•ˆ í•¨)
            params['seq_len'] = SEQ_LEN   # 112ì¼
            params['pred_len'] = PRED_LEN # 28ì¼
            print(f"   [ì¼ë³„ ë°ì´í„° ëª¨ë“œ] seq_len={SEQ_LEN}, pred_len={PRED_LEN} ê³ ì •")
        else:
            # ì£¼ì°¨ë³„ ë°ì´í„°: í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
            if 'seq_len' in search_space:
                params['seq_len'] = trial.suggest_categorical('seq_len', search_space['seq_len'])
            else:
                params['seq_len'] = SEQ_LEN

            if 'pred_len' in search_space:
                params['pred_len'] = trial.suggest_categorical('pred_len', search_space['pred_len'])
            else:
                params['pred_len'] = PRED_LEN

        # patch_len: ì¼ë³„/ì£¼ì°¨ë³„ ëª¨ë‘ íƒìƒ‰
        if 'patch_len' in search_space:
            params['patch_len'] = trial.suggest_categorical('patch_len', search_space['patch_len'])
        else:
            params['patch_len'] = PATCH_LEN
        
        # d_modelì€ 4ì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨ (MultiScaleCNN ë¶„ê¸° 4ê°œ)
        if params['d_model'] % 4 != 0:
            params['d_model'] = (params['d_model'] // 4) * 4
        
        # n_headsëŠ” d_modelì˜ ì•½ìˆ˜ì—¬ì•¼ í•¨
        while params['d_model'] % params['n_heads'] != 0:
            params['n_heads'] //= 2
            if params['n_heads'] < 1:
                params['n_heads'] = 1
                break
        
        # ë°ì´í„° ë¶„í• 
        (s0, e0), (s1, e1), (s2, e2) = make_splits(len(y))
        X_tr, X_va = X[s0:e0], X[s1:e1]
        y_tr, y_va = y[s0:e0], y[s1:e1]
        
        # Scaling (íƒ€ê²Ÿ: Log ë³€í™˜ ì ìš©, í”¼ì²˜: Log ë³€í™˜ ë¯¸ì ìš©)
        scaler_y = get_scaler(for_target=True)
        y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
        y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
        
        scaler_x = get_scaler(for_target=False)
        X_tr_sc = scaler_x.fit_transform(X_tr)
        X_va_sc = scaler_x.transform(X_va)
        
        F = X.shape[1]
        
        # Dataset ìƒì„±
        try:
            ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, params['seq_len'], params['pred_len'], 
                                   params['patch_len'], STRIDE)
            ds_va = PatchTSTDataset(X_va_sc, y_va_sc, params['seq_len'], params['pred_len'],
                                   params['patch_len'], STRIDE)
        except:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°
            return float('inf')
        
        if len(ds_tr) < 1 or len(ds_va) < 1:
            return float('inf')
        
        dl_tr = DataLoader(ds_tr, batch_size=params['batch_size'], shuffle=True, drop_last=False)
        dl_va = DataLoader(ds_va, batch_size=params['batch_size'], shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        model = PatchTSTModel(
            in_features=F, patch_len=params['patch_len'], d_model=params['d_model'],
            n_heads=params['n_heads'], n_layers=params['enc_layers'], ff_dim=params['ff_dim'],
            dropout=params['dropout'], pred_len=params['pred_len'], head_hidden=HEAD_HIDDEN
        ).to(DEVICE)
        
        crit = nn.HuberLoss(delta=1.0)
        opt = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])
        
        # Early stoppingì„ ìœ„í•œ ë³€ìˆ˜
        best_val_loss = float('inf')
        patience_count = 0
        early_stop_patience = 20  # Optunaì—ì„œëŠ” ë” ì§§ê²Œ
        
        # í•™ìŠµ (Optunaì—ì„œëŠ” ì ì€ ì—í¬í¬)
        max_epochs = 50
        for ep in range(1, max_epochs + 1):
            # Train
            model.train()
            tr_loss_sum = 0
            n = 0
            for Xb, yb, _ in dl_tr:
                Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
                opt.zero_grad()
                pred = model(Xb)
                loss = crit(pred, yb)
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                opt.step()
                bs = yb.size(0)
                tr_loss_sum += loss.item() * bs
                n += bs
            
            tr_loss = tr_loss_sum / max(1, n)
            
            # Validation
            model.eval()
            va_loss_sum = 0
            va_mae_sum = 0
            n = 0
            # Peak MAE ê³„ì‚°ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
            all_preds = []
            all_targets = []
            
            with torch.no_grad():
                for Xb, yb, _ in dl_va:
                    Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
                    pred = model(Xb)
                    loss = crit(pred, yb)
                    bs = yb.size(0)
                    va_loss_sum += loss.item() * bs
                    va_mae_sum += batch_mae_in_original_units(pred, yb, scaler_y) * bs
                    n += bs
                    
                    # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (Peak MAE ê³„ì‚°ìš©)
                    pred_orig = scaler_y.inverse_transform(pred.cpu().numpy().reshape(-1, 1)).ravel()
                    target_orig = scaler_y.inverse_transform(yb.cpu().numpy().reshape(-1, 1)).ravel()
                    all_preds.extend(pred_orig)
                    all_targets.extend(target_orig)
            
            va_loss = va_loss_sum / max(1, n)
            va_mae = va_mae_sum / max(1, n)
            
            # ğŸ”´ Peak MAE ê³„ì‚° (train ë°ì´í„° ê¸°ì¤€ ìƒìœ„ 10% threshold)
            all_preds = np.array(all_preds)
            all_targets = np.array(all_targets)
            peak_threshold = np.quantile(y_tr, 0.9)  # train ë°ì´í„° ê¸°ì¤€ í”¼í¬
            peak_mask = all_targets >= peak_threshold
            
            if peak_mask.sum() > 0:  # í”¼í¬ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                peak_mae = np.mean(np.abs(all_preds[peak_mask] - all_targets[peak_mask]))
            else:
                peak_mae = 0.0  # í”¼í¬ ì—†ìœ¼ë©´ 0
            
            # ğŸ”´ ë³µí•© ëª©ì  í•¨ìˆ˜: ì „ì²´ MAE + í”¼í¬ MAE
            combined_metric = va_mae + 0.6 * peak_mae
            
            # Early stopping (ë³µí•© metric ê¸°ì¤€)
            if combined_metric < best_val_loss:
                best_val_loss = combined_metric
                patience_count = 0
            else:
                patience_count += 1
                if patience_count >= early_stop_patience:
                    break
            
            # Optuna pruning (ì¤‘ê°„ ê²°ê³¼ê°€ ë‚˜ì˜ë©´ ì¡°ê¸° ì¢…ë£Œ)
            trial.report(combined_metric, ep)
            if trial.should_prune():
                raise optuna.TrialPruned()
        
        # ğŸ”´ ë³µí•© Metric ë°˜í™˜ (Val MAE + 0.6 * Peak MAE)
        return combined_metric
    
    # Optuna study ìƒì„± ë° ì‹¤í–‰
    study = optuna.create_study(
        direction='minimize',
        pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=10)
    )
    
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("âœ… Optuna ìµœì í™” ì™„ë£Œ")
    print("=" *  70)
    print(f"\nğŸ† Best Trial:")
    print(f"  - Value (Val MAE + 0.6*Peak MAE): {study.best_trial.value:.4f}")
    print(f"\nğŸ“Š Best Hyperparameters:")
    for key, value in study.best_params.items():
        print(f"  - {key}: {value}")
    
    # Best parameters ì €ì¥
    best_params_file = BASE_DIR / "best_hyperparameters.json"
    import json
    with open(best_params_file, 'w') as f:
        json.dump(study.best_params, f, indent=2)
    print(f"\nğŸ’¾ Best parameters saved to: {best_params_file}")
    
    return study.best_params

# =========================
# train_and_eval (main)
# =========================
def train_and_eval(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list,
                   compute_fi=False, save_fi=False, optuna_params=None):
    """
    í†µí•© í•™ìŠµ + í‰ê°€ í•¨ìˆ˜.
    compute_fi=True -> feature importance ê³„ì‚°
    save_fi=True -> CSV/plot ì €ì¥
    optuna_params=dict -> Optuna ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
    """
    # Optuna íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì ìš©
    if optuna_params:
        global D_MODEL, N_HEADS, ENC_LAYERS, FF_DIM, DROPOUT, LR, WEIGHT_DECAY, BATCH_SIZE, SEQ_LEN, PRED_LEN, PATCH_LEN
        D_MODEL = optuna_params.get('d_model', D_MODEL)
        N_HEADS = optuna_params.get('n_heads', N_HEADS)
        ENC_LAYERS = optuna_params.get('enc_layers', ENC_LAYERS)
        FF_DIM = optuna_params.get('ff_dim', FF_DIM)
        DROPOUT = optuna_params.get('dropout', DROPOUT)
        LR = optuna_params.get('lr', LR)
        WEIGHT_DECAY = optuna_params.get('weight_decay', WEIGHT_DECAY)
        BATCH_SIZE = optuna_params.get('batch_size', BATCH_SIZE)
        SEQ_LEN = optuna_params.get('seq_len', SEQ_LEN)
        PRED_LEN = optuna_params.get('pred_len', PRED_LEN)
        PATCH_LEN = optuna_params.get('patch_len', PATCH_LEN)
        
        print("\n" + "=" * 70)
        print("ğŸ¯ Optuna ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ")
        print("=" * 70)
        for key, value in optuna_params.items():
            print(f"  - {key}: {value}")
        print("=" * 70 + "\n")
    
    """
    í†µí•© í•™ìŠµ + í‰ê°€ í•¨ìˆ˜.
    compute_fi=True -> feature importance ê³„ì‚°
    save_fi=True -> CSV/plot ì €ì¥
    """
    torch.manual_seed(SEED); np.random.seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
    print(f"[Config] EPOCHS:{EPOCHS}, BATCH_SIZE:{BATCH_SIZE}, SEQ_LEN:{SEQ_LEN}, PRED_LEN:{PRED_LEN}")
    print(f"[Config] PATCH_LEN:{PATCH_LEN}, STRIDE:{STRIDE}, LR:{LR}, Warmup:{WARMUP_EPOCHS}, Patience:{PATIENCE}")
    print(f"[Config] Log Transform: {Config.USE_LOG_TRANSFORM} (eps={Config.LOG_EPSILON}), Peak Weight: Î±={Config.PEAK_WEIGHT_ALPHA}, Quantile={Config.PEAK_THRESHOLD_QUANTILE}")

    N = len(y)
    split_tr = int(0.7*N); split_va = int(0.85*N)
    X_tr, y_tr = X[:split_tr], y[:split_tr]
    X_va, y_va = X[split_tr:split_va], y[split_tr:split_va]
    X_te, y_te = X[split_va:], y[split_va:]

    # ì „ì—­ get_scaler í•¨ìˆ˜ ì‚¬ìš© (Log ë³€í™˜ì€ íƒ€ê²Ÿë§Œ)
    scaler_y = get_scaler(for_target=True)  # íƒ€ê²Ÿ: Log ë³€í™˜ ì ìš©
    y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
    y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
    y_te_sc = scaler_y.transform(y_te.reshape(-1,1)).ravel()

    scaler_x = get_scaler(for_target=False)  # í”¼ì²˜: Log ë³€í™˜ ë¯¸ì ìš©
    X_tr_sc = scaler_x.fit_transform(X_tr)
    X_va_sc = scaler_x.transform(X_va)
    X_te_sc = scaler_x.transform(X_te)

    F = X.shape[1]
    print(f"[Shapes] X_tr:{X_tr.shape}, X_va:{X_va.shape}, X_te:{X_te.shape} | F={F}")
    print(f"[Info] Model input feature order -> {feat_names}")

    ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_va = PatchTSTDataset(X_va_sc, y_va_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_te = PatchTSTDataset(X_te_sc, y_te_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)

    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)

    model = PatchTSTModel(
        in_features=F, patch_len=PATCH_LEN, d_model=D_MODEL, n_heads=N_HEADS,
        n_layers=ENC_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT,
        pred_len=PRED_LEN, head_hidden=HEAD_HIDDEN
    ).to(DEVICE)

    def peak_weighted_loss(pred, target, peak_quantile=0.9, alpha=3.0):
        """
        Peak-aware weighted MAE loss.
        pred, target: (B, H)
        """
        with torch.no_grad():
            thresh = torch.quantile(target, peak_quantile)
            weights = torch.ones_like(target)
            weights[target >= thresh] = alpha
        return torch.mean(weights * torch.abs(pred - target))

    crit = peak_weighted_loss
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)

    hist = {"train_loss":[], "val_loss":[], "train_mae":[], "val_mae":[]}

    best_val = float("inf"); best_state=None; noimp=0
    printed_batch_info = False
    for ep in range(1, EPOCHS+1):
        model.train(); tr_loss_sum=0; tr_mae_sum=0; n=0
        for g in opt.param_groups:
            g['lr'] = warmup_lr(ep, LR, WARMUP_EPOCHS)

        for Xb, yb, _ in dl_tr:
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            if not printed_batch_info:
                print(f"[Batch shapes] Xb:{Xb.shape}, yb:{yb.shape}")
                printed_batch_info=True
            opt.zero_grad()
            pred=model(Xb)
            loss=crit(pred,yb)
            loss.backward(); opt.step()

            tr_loss_sum += loss.item()*yb.size(0)
            tr_mae_sum += batch_mae_in_original_units(pred, yb, scaler_y)*yb.size(0)
            n+=yb.size(0)

        tr_loss_avg = tr_loss_sum/max(1,n)
        tr_mae_avg  = tr_mae_sum/max(1,n)

        model.eval(); va_loss_sum=0; va_mae_sum=0; m=0
        with torch.no_grad():
            for Xb,yb,_ in dl_va:
                Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
                pred=model(Xb)
                loss=crit(pred,yb)
                va_loss_sum += loss.item()*yb.size(0)
                va_mae_sum  += batch_mae_in_original_units(pred,yb,scaler_y)*yb.size(0)
                m+=yb.size(0)
        va_loss_avg=va_loss_sum/max(1,m)
        va_mae_avg =va_mae_sum/max(1,m)

        hist["train_loss"].append(tr_loss_avg)
        hist["val_loss"].append(va_loss_avg)
        hist["train_mae"].append(tr_mae_avg)
        hist["val_mae"].append(va_mae_avg)

        if ep<=5 or ep%5==0:
            print(f"Epoch {ep:3d}/{EPOCHS} | TrL:{tr_loss_avg:.6f} TrMAE:{tr_mae_avg:.6f} | VaL:{va_loss_avg:.6f} VaMAE:{va_mae_avg:.6f}")

        if va_mae_avg < best_val:
            best_val = va_mae_avg
            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}
            noimp=0
        else:
            noimp+=1
            if noimp>=PATIENCE:
                print(f"Early stop at epoch {ep} (no improvement for {PATIENCE} epochs)")
                break

        scheduler.step()

    if best_state is not None:
        model.load_state_dict(best_state)
    print(f"Best Val MAE: {best_val:.6f}")

    # Test - ëª¨ë“  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° + Horizonë³„ ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
    model.eval()
    te_mae_sum=0; te_mse_sum=0; te_rmse_sum=0; k=0
    all_preds = []  # ëª¨ë“  ì˜ˆì¸¡ê°’ ìˆ˜ì§‘
    all_trues = []  # ëª¨ë“  ì‹¤ì œê°’ ìˆ˜ì§‘
    
    with torch.no_grad():
        for Xb,yb,_ in dl_te:
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            pred=model(Xb)
            te_mae_sum += batch_mae_in_original_units(pred,yb,scaler_y)*yb.size(0)
            te_mse_sum += batch_mse_in_original_units(pred,yb,scaler_y)*yb.size(0)
            te_rmse_sum += batch_rmse_in_original_units(pred,yb,scaler_y)*yb.size(0)
            k+=yb.size(0)
            
            # ì˜ˆì¸¡ê°’/ì‹¤ì œê°’ ìˆ˜ì§‘ (ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜)
            pred_np = pred.cpu().numpy()
            yb_np = yb.cpu().numpy()
            pred_orig = scaler_y.inverse_transform(pred_np.reshape(-1,1)).reshape(-1, PRED_LEN)
            yb_orig = scaler_y.inverse_transform(yb_np.reshape(-1,1)).reshape(-1, PRED_LEN)
            all_preds.append(pred_orig)
            all_trues.append(yb_orig)
    
    te_mae_avg = te_mae_sum/max(1,k)
    te_mse_avg = te_mse_sum/max(1,k)
    te_rmse_avg = te_rmse_sum/max(1,k)
    
    # ëª¨ë“  ì˜ˆì¸¡ê°’/ì‹¤ì œê°’ ë³‘í•©
    all_preds = np.concatenate(all_preds, axis=0)  # (N, PRED_LEN)
    all_trues = np.concatenate(all_trues, axis=0)  # (N, PRED_LEN)
    
    print("\n" + "="*60)
    print("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€")
    print("="*60)
    print(f"MAE  (Mean Absolute Error):      {te_mae_avg:.6f}")
    print(f"MSE  (Mean Squared Error):       {te_mse_avg:.6f}")
    print(f"RMSE (Root Mean Squared Error):  {te_rmse_avg:.6f}")
    print("="*60)
    
    # ===== Horizonë³„ ì˜ˆì¸¡ê°’ (ìµœì‹  ì˜ˆì¸¡ ì‹œì  ê¸°ì¤€) =====
    print("\n" + "="*60)
    print("ğŸ“… ìµœì‹  ì˜ˆì¸¡ ì‹œì  ê¸°ì¤€ Horizonë³„ ì˜ˆì¸¡ê°’")
    print("="*60)
    
    # ê°€ì¥ ìµœê·¼ ì˜ˆì¸¡ ì‹œì  (ë§ˆì§€ë§‰ ìƒ˜í”Œ)
    last_idx = len(all_preds) - 1
    last_pred = all_preds[last_idx]  # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ì‹œì ì˜ ì˜ˆì¸¡ê°’ë“¤ (PRED_LENê°œ)
    last_true = all_trues[last_idx]  # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ì‹œì ì˜ ì‹¤ì œê°’ë“¤ (PRED_LENê°œ)
    
    print(f"\nğŸ“ ì˜ˆì¸¡ ì‹œì‘ ì‹œì : í…ŒìŠ¤íŠ¸ ë°ì´í„° ë§ˆì§€ë§‰ ìƒ˜í”Œ (index {last_idx})")
    print(f"   (ì´ ì‹œì ì—ì„œ í–¥í›„ {PRED_LEN}ì£¼ë¥¼ ì˜ˆì¸¡)")
    print()
    
    horizons_to_check = [1, 2, 3, 4]  # 1ì£¼, 2ì£¼, 3ì£¼, 4ì£¼ í›„
    
    print(f"{'Horizon':<12} {'ì˜ˆì¸¡ê°’':>12} {'ì‹¤ì œê°’':>12} {'ì˜¤ì°¨':>12} {'ì˜¤ì°¨ìœ¨':>10}")
    print("-" * 60)
    
    for h in horizons_to_check:
        if h <= PRED_LEN:
            h_idx = h - 1  # 0-indexed
            pred_val = last_pred[h_idx]
            true_val = last_true[h_idx]
            error = pred_val - true_val
            error_pct = (error / true_val * 100) if true_val != 0 else 0
            
            print(f"{h}ì£¼ í›„ ({h*7}ì¼)  {pred_val:>12.2f} {true_val:>12.2f} {error:>+12.2f} {error_pct:>+9.1f}%")
    
    print("-" * 60)
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„ì— ëŒ€í•œ Horizonë³„ ì„±ëŠ¥ í†µê³„ (ì°¸ê³ ìš©)
    print(f"\nğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„ Horizonë³„ ì„±ëŠ¥ (ì°¸ê³ ):")
    for h in horizons_to_check:
        if h <= PRED_LEN:
            h_idx = h - 1
            h_preds = all_preds[:, h_idx]
            h_trues = all_trues[:, h_idx]
            h_mae = np.mean(np.abs(h_preds - h_trues))
            print(f"   {h}ì£¼ í›„: MAE={h_mae:.2f}")
    
    print("\n" + "="*60)
    
    # ===== Horizonë³„ ê²°ê³¼ CSV ì €ì¥ =====
    horizon_results = []
    for i in range(len(all_preds)):
        row = {'sample_idx': i}
        for h in range(1, PRED_LEN + 1):
            row[f'pred_{h}w'] = all_preds[i, h-1]
            row[f'true_{h}w'] = all_trues[i, h-1]
            row[f'error_{h}w'] = all_preds[i, h-1] - all_trues[i, h-1]
        horizon_results.append(row)
    
    df_horizon = pd.DataFrame(horizon_results)
    horizon_csv_path = str(BASE_DIR / "horizon_predictions.csv")
    df_horizon.to_csv(horizon_csv_path, index=False)
    print(f"ğŸ“Š Horizonë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {horizon_csv_path}")

    # Plot curves
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(hist["train_mae"],label="Train MAE")
    plt.plot(hist["val_mae"],label="Val MAE")
    plt.xlabel("Epoch"); plt.ylabel("MAE (original units)"); plt.legend(); plt.title("MAE curves")
    plt.subplot(1,2,2)
    plt.plot(hist["train_loss"],label="Train Loss")
    plt.plot(hist["val_loss"],label="Val Loss")
    plt.xlabel("Epoch"); plt.ylabel("Huber Loss"); plt.legend(); plt.title("Loss curves")
    plt.tight_layout()
    plt.savefig(PLOT_MA_CURVES, dpi=150)
    print(f"MAE/loss curves saved to {PLOT_MA_CURVES}")
    plt.close()  # ì°½ì„ ë‹«ì•„ ë©”ëª¨ë¦¬ ì ˆì•½

    # Last window
    last_seq_idx = len(y_te_sc) - SEQ_LEN - PRED_LEN
    if last_seq_idx >= 0:
        seq_X = X_te_sc[last_seq_idx:last_seq_idx+SEQ_LEN]  # (SEQ_LEN, F)
        
        # Patchify: Datasetì˜ __getitem__ê³¼ ë™ì¼í•œ ë¡œì§
        patches = []
        pos = 0
        while pos + PATCH_LEN <= SEQ_LEN:
            patches.append(seq_X[pos:pos+PATCH_LEN, :])  # (PATCH_LEN, F)
            pos += STRIDE
        X_patch = np.stack(patches, axis=0)  # (P, PATCH_LEN, F)
        
        # Tensorë¡œ ë³€í™˜í•˜ê³  batch ì°¨ì› ì¶”ê°€
        seq_t = torch.from_numpy(X_patch).unsqueeze(0).float().to(DEVICE)  # (1, P, PATCH_LEN, F)
        
        model.eval()
        with torch.no_grad():
            p = model(seq_t).cpu().numpy().ravel()
        p_orig = scaler_y.inverse_transform(p.reshape(-1,1)).ravel()
        y_true_last = scaler_y.inverse_transform(y_te_sc[last_seq_idx+SEQ_LEN:last_seq_idx+SEQ_LEN+PRED_LEN].reshape(-1,1)).ravel()
        plt.figure(figsize=(8,4))
        plt.plot(range(len(y_true_last)), y_true_last, marker='o', label="True")
        plt.plot(range(len(p_orig)), p_orig, marker='x', label="Pred")
        plt.xlabel("Future step (horizon)"); plt.ylabel("ILI")
        plt.title(f"Last window prediction (SEQ_LEN={SEQ_LEN}, PRED_LEN={PRED_LEN})")
        plt.legend(); plt.grid(True, alpha=0.3)
        plt.savefig(PLOT_LAST_WINDOW, dpi=150)
        print(f"Last window plot saved to {PLOT_LAST_WINDOW}")
        plt.close()  # ì°½ì„ ë‹«ì•„ ë©”ëª¨ë¦¬ ì ˆì•½

    # =========================
    # Plot: Test Range (Overlap-Avg, Weighted)
    # =========================
    context = y_va_sc[-SEQ_LEN:]                       # validation context
    y_ct_sc = np.concatenate([context, y_te_sc])       # context + test
    X_ct_sc = np.concatenate([X_va_sc[-SEQ_LEN:], X_te_sc], axis=0)

    ds_ct = PatchTSTDataset(X_ct_sc, y_ct_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    dl_ct = DataLoader(ds_ct, batch_size=BATCH_SIZE, shuffle=False)

    model.eval()
    preds_ct, starts_ct = [], []
    with torch.no_grad():
        for Xb, _, i0 in dl_ct:
            Xb = Xb.to(DEVICE)
            preds_ct.append(model(Xb).cpu().numpy())
            starts_ct.append(i0.numpy())

    preds_ct = np.concatenate(preds_ct, axis=0)    # (N,H)
    starts_ct = np.concatenate(starts_ct, axis=0)

    preds_ct_orig = scaler_y.inverse_transform(
        preds_ct.reshape(-1,1)
    ).reshape(-1, PRED_LEN)

    test_len = len(y_te)
    recon_sum = np.zeros(test_len)
    recon_cnt = np.zeros(test_len)

    # horizon weights (early step emphasized)
    h_weights = np.linspace(RECON_W_START, RECON_W_END, PRED_LEN)

    for k, s in enumerate(starts_ct):
        base = int(s) + SEQ_LEN - SEQ_LEN
        for h in range(PRED_LEN):
            idx = base + h
            if 0 <= idx < test_len:
                recon_sum[idx] += preds_ct_orig[k, h] * h_weights[h]
                recon_cnt[idx] += h_weights[h]

    recon = np.where(recon_cnt > 0, recon_sum / recon_cnt, np.nan)

    truth = y_te
    labels_te = labels[len(y) - len(y_te):]

    valid = ~np.isnan(recon)
    recon = recon[valid]
    truth = truth[valid]
    labels_te = [labels_te[i] for i in np.where(valid)[0]]

    plt.figure(figsize=(18,6))
    plt.plot(truth, linewidth=2.5, marker='o', markersize=3,
             label=f"Truth (test segment, n={len(truth)})", color="navy")
    plt.plot(recon, linewidth=2.5, marker='s', markersize=3,
             label="Prediction (overlap-avg, weighted)", color="darkorange")

    plt.title(
        f"Test Range: Truth vs Prediction | {labels_te[0]} ~ {labels_te[-1]}",
        fontsize=14, fontweight="bold"
    )
    plt.xlabel("Season - Week", fontsize=12)
    plt.ylabel("ILI per 1,000 Population", fontsize=12)

    tick_step = max(1, len(labels_te) // 20)
    tick_idx = list(range(0, len(labels_te), tick_step))
    if tick_idx[-1] != len(labels_te)-1:
        tick_idx.append(len(labels_te)-1)

    plt.xticks(tick_idx, [labels_te[i] for i in tick_idx],
               rotation=45, ha="right", fontsize=9)

    plt.grid(True, alpha=0.3, linestyle="--")
    plt.legend(loc="upper left", fontsize=11)
    plt.tight_layout()

    plt.savefig(PLOT_TEST_RECON, dpi=300, bbox_inches="tight")
    print(f"Saved plot -> {PLOT_TEST_RECON}")
    plt.close()

    # Feature importance
    fi_df = None
    if compute_fi:
        print("\n[Computing Feature Importance...]")
        fi_df = compute_feature_importance(
            model, X_va_sc, y_va_sc, X_te_sc, y_te_sc,
            scaler_y, feat_names, random_state=SEED
        )
        print("\n[Feature Importance (sorted by mean_delta_val)]")
        print(fi_df.to_string(index=False))

        if save_fi:
            plot_feature_importance(
                fi_df,
                out_csv=str(BASE_DIR / "feature_importance.csv"),
                out_png=str(BASE_DIR / "feature_importance.png")
            )

    # ë°˜í™˜: ì™¸ë¶€ ì…€ì—ì„œ ì¬í™œìš© ê°€ëŠ¥í•˜ë„ë¡
    return model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df

# =========================
# ì‹¤í–‰ë¶€ (ê²°ê³¼ ì¶œë ¥)
# =========================
if __name__ == "__main__":
    import argparse
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ë¡œë“œ
    env_age_group = os.getenv('AGE_GROUP', '').strip() or None
    env_subtype = os.getenv('SUBTYPE', '').strip() or None
    env_subtype_only = os.getenv('SUBTYPE_ONLY', 'false').lower() == 'true'
    env_raw_data = os.getenv('USE_RAW_DATA', 'false').lower() == 'true'
    env_data_dir = os.getenv('DATA_DIR', 'data/before')
    
    parser = argparse.ArgumentParser(
        description='PatchTST ì¸í”Œë£¨ì—”ì ì˜ˆì¸¡ ëª¨ë¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼):
  AGE_GROUP      ì—°ë ¹ëŒ€ (ì˜ˆ: 19-49ì„¸, 65ì„¸ì´ìƒ)
  SUBTYPE        ì•„í˜• (A ë˜ëŠ” B)
  SUBTYPE_ONLY   ì•„í˜•ë³„ ì˜ˆì¸¡ ëª¨ë“œ (true/false)
  USE_RAW_DATA   ì›ë³¸ CSV ì‚¬ìš© (true/false)
  DATA_DIR       ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬

ëª…ë ¹ì¤„ ì¸ìê°€ í™˜ê²½ë³€ìˆ˜ë³´ë‹¤ ìš°ì„ í•©ë‹ˆë‹¤.
""")
    parser.add_argument('--age-group', type=str, default=env_age_group,
                        help=f'ì—°ë ¹ëŒ€ ì„ íƒ (ì˜ˆ: 19-49ì„¸, 65ì„¸ì´ìƒ, 0-6ì„¸). í™˜ê²½ë³€ìˆ˜: AGE_GROUP={env_age_group}')
    parser.add_argument('--subtype', type=str, default=env_subtype,
                        help=f'ì•„í˜• ì„ íƒ (A, B). í™˜ê²½ë³€ìˆ˜: SUBTYPE={env_subtype}')
    parser.add_argument('--subtype-only', action='store_true', default=env_subtype_only,
                        help=f'ì•„í˜•ë³„ ê²€ì¶œë¥ ë§Œ ì˜ˆì¸¡ (ds_0107 ë°ì´í„° ì‚¬ìš©). í™˜ê²½ë³€ìˆ˜: SUBTYPE_ONLY={env_subtype_only}')
    parser.add_argument('--raw-data', action='store_true', default=env_raw_data,
                        help=f'ì›ë³¸ CSV ë°ì´í„°ì—ì„œ ì§ì ‘ ë¡œë“œ. í™˜ê²½ë³€ìˆ˜: USE_RAW_DATA={env_raw_data}')
    parser.add_argument('--data-dir', type=str, default=env_data_dir,
                        help=f'ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬. í™˜ê²½ë³€ìˆ˜: DATA_DIR={env_data_dir}')
    parser.add_argument('--list-options', action='store_true',
                        help='ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ë ¹ëŒ€ì™€ ì•„í˜• ëª©ë¡ ì¶œë ¥')
    parser.add_argument('--validate-data', action='store_true',
                        help='merged CSVì™€ ì›ë³¸ ë°ì´í„° í•„í„°ë§ ê²°ê³¼ ë¹„êµ ê²€ì¦ (íŠ¹ì • ì—°ë ¹ëŒ€ ë˜ëŠ” ì „ì²´)')
    parser.add_argument('--validate-all', action='store_true',
                        help='ëª¨ë“  ì£¼ìš” ì—°ë ¹ëŒ€ì— ëŒ€í•´ ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦ (--validate-dataì™€ í•¨ê»˜ ì‚¬ìš©)')
    args = parser.parse_args()
    
    # í˜„ì¬ ì„¤ì • ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ í˜„ì¬ ëª¨ë¸ ì„¤ì •")
    print("=" * 60)
    print(f"   ì—°ë ¹ëŒ€ (AGE_GROUP): {args.age_group or 'ì „ì²´ (ë¯¸ì§€ì •)'} {'[env]' if args.age_group == env_age_group and env_age_group else ''}")
    print(f"   ì•„í˜• (SUBTYPE): {args.subtype or 'ìš°ì„¸ ì•„í˜• ìë™ ì„ íƒ'} {'[env]' if args.subtype == env_subtype and env_subtype else ''}")
    print(f"   ì•„í˜• ì „ìš© ëª¨ë“œ (SUBTYPE_ONLY): {args.subtype_only} {'[env]' if args.subtype_only == env_subtype_only else ''}")
    print(f"   ì›ë³¸ ë°ì´í„° ì‚¬ìš© (USE_RAW_DATA): {args.raw_data} {'[env]' if args.raw_data == env_raw_data else ''}")
    print(f"   ë°ì´í„° ë””ë ‰í† ë¦¬ (DATA_DIR): {args.data_dir} {'[env]' if args.data_dir == env_data_dir else ''}")
    print("=" * 60)
    
    # --validate-data ì˜µì…˜: ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦ í›„ ì¢…ë£Œ
    if args.validate_data:
        print("\nğŸ” ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ê²€ì¦ ëª¨ë“œ")
        if args.validate_all:
            # ëª¨ë“  ì—°ë ¹ëŒ€ ê²€ì¦
            validate_all_age_groups(
                data_dir=args.data_dir,
                merged_csv_path="merged_influenza_data.csv"
            )
        elif args.age_group:
            # íŠ¹ì • ì—°ë ¹ëŒ€ë§Œ ê²€ì¦
            validate_data_sources(
                age_group=args.age_group,
                data_dir=args.data_dir,
                merged_csv_path="merged_influenza_data.csv",
                verbose=True
            )
        else:
            # í™˜ê²½ë³€ìˆ˜ì— ì—°ë ¹ëŒ€ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì—°ë ¹ëŒ€ ê²€ì¦
            validate_all_age_groups(
                data_dir=args.data_dir,
                merged_csv_path="merged_influenza_data.csv"
            )
        exit(0)
    
    # --list-options ì˜µì…˜: ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜ ì¶œë ¥ í›„ ì¢…ë£Œ
    if args.list_options:
        print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜:")
        
        # ì›ë³¸ ë°ì´í„°ì—ì„œ ì—°ë ¹ëŒ€ ëª©ë¡ ì¡°íšŒ
        age_info = get_available_age_groups(args.data_dir)
        
        print(f"\nğŸ“‚ ì›ë³¸ ë°ì´í„° ì—°ë ¹ëŒ€ (--raw-data ëª¨ë“œ):")
        for dsid, ages in age_info.items():
            print(f"   {dsid}: {ages}")
        
        # ê³µí†µ ì—°ë ¹ëŒ€ ì°¾ê¸°
        if age_info:
            common_ages = set(age_info.get('ds_0101', []))
            for ages in age_info.values():
                common_ages &= set(ages)
            print(f"\nğŸ“Š ê³µí†µ ì—°ë ¹ëŒ€ (ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš© ê°€ëŠ¥):")
            for ag in sorted(common_ages):
                print(f"   - {ag}")
        
        print(f"\nì•„í˜• (--subtype-only --subtype <A|B>):")
        print(f"   - A: ì¸í”Œë£¨ì—”ì Aí˜•")
        print(f"   - B: ì¸í”Œë£¨ì—”ì Bí˜•")
        
        # ds_0107 ì•„í˜•ë³„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        df_subtype = load_subtype_data(subtype='all')
        if not df_subtype.empty:
            print(f"\nì•„í˜•ë³„ ê²€ì¶œë¥  ë°ì´í„° (ds_0107):")
            for st in df_subtype['subtype'].unique():
                count = len(df_subtype[df_subtype['subtype'] == st])
                print(f"   - {st}: {count}ê°œ ë ˆì½”ë“œ")
        
        exit(0)
    
    print("\n" + "ğŸš€ " * 20)
    print("ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("ğŸš€ " * 20 + "\n")
    
    # ì•„í˜•ë³„ ê²€ì¶œë¥ ë§Œ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë“œ (ds_0107)
    if args.subtype_only:
        if not args.subtype:
            print("âš ï¸ --subtype-only ì˜µì…˜ ì‚¬ìš© ì‹œ --subtype (A ë˜ëŠ” B)ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            exit(1)
        
        print("=" * 60)
        print(f"ğŸ§¬ ì•„í˜•ë³„ ê²€ì¶œë¥  ì˜ˆì¸¡ ëª¨ë“œ: {args.subtype}í˜•")
        print("=" * 60)
        
        # ì•„í˜•ë³„ ë°ì´í„° ì¤€ë¹„
        X, y, labels, feat_names = prepare_subtype_data(subtype=args.subtype, data_dir=args.data_dir)
        
        print(f"\nğŸ“Š ì•„í˜• {args.subtype} ê²€ì¶œë¥  ë°ì´í„°:")
        print(f"   - Data points: {len(y)}")
        print(f"   - Features: {feat_names}")
        
        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        best_params = None
        
        # USE_OPTUNA í”Œë˜ê·¸ì— ë”°ë¼ ì²˜ë¦¬
        if USE_OPTUNA and OPTUNA_AVAILABLE:
            # Optunaë¡œ ìµœì í™” ì‹¤í–‰
            best_params = optimize_hyperparameters(X, y, labels, feat_names, n_trials=N_TRIALS)
            # Optuna ìµœì í™” ê²°ê³¼ ì €ì¥
            if best_params:
                save_best_hyperparameters(best_params)
        else:
            # Optuna ì‚¬ìš© ì•ˆ í•¨
            if USE_OPTUNA and not OPTUNA_AVAILABLE:
                print("\nâš ï¸ Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (USE_OPTUNA=True)")
                print("   ì„¤ì¹˜ ëª…ë ¹: pip install optuna")
            
            # JSON íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
            best_params = load_best_hyperparameters()
            
            # JSON íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if best_params is None:
                print("\nğŸ“‹ JSON íŒŒì¼ ì—†ìŒ â†’ Configì˜ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
                best_params = get_default_hyperparameters()
                print("ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
                for key, value in best_params.items():
                    print(f"   - {key}: {value}")
        
        model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df = train_and_eval(
            X, y, labels, feat_names,
            compute_fi=True,
            save_fi=True,
            optuna_params=best_params
        )
        
        print(f"\n=== ì•„í˜• {args.subtype} ê²€ì¶œë¥  ì˜ˆì¸¡ ê²°ê³¼ ===")
        print(f"Feature ê°œìˆ˜: {len(feat_names)}")
        exit(0)
    
    # ===== ì—°ë ¹ëŒ€ë³„ ì›ë³¸ ë°ì´í„° ëª¨ë“œ =====
    if args.raw_data or args.age_group:
        # ì—°ë ¹ëŒ€ ì§€ì • ì•ˆ í–ˆìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        age_group = args.age_group or '19-49ì„¸'
        
        print("=" * 60)
        print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° ëª¨ë“œ: ì—°ë ¹ëŒ€ '{age_group}' ë°ì´í„° ë¡œë“œ")
        print("=" * 60)
        
        # ì›ë³¸ ë°ì´í„°ì—ì„œ ì§ì ‘ ë¡œë“œ ë° ì „ì²˜ë¦¬
        X, y, labels, feat_names = load_and_prepare_by_age(
            age_group=age_group,
            data_dir=args.data_dir,
            use_exog=USE_EXOG
        )
        
        print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   - Data points: {len(y)}")
        print(f"   - Features used ({len(feat_names)}): {feat_names}")
        
        best_params = None
        
        # USE_OPTUNA í”Œë˜ê·¸ì— ë”°ë¼ ì²˜ë¦¬
        if USE_OPTUNA:
            if not OPTUNA_AVAILABLE:
                print("\nâš ï¸ Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (USE_OPTUNA=True)")
                print("   ì„¤ì¹˜ ëª…ë ¹: pip install optuna")
                # Optunaê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                best_params = get_default_hyperparameters()
            else:
                # Optunaë¡œ ìµœì í™” ì‹¤í–‰
                best_params = optimize_hyperparameters(X, y, labels, feat_names, n_trials=N_TRIALS)
                # Optuna ìµœì í™” ê²°ê³¼ ì €ì¥
                if best_params:
                    save_best_hyperparameters(best_params)
        else:
            # Optuna ì‚¬ìš© ì•ˆ í•¨ (USE_OPTUNA=False)
            # JSON íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
            best_params = load_best_hyperparameters()
            
            # JSON íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            if best_params is None:
                print("\nğŸ“‹ JSON íŒŒì¼ ì—†ìŒ â†’ Configì˜ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
                best_params = get_default_hyperparameters()
                print("ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
                for key, value in best_params.items():
                    print(f"   - {key}: {value}")
        
        # ìµœì¢… í•™ìŠµ ì‹¤í–‰
        model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df = train_and_eval(
            X, y, labels, feat_names,
            compute_fi=True,
            save_fi=True,
            optuna_params=best_params
        )

        print(f"\n=== [ê²°ê³¼ ìš”ì•½: ì—°ë ¹ëŒ€ '{age_group}'] ===")
        print(f"Feature ê°œìˆ˜: {len(feat_names)}")
        if fi_df is not None:
            print("\n[Top 10 Feature Importance]")
            print(fi_df.head(10).to_string(index=False))
        else:
            print("Feature Importance ê³„ì‚°ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        exit(0)
    
    # ===== PostgreSQL ëª¨ë“œ (ê¸°ë³¸) =====
    print("=" * 60)
    print("ğŸ’¾ PostgreSQL ëª¨ë“œ: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ")
    print("=" * 60)
    
    # PostgreSQLì—ì„œ ë°ì´í„° ë¡œë“œ
    df = load_data_from_postgres()
    
    print("\n" + "âœ… " * 30)
    print("ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
    print("âœ… " * 30 + "\n")
    
    # ë°ì´í„° í™•ì¸
    print(f"\nğŸ“Š DataFrame ì •ë³´:")
    print(f"   - Shape: {df.shape}")
    print(f"   - Columns: {list(df.columns)}")
    
    # ë‚ ì”¨ ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
    weather_cols_in_data = [c for c in ['min_temp', 'max_temp', 'avg_humidity'] if c in df.columns]
    if weather_cols_in_data:
        print(f"\nğŸŒ¡ï¸  ë‚ ì”¨ ë°ì´í„° í¬í•¨ í™•ì¸:")
        print(f"   âœ… PostgreSQL weather_data í…Œì´ë¸”ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´")
        print(f"   - í¬í•¨ëœ ë‚ ì”¨ ì»¬ëŸ¼: {weather_cols_in_data}")
        for col in weather_cols_in_data:
            data = df[col].dropna()
            if len(data) > 0:
                print(f"      â€¢ {col}: í‰ê·  {data.mean():.2f}, ë²”ìœ„ [{data.min():.2f}, {data.max():.2f}]")
    else:
        print(f"\nâš ï¸  ë‚ ì”¨ ë°ì´í„° ë¯¸í¬í•¨ (weather_data í…Œì´ë¸” í™•ì¸ í•„ìš”)")
    print(f"\nì²˜ìŒ 5ê°œ í–‰:")
    print(df.head())
    print(f"\në°ì´í„° íƒ€ì…:")
    print(df.dtypes)
    
    print(f"\nğŸ”§ USE_EXOG = '{USE_EXOG}'  (auto-detects vaccine/resp columns)")
    
    # DataFrameì„ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì „ì²˜ë¦¬
    print("\nğŸ“ˆ ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    X, y, labels, feat_names = load_and_prepare(
        df=df, 
        use_exog=USE_EXOG,
        age_group=args.age_group,
        subtype=args.subtype
    )
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   - Data points: {len(y)}")
    print(f"   - Features used ({len(feat_names)}): {feat_names}")
    
    best_params = None
    
    # USE_OPTUNA í”Œë˜ê·¸ì— ë”°ë¼ ì²˜ë¦¬
    if USE_OPTUNA:
        if not OPTUNA_AVAILABLE:
            print("\nâš ï¸ Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (USE_OPTUNA=True)")
            print("   ì„¤ì¹˜ ëª…ë ¹: pip install optuna")
            # Optunaê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            best_params = get_default_hyperparameters()
        else:
            # Optunaë¡œ ìµœì í™” ì‹¤í–‰
            best_params = optimize_hyperparameters(X, y, labels, feat_names, n_trials=N_TRIALS)
            # Optuna ìµœì í™” ê²°ê³¼ ì €ì¥
            if best_params:
                save_best_hyperparameters(best_params)
    else:
        # Optuna ì‚¬ìš© ì•ˆ í•¨ (USE_OPTUNA=False)
        # JSON íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        best_params = load_best_hyperparameters()
        
        # JSON íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if best_params is None:
            print("\nğŸ“‹ JSON íŒŒì¼ ì—†ìŒ â†’ Configì˜ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            best_params = get_default_hyperparameters()
            print("ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
            for key, value in best_params.items():
                print(f"   - {key}: {value}")
    
    # ìµœì¢… í•™ìŠµ ì‹¤í–‰
    model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df = train_and_eval(
        X, y, labels, feat_names,
        compute_fi=True,
        save_fi=True,
        optuna_params=best_params
    )

    print("\n=== [ê²°ê³¼ ìš”ì•½] ===")
    print(f"Feature ê°œìˆ˜: {len(feat_names)}")
    if fi_df is not None:
        print("\n[Top 10 Feature Importance]")
        print(fi_df.head(10).to_string(index=False))
    else:
        print("Feature Importance ê³„ì‚°ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")