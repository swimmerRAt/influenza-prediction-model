import math
from pathlib import Path
from typing import List, Tuple, Optional
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from dotenv import load_dotenv

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

# DuckDB for efficient data loading
from database.db_utils import TimeSeriesDB, load_from_duckdb

# =========================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# =========================
print("=" * 60)
print("ğŸ” í™˜ê²½ë³€ìˆ˜ ë¡œë“œ")
print("=" * 60)

# .env íŒŒì¼ ê²½ë¡œ í™•ì¸
env_path = Path.cwd() / '.env'
print(f"1. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
print(f"2. .env íŒŒì¼ ê²½ë¡œ: {env_path}")
print(f"3. .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {env_path.exists()}")

# .env íŒŒì¼ ë¡œë“œ
load_result = load_dotenv(env_path, verbose=True, override=True)
print(f"4. .env ë¡œë“œ ê²°ê³¼: {load_result}")
print("=" * 60 + "\n")

# =========================
# Paths & device
# =========================
BASE_DIR = Path.cwd()

# CSV íŒŒì¼ í›„ë³´ ê²½ë¡œ
CANDIDATE_CSVS = [
    BASE_DIR / "data" / "merged" / "merged_influenza_data.csv",
    BASE_DIR / "merged_influenza_data.csv",
    BASE_DIR / "data" / "merged_influenza_data.csv",
]

# =========================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜
# =========================
def load_data_from_duckdb_or_csv(csv_path=None, use_duckdb=None):
    """
    DuckDB ë˜ëŠ” ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    csv_path : Path, optional
        CSV ì‚¬ìš© ì‹œ íŒŒì¼ ê²½ë¡œ
    use_duckdb : bool, optional
        Trueë©´ DuckDB ì‚¬ìš©, Falseë©´ CSV ì§ì ‘ ë¡œë“œ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ USE_DUCKDB)
    
    Returns:
    --------
    pd.DataFrame
        ë¡œë“œëœ ë°ì´í„°
    """
    if use_duckdb is None:
        use_duckdb = os.getenv('USE_DUCKDB', 'true').lower() == 'true'
    
    print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ëª¨ë“œ: use_duckdb={use_duckdb}")
    
    # DuckDB ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    db_path = Path("database/influenza_data.duckdb")
    
    if use_duckdb and db_path.exists():
        print("=" * 50)
        print("ğŸ’¾ DuckDB ëª¨ë“œ: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
        print("=" * 50)
        try:
            df = load_from_duckdb(
                db_path=str(db_path),
                table_name="influenza_data"
            )
            print(f"âœ… DuckDB ë¡œë“œ ì™„ë£Œ: {df.shape}")
            return df
        except Exception as e:
            print(f"âš ï¸ DuckDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"ğŸ“ CSV íŒŒì¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
            use_duckdb = False
    
    if not use_duckdb or not db_path.exists():
        print("=" * 50)
        print("ğŸ“ CSV ëª¨ë“œ: ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        print("=" * 50)
        if csv_path is None:
            csv_path = pick_csv_path()
        
        # CSV íŒŒì¼ì´ í¬ë©´ DuckDBë¡œ ë³€í™˜ ì œì•ˆ
        csv_size_mb = csv_path.stat().st_size / (1024 * 1024)
        if csv_size_mb > 100:  # 100MB ì´ìƒ
            print(f"\nğŸ’¡ íŒ: CSV íŒŒì¼ì´ {csv_size_mb:.1f}MBë¡œ í½ë‹ˆë‹¤.")
            print(f"   DuckDBë¡œ ë³€í™˜í•˜ë©´ ë¡œë”© ì†ë„ê°€ 10~100ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
            print(f"   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë³€í™˜í•˜ì„¸ìš”:")
            print(f"   python database/db_utils.py\n")
        
        print(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        start_time = time.time()
        df = pd.read_csv(csv_path)
        elapsed = time.time() - start_time
        print(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {csv_path}, {df.shape} ({elapsed:.2f}ì´ˆ)")
        return df

def pick_csv_path():
    for p in CANDIDATE_CSVS:
        if p.exists():
            return p
    raise FileNotFoundError("No input CSV found among:\n" + "\n".join(map(str, CANDIDATE_CSVS)))

# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì • (DuckDB ë°±ì—…ìš©)
print("\n" + "=" * 60)
print("ğŸ“‚ CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •")
print("=" * 60)

try:
    CSV_PATH = pick_csv_path()
    print(f"âœ… CSV íŒŒì¼ ë°œê²¬: {CSV_PATH.name}")
except FileNotFoundError as e:
    print(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print(f"   ê²€ìƒ‰í•œ ê²½ë¡œ: {CANDIDATE_CSVS}")
    CSV_PATH = None

print("=" * 60 + "\n")

def pick_device():
    if torch.cuda.is_available():
        return "cuda"
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"

DEVICE = pick_device()
SEED   = 42

print(f"ğŸ–¥ï¸ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {DEVICE}")
print(f"ğŸ² ëœë¤ ì‹œë“œ: {SEED}\n")


# =========================
# Hyperparameters
# =========================
EPOCHS      = 100
BATCH_SIZE  = 64        # ì†Œê·œëª¨ ì‹œê³„ì—´ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ë„ë¡ ì•½ê°„ ë‚®ì¶¤
SEQ_LEN     = 12
PRED_LEN    = 3
PATCH_LEN   = 4          # â† CNNì´ ìµœì†Œ 3~5 ì»¤ë„ ì ìš© ê°€ëŠ¥í•˜ë„ë¡ í™•ëŒ€
STRIDE      = 1

D_MODEL     = 128        # 4ì˜ ë°°ìˆ˜(ë©€í‹°ìŠ¤ì¼€ì¼ ë¶„ê¸° 4ê°œ í•©ì‚°)
N_HEADS     = 2
ENC_LAYERS  = 4
FF_DIM      = 128
DROPOUT     = 0.3        # ì•½ê°„ ê°•í™”
HEAD_HIDDEN = [64, 64]

LR              = 5e-4
WEIGHT_DECAY    = 5e-4
PATIENCE        = 60
WARMUP_EPOCHS   = 30

SCALER_TYPE     = "robust"   # ë…¸ì´ì¦ˆ/ê¼¬ë¦¬ê°’ ëŒ€ì‘ì— ìœ ë¦¬ (ì›í•˜ë©´ "standard"ë¡œ ë³€ê²½)

# ì™¸ìƒ íŠ¹ì§• ì‚¬ìš© ëª¨ë“œ: "auto"|"none"|"vax"|"resp"|"both"
USE_EXOG        = "all"

OUT_CSV          = str(BASE_DIR / "ili_predictions.csv")
PLOT_LAST_WINDOW = str(BASE_DIR / "plot_last_window.png")
PLOT_TEST_RECON  = str(BASE_DIR / "plot_test_reconstruction.png")
PLOT_MA_CURVES   = str(BASE_DIR / "plot_ma_curves.png")

# overlap ì¬êµ¬ì„± ê°€ì¤‘ì¹˜ (t+1ì„ ì¡°ê¸ˆ ë” ì‹ ë¢°)
RECON_W_START, RECON_W_END = 2.0, 0.5

# --- Feature switches ---
INCLUDE_SEASONAL_FEATS = True   # week_sin, week_cosë¥¼ ì…ë ¥ í”¼ì²˜ì— í¬í•¨í• ì§€

# =========================
# utils
# =========================
from datetime import date

def _iso_weeks_in_year(y: int) -> int:
    # ISO ë‹¬ë ¥ì˜ ë§ˆì§€ë§‰ ì£¼ ë²ˆí˜¸(52 ë˜ëŠ” 53)
    return date(y, 12, 28).isocalendar().week

def weekly_to_daily_interp(
    df: pd.DataFrame,
    season_col: str = "season_norm",
    week_col: str = "week",
    target_col: str = "ili",
) -> pd.DataFrame:
    """
    ì£¼ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì¼ ë‹¨ìœ„ë¡œ í™•ì¥(ì„ í˜•ë³´ê°„). season/week ì—†ìœ¼ë©´ labelì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜,
    ìµœí›„ì—ëŠ” ì—°ì† ì£¼ì°¨ë¥¼ ìƒì„±í•´ ë³´ê°„í•©ë‹ˆë‹¤.
    ë°˜í™˜: date ì»¬ëŸ¼ í¬í•¨í•œ ì¼ ë‹¨ìœ„ DF
    """
    df = df.copy()
    df.columns = df.columns.str.replace("\ufeff", "", regex=True).str.strip()

    # --- ì‹œì¦Œ/ì£¼ì°¨ í™•ë³´ ---
    has_season = season_col in df.columns
    has_week   = week_col in df.columns

    if not (has_season and has_week):
        # labelì—ì„œ ì‹œì¦Œ/ì£¼ì°¨ ì¶”ì¶œ ì‹œë„: "2024-2025 season - W29"
        if "label" in df.columns:
            import re
            def _parse_label(lbl):
                m = re.search(r"(\d{4}-\d{4}).*W\s*([0-9]+)", str(lbl))
                if m:
                    return m.group(1), int(m.group(2))
                return None
            parsed = df["label"].map(_parse_label)
            if not has_season:
                df[season_col] = [p[0] if p else np.nan for p in parsed]
                has_season = True
            if not has_week:
                df[week_col] = [p[1] if p else np.nan for p in parsed]
                has_week = True

    # ìµœí›„ì˜ ìˆ˜ë‹¨: season_normì´ ì—†ìœ¼ë©´ ë‹¨ì¼ ì‹œì¦Œìœ¼ë¡œ, week ì—†ìœ¼ë©´ 1..N
    if not has_season:
        # ì²« í–‰ì˜ ì—°ë„ë¥¼ ì°¾ì•„ ëŒ€ì²´ ì‹œì¦Œëª… ë§Œë“¤ê¸°
        # ì—†ìœ¼ë©´ "0000-0001"
        first_year = None
        if "date" in df.columns:
            try:
                first_year = pd.to_datetime(df["date"]).dt.year.min()
            except Exception:
                pass
        if first_year is None:
            first_year = pd.Timestamp.today().year
        df[season_col] = f"{first_year}-{first_year+1}"
        has_season = True

    if not has_week:
        df[week_col] = np.arange(1, len(df) + 1, dtype=int)
        has_week = True

    # ìˆ«ìí™”
    df[week_col] = pd.to_numeric(df[week_col], errors="coerce")
    # ì‹œì¦Œ ë¬¸ìì—´ ì •ê·œí™”
    def _norm_season_text_local(s: str) -> str:
        ss = str(s).replace("ì ˆê¸°", "")
        import re
        m = re.search(r"(\d{4})\s*-\s*(\d{4})", ss)
        return f"{m.group(1)}-{m.group(2)}" if m else ss.strip()
    df[season_col] = df[season_col].astype(str).map(_norm_season_text_local)

    # --- ISO ì£¼ ì‹œì‘ì¼ ì‚°ì¶œ (ì‹œì¦Œ ê·œì¹™ ë°˜ì˜) ---
    week_starts = []
    for _, row in df.iterrows():
        season = str(row[season_col])
        try:
            y0 = int(season.split("-")[0])
        except Exception:
            y0 = pd.Timestamp.today().year
        wk = int(row[week_col]) if not pd.isna(row[week_col]) else 1
        iso_year = y0 if wk >= 36 else (y0 + 1)
        # í•´ë‹¹ ISOë…„ì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì£¼ ë„˜ì§€ ì•Šë„ë¡ ë³´ì •
        wk = min(max(1, wk), _iso_weeks_in_year(iso_year))
        # ì›”ìš”ì¼(1) ê¸°ì¤€ ì£¼ ì‹œì‘ì¼
        week_starts.append(pd.Timestamp.fromisocalendar(iso_year, wk, 1))
    df["week_start"] = week_starts

    # --- ì¤‘ë³µ week_start ì²˜ë¦¬: ìˆ˜ì¹˜=mean, ë¹„ìˆ˜ì¹˜=first ---
    if df["week_start"].duplicated().any():
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        agg = {c: "mean" for c in num_cols}
        # ë¹„ìˆ˜ì¹˜ ì»¬ëŸ¼(ë¼ë²¨/ì‹œì¦Œ ë“±)ì€ ì²« ê°’ ìœ ì§€
        for c in df.columns:
            if c not in num_cols and c != "week_start":
                agg[c] = "first"
        df = df.groupby("week_start", as_index=False).agg(agg)

    # --- ì¼ ë‹¨ìœ„ ë¦¬ìƒ˜í”Œ ---
    df = df.set_index("week_start").sort_index()
    df_daily = df.resample("D").asfreq()

    # ìˆ˜ì¹˜í˜•ì€ ì„ í˜•ë³´ê°„
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for c in num_cols:
        df_daily[c] = df_daily[c].interpolate(method="linear", limit_direction="both")

    # ë²”ì£¼í˜•ì€ ì•ë’¤ ì±„ì›€
    cat_cols = [c for c in df.columns if c not in num_cols]
    for c in cat_cols:
        df_daily[c] = df_daily[c].ffill().bfill()

    # ê²°ê³¼
    out = df_daily.reset_index().rename(columns={"week_start": "date"})
    # dateëŠ” datetimeìœ¼ë¡œ ê°•ì œ
    out["date"] = pd.to_datetime(out["date"])
    return out
    
def set_seed(seed=42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)



def make_splits(n: int, train_ratio=0.7, val_ratio=0.15):
    n_train = int(n * train_ratio)
    n_val   = int(n * val_ratio)
    return (0, n_train), (n_train, n_train+n_val), (n_train+n_val, n)

def get_scaler(name=None):
    s = (name or SCALER_TYPE).lower()
    if s == "robust":  return RobustScaler()
    if s == "minmax":  return MinMaxScaler()
    return StandardScaler()

def _norm_season_text(s: str) -> str:
    ss = str(s).replace("ì ˆê¸°", "")
    import re
    m = re.search(r"(\d{4})\s*-\s*(\d{4})", ss)
    return f"{m.group(1)}-{m.group(2)}" if m else ss.strip()

# =========================
# data loader (multivariate-ready)
# =========================
def load_and_prepare(df: pd.DataFrame, use_exog: str = "auto") -> Tuple[np.ndarray, np.ndarray, list, list]:
    """
    DuckDB ë˜ëŠ” CSV ë°ì´í„°ë¥¼ PatchTST ëª¨ë¸ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬
    
    Returns:
        X: (N, F) features (first column should be target variable)
        y: (N,) target (ì˜ì‚¬í™˜ì ë¶„ìœ¨)
        labels: list[str] for plotting ticks
        used_feat_names: list[str] feature column names (len=F)
    
    Parameters:
        df: DuckDB ë˜ëŠ” APIì—ì„œ ê°€ì ¸ì˜¨ DataFrame
        use_exog: ì™¸ìƒë³€ìˆ˜ ì‚¬ìš© ëª¨ë“œ
    """
    if df is None:
        raise ValueError("dfëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
    
    df = df.copy()
    
    print(f"\nğŸ“Š ì›ë³¸ ë°ì´í„° êµ¬ì¡°:")
    print(f"   - Shape: {df.shape}")
    print(f"   - Columns: {list(df.columns)}")
    
    # ===== DuckDB ë°ì´í„° í˜•ì‹ ê°ì§€ ë° ì²˜ë¦¬ =====
    is_duckdb_format = all(col in df.columns for col in ['ì—°ë„', 'ì£¼ì°¨', 'ì—°ë ¹ëŒ€'])
    
    if is_duckdb_format:
        print(f"\nğŸ” DuckDB ë°ì´í„° í˜•ì‹ ê°ì§€ë¨ - ì—°ë ¹ëŒ€ë³„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        # ì—°ë ¹ëŒ€ë³„ ë°ì´í„° í™•ì¸
        age_groups = df['ì—°ë ¹ëŒ€'].unique()
        print(f"   - ê³ ìœ  ì—°ë ¹ëŒ€: {len(age_groups)}ê°œ")
        print(f"   - ì—°ë ¹ëŒ€ ëª©ë¡: {sorted(age_groups)[:5]}...")
        
        # ì—¬ëŸ¬ ì—°ë ¹ëŒ€ ì¤‘ ë°ì´í„°ê°€ ê°€ì¥ í’ë¶€í•œ ì—°ë ¹ëŒ€ ì„ íƒ
        # ìš°ì„ ìˆœìœ„: 19-49ì„¸ (ê°€ì¥ ì¼ë°˜ì ) > 65ì„¸ì´ìƒ > 65ì„¸ ì´ìƒ > ì „ì²´ í‰ê· 
        candidate_age_groups = ['19-49ì„¸', '65ì„¸ì´ìƒ', '65ì„¸ ì´ìƒ', '0-6ì„¸']
        target_age_group = None
        
        for candidate in candidate_age_groups:
            if candidate in age_groups:
                # í•´ë‹¹ ì—°ë ¹ëŒ€ì˜ ë°ì´í„° í’ˆì§ˆ í™•ì¸
                temp_df = df[df['ì—°ë ¹ëŒ€'] == candidate].copy()
                valid_ili = temp_df['ì˜ì‚¬í™˜ì ë¶„ìœ¨'].notna().sum()
                if valid_ili > 100:  # ìµœì†Œ 100ê°œ ì´ìƒì˜ ìœ íš¨ ë°ì´í„°
                    target_age_group = candidate
                    break
        
        if target_age_group and target_age_group in age_groups:
            print(f"   - '{target_age_group}' ì—°ë ¹ëŒ€ ë°ì´í„° ì‚¬ìš©")
            df_age = df[df['ì—°ë ¹ëŒ€'] == target_age_group].copy()
            
            # â­ ì˜ì‚¬í™˜ì ë¶„ìœ¨ì´ NaNì¸ í–‰ ì œê±° (í•„ìˆ˜!)
            before_count = len(df_age)
            df_age = df_age[df_age['ì˜ì‚¬í™˜ì ë¶„ìœ¨'].notna()].copy()
            after_count = len(df_age)
            if before_count > after_count:
                print(f"   - ì˜ì‚¬í™˜ì ë¶„ìœ¨ NaN í–‰ ì œê±°: {before_count - after_count}ê°œ ì œê±°ë¨")
            
            # ì˜ˆë°©ì ‘ì¢…ë¥ ì´ ëª¨ë‘ NaNì¸ ê²½ìš° ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
            if df_age['ì˜ˆë°©ì ‘ì¢…ë¥ '].notna().sum() == 0:
                print(f"   - '{target_age_group}' ì—°ë ¹ëŒ€ì— ì˜ˆë°©ì ‘ì¢…ë¥  ë°ì´í„° ì—†ìŒ - ì „ì²´ í‰ê·  ì‚¬ìš©")
                # ì—°ë„/ì£¼ì°¨ë³„ë¡œ ì „ì²´ ì—°ë ¹ëŒ€ì˜ ì˜ˆë°©ì ‘ì¢…ë¥  í‰ê·  ê³„ì‚°
                vaccine_avg = df.groupby(['ì—°ë„', 'ì£¼ì°¨'], as_index=False)['ì˜ˆë°©ì ‘ì¢…ë¥ '].mean()
                vaccine_avg = vaccine_avg.rename(columns={'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'ì˜ˆë°©ì ‘ì¢…ë¥ _ì „ì²´í‰ê· '})
                df_age = df_age.merge(vaccine_avg, on=['ì—°ë„', 'ì£¼ì°¨'], how='left')
                df_age['ì˜ˆë°©ì ‘ì¢…ë¥ '] = df_age['ì˜ˆë°©ì ‘ì¢…ë¥ _ì „ì²´í‰ê· ']
                df_age = df_age.drop(columns=['ì˜ˆë°©ì ‘ì¢…ë¥ _ì „ì²´í‰ê· '])
            
            df = df_age
        else:
            # ì ì ˆí•œ ë‹¨ì¼ ì—°ë ¹ëŒ€ê°€ ì—†ìœ¼ë©´ ì—°ë„/ì£¼ì°¨ë³„ í‰ê·  ì‚¬ìš©
            print(f"   - ì—°ë„/ì£¼ì°¨ë³„ ì „ì²´ ì—°ë ¹ëŒ€ í‰ê·  ì‚¬ìš©")
            numeric_cols = ['ì˜ì‚¬í™˜ì ë¶„ìœ¨', 'ì…ì›í™˜ì ìˆ˜', 'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ', 'ì˜ˆë°©ì ‘ì¢…ë¥ ', 'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì']
            agg_dict = {col: 'mean' for col in numeric_cols if col in df.columns}
            agg_dict['ì•„í˜•'] = 'first'  # ì•„í˜•ì€ ì²« ê°’ ì‚¬ìš©
            
            df = df.groupby(['ì—°ë„', 'ì£¼ì°¨'], as_index=False).agg(agg_dict)
        
        # ì»¬ëŸ¼ëª… ë§¤í•‘ (DuckDB -> ê¸°ì¡´ í˜•ì‹)
        column_mapping = {
            'ì—°ë„': 'year',
            'ì£¼ì°¨': 'week',
            'ì˜ì‚¬í™˜ì ë¶„ìœ¨': 'ili',
            'ì˜ˆë°©ì ‘ì¢…ë¥ ': 'vaccine_rate',
            'ì…ì›í™˜ì ìˆ˜': 'hospitalization',
            'ì¸í”Œë£¨ì—”ì ê²€ì¶œë¥ ': 'detection_rate',
            'ì‘ê¸‰ì‹¤ ì¸í”Œë£¨ì—”ì í™˜ì': 'emergency_patients'
        }
        
        df = df.rename(columns=column_mapping)
        
        # ì •ë ¬
        df = df.sort_values(['year', 'week']).reset_index(drop=True)
        
        # season_norm ìƒì„± (week 36 ì´ìƒì€ í˜„ì¬ ì—°ë„ ì‹œì¦Œ, ë¯¸ë§Œì€ ë‹¤ìŒ ì—°ë„ ì‹œì¦Œ)
        df['season_norm'] = df.apply(
            lambda row: f"{int(row['year'])}-{int(row['year'])+1}" if row['week'] >= 36 
                       else f"{int(row['year'])-1}-{int(row['year'])}",
            axis=1
        )
        
        print(f"\nâœ… DuckDB ë°ì´í„° ë³€í™˜ ì™„ë£Œ:")
        print(f"   - ë³€í™˜ í›„ Shape: {df.shape}")
        print(f"   - ì—°ë„ ë²”ìœ„: {df['year'].min():.0f} ~ {df['year'].max():.0f}")
        print(f"   - ì£¼ì°¨ ë²”ìœ„: {df['week'].min():.0f} ~ {df['week'].max():.0f}")
        print(f"   - ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {len(df)}")
    
    # ===== ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§ =====
    # ì£¼ ë‹¨ìœ„ -> ì¼ ë‹¨ìœ„ ë³´ê°„ (ì„ íƒì‚¬í•­)
    # df = weekly_to_daily_interp(df, season_col="season_norm", week_col="week", target_col="ili")
    
    # ì •ë ¬ í™•ì¸
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date").reset_index(drop=True)
    elif {"season_norm", "week"}.issubset(df.columns):
        df["season_norm"] = df["season_norm"].astype(str).map(_norm_season_text)
        df["week"] = pd.to_numeric(df["week"], errors="coerce")
        df = df.sort_values(["season_norm", "week"]).reset_index(drop=True)
    elif "label" in df.columns:
        df = df.sort_values(["label"]).reset_index(drop=True)

    # íƒ€ê¹ƒ ë³€ìˆ˜ í™•ì¸
    if "ili" not in df.columns:
        raise ValueError("ë°ì´í„°ì— 'ili' (ì˜ì‚¬í™˜ì ë¶„ìœ¨) ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    df["ili"] = pd.to_numeric(df["ili"], errors="coerce")
    if df["ili"].isna().any():
        print(f"   âš ï¸ 'ili' ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ {df['ili'].isna().sum()}ê°œ ë°œê²¬ - ë³´ê°„ ì²˜ë¦¬")
        df["ili"] = df["ili"].interpolate(method="linear", limit_direction="both").fillna(df["ili"].median())
    
    # --- Seasonality feature ì¶”ê°€ ---
    if "week" in df.columns:
        df["week_sin"] = np.sin(2 * np.pi * df["week"] / 52.0)
        df["week_cos"] = np.cos(2 * np.pi * df["week"] / 52.0)
    else:
        df["week_sin"] = 0.0
        df["week_cos"] = 0.0

    # --- Alias ë§¤í•‘ ---
    if "hospitalization" in df.columns and "respiratory_index" not in df.columns:
        df["respiratory_index"] = df["hospitalization"]
    if "case_count" in df.columns and "respiratory_index" not in df.columns:
        df["respiratory_index"] = df["case_count"]

    # ê¸°í›„ í”¼ì²˜ í›„ë³´
    climate_feats = []
    if "wx_week_avg_temp" in df.columns:     climate_feats.append("wx_week_avg_temp")
    if "wx_week_avg_rain" in df.columns:     climate_feats.append("wx_week_avg_rain")
    if "wx_week_avg_humidity" in df.columns: climate_feats.append("wx_week_avg_humidity")
    if "detection_rate" in df.columns:       climate_feats.append("detection_rate")  # DuckDB íŠ¹ì„±

    # ì™¸ìƒ í›„ë³´ ì¡´ì¬ ì—¬ë¶€
    has_vax  = "vaccine_rate" in df.columns
    has_resp = "respiratory_index" in df.columns or "hospitalization" in df.columns

    # ì–´ë–¤ íŠ¹ì§•ì„ ì“¸ì§€ ê²°ì •
    mode = use_exog.lower()
    if mode == "auto":
        chosen = ["ili"]
        if has_vax:  chosen.append("vaccine_rate")
        if has_resp and "respiratory_index" in df.columns: chosen.append("respiratory_index")
        elif has_resp and "hospitalization" in df.columns: chosen.append("hospitalization")
        chosen += climate_feats
    elif mode == "none":
        chosen = ["ili"]
    elif mode == "vax":
        chosen = ["ili"] + (["vaccine_rate"] if has_vax else [])
    elif mode == "resp":
        chosen = ["ili"]
        if has_resp and "respiratory_index" in df.columns: 
            chosen.append("respiratory_index")
        elif has_resp and "hospitalization" in df.columns: 
            chosen.append("hospitalization")
    elif mode == "both":
        chosen = ["ili"]
        if has_vax:  chosen.append("vaccine_rate")
        if has_resp and "respiratory_index" in df.columns: chosen.append("respiratory_index")
        elif has_resp and "hospitalization" in df.columns: chosen.append("hospitalization")
        chosen += climate_feats
    elif mode == "climate":
        chosen = ["ili"] + climate_feats
    elif mode == "all":
        chosen = ["ili"]
        if has_vax:  chosen.append("vaccine_rate")
        if has_resp and "respiratory_index" in df.columns: chosen.append("respiratory_index")
        elif has_resp and "hospitalization" in df.columns: chosen.append("hospitalization")
        chosen += climate_feats
    else:
        raise ValueError(f"Unknown USE_EXOG mode: {use_exog}")

    # ìˆ«ìí™” & ë³´ê°„
    for c in chosen:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            if df[c].isna().any():
                # ì„ í˜• ë³´ê°„ í›„ medianìœ¼ë¡œ ë‚¨ì€ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
                df[c] = df[c].interpolate(method="linear", limit_direction="both")
                # ì—¬ì „íˆ NaNì´ ìˆìœ¼ë©´ median ì‚¬ìš© (medianë„ NaNì´ë©´ 0 ì‚¬ìš©)
                median_val = df[c].median()
                if pd.isna(median_val):
                    median_val = 0.0
                df[c] = df[c].fillna(median_val)
        else:
            # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
            print(f"   âš ï¸ ì»¬ëŸ¼ '{c}'ê°€ ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
            df[c] = 0.0

    # ë¼ë²¨
    if "label" in df.columns and df["label"].notna().any():
        labels = df["label"].astype(str).tolist()
    elif {"season_norm","week"}.issubset(df.columns):
        labels = (df["season_norm"].astype(str) + " season - W" + df["week"].astype(int).astype(str)).tolist()
    else:
        labels = [f"idx_{i}" for i in range(len(df))]

    # X, y êµ¬ì„±
    feat_names = chosen[:]
    if INCLUDE_SEASONAL_FEATS and {"week_sin", "week_cos"}.issubset(df.columns):
        feat_names += ["week_sin", "week_cos"]

    # ì„ íƒëœ ì…ë ¥ í”¼ì²˜ ë¡œê·¸
    print(f"\n[Data] Exogenous detected -> vaccine_rate: {has_vax} | respiratory/hospitalization: {has_resp} | climate_feats: {climate_feats}")
    print(f"[Data] Selected feature columns (order) -> {feat_names}")

    X = df[feat_names].to_numpy(dtype=float)
    y = df["ili"].to_numpy(dtype=float)
    
    print(f"\nâœ… ìµœì¢… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"   - X shape: {X.shape}")
    print(f"   - y shape: {y.shape}")
    print(f"   - Features: {len(feat_names)}")
    
    return X, y, labels, feat_names

# =========================
# dataset
# =========================
class PatchTSTDataset(Dataset):
    """Multivariate X (N,F) + y (N,) -> (patchified) windows."""
    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len:int, pred_len:int, patch_len:int, stride:int):
        assert len(X) == len(y)
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)
        self.seq_len, self.pred_len = seq_len, pred_len
        self.patch_len, self.stride = patch_len, stride
        max_start = len(self.y) - (seq_len + pred_len)
        self.indices = list(range(max(0, max_start + 1)))

    def __len__(self): return len(self.indices)

    def __getitem__(self, idx):
        i = self.indices[idx]
        seq_X = self.X[i:i+self.seq_len, :]                      # (L, F)
        tgt_y = self.y[i+self.seq_len:i+self.seq_len+self.pred_len]  # (H,)

        # patchify along time axis
        patches = []
        pos = 0
        while pos + self.patch_len <= self.seq_len:
            patches.append(seq_X[pos:pos+self.patch_len, :])     # (patch_len, F)
            pos += self.stride
        X_patch = np.stack(patches, axis=0)                      # (P, patch_len, F)
        return torch.from_numpy(X_patch).float(), torch.from_numpy(tgt_y).float(), i

# =========================
# model (Multi-Scale CNN + TokenConvMixer + PatchTST + AttnPool)
# =========================
class MultiScaleCNNPatchEmbed(nn.Module):
    """
    (B, P, L, F) -> [ê° íŒ¨ì¹˜] ë©€í‹°ìŠ¤ì¼€ì¼ Conv1d ë¶„ê¸°(k=2/3/5, ë˜ í•˜ë‚˜ëŠ” dilation=2) â†’ GAP â†’ (B, P, D)
    - ë¶„ê¸° 4ê°œ ì¶œë ¥ concat â†’ D_MODEL
    - íŒ¨ì¹˜ ë‚´ë¶€ì˜ ê¸‰ê²©/ì™„ë§Œ/ì”ì§„ë™ íŒ¨í„´ì„ ë™ì‹œì— í¬ì°©
    """
    def __init__(self, in_features: int, patch_len: int, d_model: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % 4 == 0, "d_modelì€ 4ì˜ ë°°ìˆ˜ê°€ ë˜ì–´ì•¼ ë©€í‹°ìŠ¤ì¼€ì¼ ë¶„ê¸° í•©ì‚°ì´ ë§ìŠµë‹ˆë‹¤."
        out_ch = d_model // 4
    # ì»¤ë„ í¬ê¸°ë¥¼ patch_lenì— ë¹„ë¡€í•˜ê²Œ ì„¤ì •
        self.b2 = nn.Conv1d(in_features, out_ch, kernel_size=1, padding=0)
        self.b3 = nn.Conv1d(in_features, out_ch, kernel_size=3, padding=1)
        self.b5 = nn.Conv1d(in_features, out_ch, kernel_size=5, padding=2)
        self.bd = nn.Conv1d(in_features, out_ch, kernel_size=3, padding=2, dilation=2)

        self.bn   = nn.BatchNorm1d(d_model)
        self.act  = nn.GELU()
        self.pool = nn.AdaptiveAvgPool1d(1)   # (B*P, D, L) â†’ (B*P, D, 1)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        # x: (B, P, L, F)
        B, P, L, F = x.shape
        x = x.view(B*P, L, F).permute(0, 2, 1)        # (B*P, F, L)

        z = torch.cat([self.b2(x), self.b3(x), self.b5(x), self.bd(x)], dim=1)  # (B*P, D, L)
        z = self.act(self.bn(z))
        z = self.pool(z).squeeze(-1)                  # (B*P, D)
        z = self.drop(z)
        return z.view(B, P, -1)                       # (B, P, D)

class TokenConvMixer(nn.Module):
    """
    íŒ¨ì¹˜ í† í° ê°„(P ì¶•) ë¡œì»¬ ì—°ì†ì„± ê°•í™”: DepthwiseConv1d(P-ì¶•) + PointwiseConv1d
    ì…ë ¥/ì¶œë ¥: (B, P, D)
    """
    def __init__(self, d_model: int, dropout: float = 0.1):
        super().__init__()
        self.dw = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)
        self.pw = nn.Conv1d(d_model, d_model, kernel_size=1)
        self.bn = nn.BatchNorm1d(d_model)
        self.act = nn.GELU()
        self.drop = nn.Dropout(dropout)

    def forward(self, z):              # (B, P, D)
        y = z.permute(0, 2, 1)         # (B, D, P)
        y = self.dw(y)
        y = self.pw(y)
        y = self.bn(y)
        y = self.act(y)
        y = self.drop(y)
        y = y.permute(0, 2, 1)         # (B, P, D)
        return z + y                   # Residual

class PositionalEncoding(nn.Module):
    def __init__(self, d_model:int, max_len:int=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).float().unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))
        pe[:,0::2] = torch.sin(position*div)
        if d_model % 2 == 1:
            pe[:,1::2] = torch.cos(position*div)[:, :pe[:,1::2].shape[1]]
        else:
            pe[:,1::2] = torch.cos(position*div)
        self.register_buffer("pe", pe.unsqueeze(0))
    def forward(self, x):
        P = x.size(1)
        return x + self.pe[:, :P, :]

class AttnPool(nn.Module):
    """Learnable-query attention pooling over patch tokens."""
    def __init__(self, d_model:int):
        super().__init__()
        self.q = nn.Parameter(torch.randn(1, 1, d_model))
        self.proj = nn.Linear(d_model, d_model, bias=False)
    def forward(self, z):           # z: (B, P, D)
        B,P,D = z.shape
        q = self.q.expand(B, -1, -1)                       # (B,1,D)
        k = self.proj(z)                                   # (B,P,D)
        attn = torch.softmax((q @ k.transpose(1,2)) / (D**0.5), dim=-1)  # (B,1,P)
        pooled = attn @ z                                  # (B,1,D)
        return pooled.squeeze(1)                           # (B,D)

class PatchTSTModel(nn.Module):
    def __init__(self, in_features:int, patch_len:int, d_model:int, n_heads:int,
                 n_layers:int, ff_dim:int, dropout:float, pred_len:int, head_hidden:List[int]):
        super().__init__()
        # â‘  ë©€í‹°ìŠ¤ì¼€ì¼ CNN íŒ¨ì¹˜ ì„ë² ë”©
        self.embed = MultiScaleCNNPatchEmbed(in_features, patch_len, d_model, dropout=dropout*0.5)
        # â‘¡ íŒ¨ì¹˜ í† í° ê°„ ë¡œì»¬ ì—°ì†ì„± ë¯¹ì„œ
        self.mixer = nn.Sequential(
            TokenConvMixer(d_model, dropout=dropout),
            TokenConvMixer(d_model, dropout=dropout),
        )
        # â‘¢ PatchTST ì¸ì½”ë”
        self.posenc = PositionalEncoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim,
            dropout=dropout, batch_first=True, activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.pool = AttnPool(d_model)

        # â‘£ ì˜ˆì¸¡ í—¤ë“œ
        mlp, in_dim = [], d_model
        for h in head_hidden[:2]:
            mlp += [nn.Linear(in_dim, h), nn.GELU(), nn.Dropout(dropout)]
            in_dim = h
        mlp.append(nn.Linear(in_dim, pred_len))
        self.head = nn.Sequential(*mlp)

    def forward(self, x):
        # x: (B, P, L, F)
        z = self.embed(x)      # (B,P,D)
        z = self.mixer(z)      # (B,P,D)
        z = self.posenc(z)
        z = self.encoder(z)
        z = self.pool(z)       # (B,D)
        return self.head(z)    # (B,H)

    def correlation_loss(pred, true):
    # pred, true: (B, H)
        pred = pred - pred.mean(dim=1, keepdim=True)
        true = true - true.mean(dim=1, keepdim=True)
        corr = (pred * true).sum(dim=1) / (
            (pred.norm(dim=1) * true.norm(dim=1)) + 1e-6
        )
        return 1 - corr.mean()
    # =========================
# helpers
# =========================
def warmup_lr(ep:int, base_lr:float, warmup_epochs:int):
    if ep <= warmup_epochs:
        return base_lr * (ep / max(1, warmup_epochs))
    return base_lr

def batch_mae_in_original_units(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    p = pred_b.detach().cpu().numpy().reshape(-1, 1)
    t = y_b.detach().cpu().numpy().reshape(-1, 1)
    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)
    return float(np.mean(np.abs(p_orig - t_orig)))

def batch_corrcoef(pred_b: torch.Tensor, y_b: torch.Tensor, scaler_y) -> float:
    """
    Pearson correlation coefficient (batch í‰ê· )
    pred_b, y_b: (B, H)
    """
    p = pred_b.detach().cpu().numpy().reshape(-1, 1)
    t = y_b.detach().cpu().numpy().reshape(-1, 1)
    p_orig = scaler_y.inverse_transform(p).reshape(-1)
    t_orig = scaler_y.inverse_transform(t).reshape(-1)

    if np.std(p_orig) < 1e-6 or np.std(t_orig) < 1e-6:
        return 0.0
    return float(np.corrcoef(p_orig, t_orig)[0,1])

# =========================
# train & evaluate
# =========================
def train_and_eval(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list):
    """
    X: (N,F), y: (N,), feat_names: ['ili', 'vaccine_rate', 'respiratory_index'] ë“±
    """
    set_seed(SEED)
    (s0,e0),(s1,e1),(s2,e2) = make_splits(len(y))
    X_tr, X_va, X_te = X[s0:e0], X[s1:e1], X[s2:e2]
    y_tr, y_va, y_te = y[s0:e0], y[s1:e1], y[s2:e2]
    lab_tr, lab_va, lab_te = labels[s0:e0], labels[s1:e1], labels[s2:e2]

    # ==== Scaling ====
    # Target scaler
    scaler_y = get_scaler()
    y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
    y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
    y_te_sc = scaler_y.transform(y_te.reshape(-1,1)).ravel()

    # Feature scaler (ì…ë ¥ íŠ¹ì§• ì „ì²´)
    scaler_x = get_scaler()
    X_tr_sc = scaler_x.fit_transform(X_tr)
    X_va_sc = scaler_x.transform(X_va)
    X_te_sc = scaler_x.transform(X_te)

    F = X.shape[1]
    print(f"[Shapes] X_tr:{X_tr.shape}, X_va:{X_va.shape}, X_te:{X_te.shape} | F={F}")
    print(f"[Info] Model input feature order -> {feat_names}")

    ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_va = PatchTSTDataset(X_va_sc, y_va_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_te = PatchTSTDataset(X_te_sc, y_te_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)

    # drop_last=False ë¡œ ë³€ê²½(ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ í•™ìŠµ ë°°ì¹˜ ë³´ì¥)
    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)

    model = PatchTSTModel(
        in_features=F, patch_len=PATCH_LEN, d_model=D_MODEL, n_heads=N_HEADS,
        n_layers=ENC_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT,
        pred_len=PRED_LEN, head_hidden=HEAD_HIDDEN
    ).to(DEVICE)

    # Loss / Optim / Scheduler
    crit = nn.HuberLoss(delta=1.0)
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)

    # ---- history for curves ----
    hist = {"train_loss":[], "val_loss":[], "train_mae":[], "val_mae":[]}

    best_val = float("inf"); best_state=None; noimp=0
    printed_batch_info = False
    for ep in range(1, EPOCHS+1):
        # ---- Train ----
        model.train(); tr_loss_sum=0; tr_mae_sum=0; n=0
        # warmup
        for g in opt.param_groups:
            g['lr'] = warmup_lr(ep, LR, WARMUP_EPOCHS)

        for Xb,yb,_ in dl_tr:
            if not printed_batch_info:
                # Xb: (B, P, L, F)  â† ìµœì¢… ëª¨ë¸ ì…ë ¥ í…ì„œ êµ¬ì¡°
                print(f"[Batch] Xb.shape={tuple(Xb.shape)} (B,P,L,F), yb.shape={tuple(yb.shape)}")
                print(f"[Batch] Feature order used -> {feat_names}")
                printed_batch_info = True
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            opt.zero_grad()
            pred = model(Xb)
            loss = crit(pred, yb)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
            bs=yb.size(0)
            tr_loss_sum += loss.item()*bs; n+=bs
            tr_mae_sum  += batch_mae_in_original_units(pred, yb, scaler_y)*bs

        tr_loss = tr_loss_sum / max(1,n)
        tr_mae  = tr_mae_sum  / max(1,n)

        # ---- Validation ----
        model.eval(); va_loss_sum=0; va_mae_sum=0; va_corr_sum = 0; n=0
        with torch.no_grad():
            for Xb,yb,_ in dl_va:
                Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
                pred = model(Xb); loss = crit(pred,yb)
                bs=yb.size(0)
                va_loss_sum += loss.item()*bs; n+=bs
                va_mae_sum  += batch_mae_in_original_units(pred, yb, scaler_y)*bs
                va_corr_sum += batch_corrcoef(pred, yb, scaler_y)*bs
        va_loss = va_loss_sum / max(1,n)
        va_mae  = va_mae_sum  / max(1,n)
        va_corr = va_corr_sum / max(1,n)

        scheduler.step()

        hist["train_loss"].append(tr_loss)
        hist["val_loss"].append(va_loss)
        hist["train_mae"].append(tr_mae)
        hist["val_mae"].append(va_mae)

        print(f"[Epoch {ep:03d}/{EPOCHS}] "
              f"LR={opt.param_groups[0]['lr']:.6f} | "
              f"Loss T/V={tr_loss:.5f}/{va_loss:.5f} | "
              f"MAE  T/V={tr_mae:.5f}/{va_mae:.5f}"
              f"Corr V={va_corr:.3f}")

        if va_loss < best_val - 1e-6:
            best_val = va_loss; noimp=0
            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}
        else:
            noimp += 1
            if noimp >= PATIENCE:
                print(f"Early stopping after {ep} epochs (no improvement {PATIENCE}).")
                break

    if best_state is not None:
        model.load_state_dict({k:v.to(DEVICE) for k,v in best_state.items()})

    # ---- Test & Metrics ----
    model.eval(); preds=[]; trues=[]; starts=[]
    with torch.no_grad():
        for Xb,yb,i0 in dl_te:
            Xb=Xb.to(DEVICE)
            preds.append(model(Xb).detach().cpu().numpy())
            trues.append(yb.numpy())
            starts.append(i0.numpy())
    yhat_sc = np.concatenate(preds,axis=0)
    ytrue_sc= np.concatenate(trues,axis=0)
    starts  = np.concatenate(starts,axis=0)

    # inverse scale (target only)
    yhat  = scaler_y.inverse_transform(yhat_sc.reshape(-1,1)).reshape(-1,PRED_LEN)
    ytrue = scaler_y.inverse_transform(ytrue_sc.reshape(-1,1)).reshape(-1,PRED_LEN)

    mse  = float(np.mean((yhat-ytrue)**2))
    rmse = float(np.sqrt(mse))
    mae  = float(np.mean(np.abs(yhat-ytrue)))
    print("\n=== Final Test Metrics ===")
    print(f"MSE : {mse:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"MAE : {mae:.6f}")

    # =========================
    # Save per-window predictions
    # =========================
    cols_true = [f"true_t+{i}" for i in range(1,PRED_LEN+1)]
    cols_pred = [f"pred_t+{i}" for i in range(1,PRED_LEN+1)]
    out = pd.DataFrame(np.hstack([ytrue, yhat]), columns=cols_true+cols_pred)
    out.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")
    print(f"Saved predictions -> {OUT_CSV}")

    # =========================
    # Plot_1: last window (H-step ahead)
    # =========================
    last_true = ytrue[-1]; last_pred = yhat[-1]
    weeks = np.arange(1, PRED_LEN+1)
    plt.figure(figsize=(10,4))
    plt.plot(weeks, last_true, label="Truth (last window)", linewidth=2)
    plt.plot(weeks, last_pred, label="Prediction (last window)", linewidth=2)
    plt.title("Last Test Window: Truth vs Prediction")
    plt.xlabel("Horizon (weeks ahead)")
    plt.ylabel("ILI per 1,000 Population")
    plt.grid(True); plt.legend()
    plt.tight_layout()
    plt.savefig(PLOT_LAST_WINDOW, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight', format='png', pad_inches=0.1)
    print(f"Saved plot -> {PLOT_LAST_WINDOW}")

    # =========================
    # Plot_2: test reconstruction (val-context included)
    # =========================
    context = y_va_sc[-SEQ_LEN:]                       # í‘œì¤€í™” ì»¨í…ìŠ¤íŠ¸
    y_ct_sc = np.concatenate([context, y_te_sc])       # [SEQ_LEN + test_len]
    # ì…ë ¥ íŠ¹ì§•ë„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨í•´ ì¬êµ¬ì„± í•„ìš” â†’ Xë„ ë™ì¼í•˜ê²Œ ë¶™ì—¬ì„œ ì˜ˆì¸¡
    X_ct_sc = np.concatenate([X_va_sc[-SEQ_LEN:], X_te_sc], axis=0)
    ds_ct = PatchTSTDataset(X_ct_sc, y_ct_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    dl_ct = DataLoader(ds_ct, batch_size=BATCH_SIZE, shuffle=False)

    model.eval(); preds_ct=[]; starts_ct=[]
    with torch.no_grad():
        for Xb, _, i0 in dl_ct:
            Xb = Xb.to(DEVICE)
            preds_ct.append(model(Xb).detach().cpu().numpy())  # (B, H)
            starts_ct.append(i0.numpy())
    yhat_ct_sc = np.concatenate(preds_ct, axis=0)
    starts_ct  = np.concatenate(starts_ct, axis=0)
    yhat_ct = scaler_y.inverse_transform(yhat_ct_sc.reshape(-1,1)).reshape(-1, PRED_LEN)

    test_len = len(y_te)
    recon_sum   = np.zeros(test_len)
    recon_count = np.zeros(test_len)
    h_weights = np.linspace(RECON_W_START, RECON_W_END, PRED_LEN)

    for k, s in enumerate(starts_ct):
        pos0_ct = int(s) + SEQ_LEN   # [context+test] ì¶•
        pos0_te = pos0_ct - SEQ_LEN  # test ì¶•ìœ¼ë¡œ ë³€í™˜
        for j in range(PRED_LEN):
            idx = pos0_te + j
            if 0 <= idx < test_len:
                w = h_weights[j]
                recon_sum[idx]   += yhat_ct[k, j] * w
                recon_count[idx] += w

    recon = np.where(recon_count > 0, recon_sum / np.maximum(1, recon_count), np.nan)

    truth_test = y_te
    x_labels = lab_te
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš© (NaN ì œê±°)
    valid_mask = ~np.isnan(truth_test) & ~np.isnan(recon)
    valid_indices = np.where(valid_mask)[0]
    
    if len(valid_indices) < len(truth_test):
        print(f"\nâš ï¸ Warning: {len(truth_test) - len(valid_indices)} NaN values removed from test reconstruction")
        truth_test = truth_test[valid_mask]
        recon = recon[valid_mask]
        x_labels = [x_labels[i] for i in valid_indices]
    
    test_len = len(truth_test)
    
    # ë””ë²„ê·¸: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë²”ìœ„ ì¶œë ¥
    print(f"\n=== Test Segment Info ===")
    print(f"Test length: {test_len}")
    print(f"First label: {x_labels[0]}")
    print(f"Last label: {x_labels[-1]}")
    print(f"Truth range: [{truth_test.min():.2f}, {truth_test.max():.2f}]")
    print(f"Truth mean: {truth_test.mean():.2f}")
    print(f"Prediction range: [{np.nanmin(recon):.2f}, {np.nanmax(recon):.2f}]")
    print(f"Prediction mean: {np.nanmean(recon):.2f}")
    
    # Xì¶• ë¼ë²¨ì„ ë” ìì£¼ í‘œì‹œ (ê°„ê²© ì¡°ì •)
    tick_step = max(1, test_len // 20)  # ì•½ 20ê°œ ë¼ë²¨ í‘œì‹œ
    tick_idx  = list(range(0, test_len, tick_step))
    if tick_idx[-1] != test_len-1:
        tick_idx.append(test_len-1)
    tick_text = [x_labels[i] for i in tick_idx]

    plt.figure(figsize=(18,6))  # ê·¸ë˜í”„ í¬ê¸° í™•ëŒ€
    plt.plot(range(test_len), truth_test, linewidth=2.5, marker='o', markersize=3, 
             label=f"Truth (test segment, n={test_len})", color='darkblue')
    plt.plot(range(test_len), recon, linewidth=2.5, marker='s', markersize=3,
             label="Prediction (overlap-avg, weighted)", color='darkorange')
    plt.title(f"Test Range: Truth vs Prediction | {x_labels[0]} ~ {x_labels[-1]}", 
              fontsize=14, fontweight='bold')
    plt.xlabel("Season - Week", fontsize=12)
    plt.ylabel("ILI per 1,000 Population", fontsize=12)
    plt.xticks(tick_idx, tick_text, rotation=45, ha="right", fontsize=9)
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.legend(fontsize=11, loc='upper left')
    
    # Yì¶• ë²”ìœ„ ëª…ì‹œì  ì„¤ì • (ì´ìƒê°’ ë°©ì§€)
    y_min = min(truth_test.min(), np.nanmin(recon))
    y_max = max(truth_test.max(), np.nanmax(recon))
    plt.ylim(y_min * 0.95, y_max * 1.05)
    
    plt.tight_layout()
    plt.savefig(PLOT_TEST_RECON, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight', format='png', pad_inches=0.1)
    print(f"Saved plot -> {PLOT_TEST_RECON}")

    # =========================
    # Plot_3: Train/Val MAE curves
    # =========================
    xs = np.arange(1, len(hist["train_mae"])+1)
    plt.figure(figsize=(10,4))
    plt.plot(xs, hist["train_mae"], linewidth=2, label="Train MAE (original units)")
    plt.plot(xs, hist["val_mae"],   linewidth=2, label="Val MAE (original units)")
    plt.title("Training Curves: MAE per epoch (lower is better)")
    plt.xlabel("Epoch")
    plt.ylabel("MAE (ILI per 1,000)")
    plt.grid(True); plt.legend()
    plt.tight_layout()
    plt.savefig(PLOT_MA_CURVES, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight', format='png', pad_inches=0.1)
    print(f"Saved plot -> {PLOT_MA_CURVES}")


# =========================
# run
# =========================
if __name__ == "__main__":
    print("\n" + "ğŸš€ " * 30)
    print("ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("ğŸš€ " * 30 + "\n")
    
    print("=" * 60)
    print("ğŸ’¾ DuckDB/CSV ëª¨ë“œ: ë¡œì»¬ ë°ì´í„° ë¡œë“œ")
    print("=" * 60)
    
    # DuckDB ë˜ëŠ” CSVì—ì„œ ë°ì´í„° ë¡œë“œ
    df = load_data_from_duckdb_or_csv()
    
    print("\n" + "âœ… " * 30)
    print("ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
    print("âœ… " * 30 + "\n")
    
    # ë°ì´í„° í™•ì¸
    print(f"ğŸ“Š DataFrame ì •ë³´:")
    print(f"   - Shape: {df.shape}")
    print(f"   - Columns: {list(df.columns)}")
    print(f"\nì²˜ìŒ 5ê°œ í–‰:")
    print(df.head())
    print(f"\në°ì´í„° íƒ€ì…:")
    print(df.dtypes)
    
    print(f"\nğŸ”§ USE_EXOG = '{USE_EXOG}'  (auto-detects vaccine/resp columns)")
    
    # DataFrameì„ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì „ì²˜ë¦¬
    print("\nğŸ“ˆ ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    X, y, labels, feat_names = load_and_prepare(df=df, use_exog=USE_EXOG)
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   - Data points: {len(y)}")
    print(f"   - Features used ({len(feat_names)}): {feat_names}")
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    print("\n" + "* " * 30)
    print("ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("* " * 30 + "\n")
    train_and_eval(X, y, labels, feat_names)

    # =========================
# Feature Importance utils
# =========================
def _eval_mae_on_split(model, X_split_sc, y_split_sc, scaler_y, feat_names, 
                       seq_len=SEQ_LEN, pred_len=PRED_LEN, patch_len=PATCH_LEN, stride=STRIDE,
                       batch_size=BATCH_SIZE):
    """í˜„ì¬ ëª¨ë¸ë¡œ í•œ ë¶„í• (va/test) ì„¸íŠ¸ì—ì„œ MAE(ì› ë‹¨ìœ„) ê³„ì‚°"""
    ds = PatchTSTDataset(X_split_sc, y_split_sc, seq_len, pred_len, patch_len, stride)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)
    model.eval()
    mae_sum, n = 0.0, 0
    with torch.no_grad():
        for Xb, yb, _ in dl:
            Xb = Xb.to(DEVICE); yb = yb.to(DEVICE)
            pred = model(Xb)  # (B, H)
            mae_sum += batch_mae_in_original_units(pred, yb, scaler_y) * yb.size(0)
            n += yb.size(0)
    return float(mae_sum / max(1, n))


def compute_feature_importance(model, 
                               X_va_sc, y_va_sc, 
                               X_te_sc=None, y_te_sc=None,
                               scaler_y=None, feat_names=None, 
                               random_state=42):
    """
    í¼ë®¤í…Œì´ì…˜(ì—´ ì„ê¸°) ì¤‘ìš”ë„ì™€ í‰ê·  ëŒ€ì²´(ê·¸ íŠ¹ì§•ì„ í‰ê· ìœ¼ë¡œ ê³ ì •) ì¤‘ìš”ë„ë¥¼ ê³„ì‚°.
    ë°˜í™˜: ì¤‘ìš”ë„ DataFrame (Î”MAEê°€ í´ìˆ˜ë¡ ì¤‘ìš”)
    """
    assert scaler_y is not None and feat_names is not None
    rng = np.random.RandomState(random_state)

    # --- ê¸°ì¤€ì„ (baseline MAE) ---
    baseline_val = _eval_mae_on_split(model, X_va_sc, y_va_sc, scaler_y, feat_names)
    print(f"[FI] Baseline Val MAE: {baseline_val:.6f}")

    baseline_tst = None
    if X_te_sc is not None and y_te_sc is not None:
        baseline_tst = _eval_mae_on_split(model, X_te_sc, y_te_sc, scaler_y, feat_names)
        print(f"[FI] Baseline Test MAE: {baseline_tst:.6f}")

    perm_deltas_val, mean_deltas_val = [], []
    perm_deltas_tst, mean_deltas_tst = [], []

    for j, name in enumerate(feat_names):
        # â‘  í¼ë®¤í…Œì´ì…˜(ì—´ ì„ê¸°)
        Xp = X_va_sc.copy()
        col = Xp[:, j].copy()
        rng.shuffle(col)
        Xp[:, j] = col
        mae_perm_val = _eval_mae_on_split(model, Xp, y_va_sc, scaler_y, feat_names)
        perm_deltas_val.append(mae_perm_val - baseline_val)

        # â‘¡ í‰ê·  ëŒ€ì²´(íŠ¹ì§• ì œê±° íš¨ê³¼)
        Xz = X_va_sc.copy()
        Xz[:, j] = X_va_sc[:, j].mean()
        mae_mean_val = _eval_mae_on_split(model, Xz, y_va_sc, scaler_y, feat_names)
        mean_deltas_val.append(mae_mean_val - baseline_val)

        if X_te_sc is not None and y_te_sc is not None:
            Xp_te = X_te_sc.copy()
            col_te = Xp_te[:, j].copy()
            rng.shuffle(col_te)
            Xp_te[:, j] = col_te
            mae_perm_tst = _eval_mae_on_split(model, Xp_te, y_te_sc, scaler_y, feat_names)
            perm_deltas_tst.append(mae_perm_tst - baseline_tst)

            Xz_te = X_te_sc.copy()
            Xz_te[:, j] = X_te_sc[:, j].mean()
            mae_mean_tst = _eval_mae_on_split(model, Xz_te, y_te_sc, scaler_y, feat_names)
            mean_deltas_tst.append(mae_mean_tst - baseline_tst)

    # DataFrame ìƒì„±
    df_fi = pd.DataFrame({
        "feature": feat_names,
        "perm_delta_val": perm_deltas_val,
        "mean_delta_val": mean_deltas_val,
    })
    if X_te_sc is not None and y_te_sc is not None:
        df_fi["perm_delta_tst"] = perm_deltas_tst
        df_fi["mean_delta_tst"] = mean_deltas_tst

    # í‰ê·  ë¸íƒ€ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    df_fi = df_fi.sort_values("mean_delta_val", ascending=False).reset_index(drop=True)
    return df_fi

def plot_feature_importance(fi_df, out_csv=None, out_png=None):
    """
    Feature Importanceë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    """
    if fi_df is None or len(fi_df) == 0:
        print("No feature importance data to plot.")
        return

    import matplotlib.pyplot as plt

    # CSV ì €ì¥
    if out_csv:
        fi_df.to_csv(out_csv, index=False)
        print(f"Feature Importance saved to {out_csv}")

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # â‘  Permutation Î” (Val)
    axes[0].barh(fi_df["feature"], fi_df["perm_delta_val"], color="steelblue")
    axes[0].set_xlabel("Î”MAE (Permutation, Val)")
    axes[0].set_title("Permutation Feature Importance (Val)")
    axes[0].invert_yaxis()

    # â‘¡ Mean Replacement Î” (Val)
    axes[1].barh(fi_df["feature"], fi_df["mean_delta_val"], color="coral")
    axes[1].set_xlabel("Î”MAE (Mean Replacement, Val)")
    axes[1].set_title("Mean Replacement Feature Importance (Val)")
    axes[1].invert_yaxis()

    plt.tight_layout()

    if out_png:
        plt.savefig(out_png, dpi=150, bbox_inches="tight")
        print(f"Feature Importance plot saved to {out_png}")
    plt.show()


# =========================
# train_and_eval (main)
# =========================
def train_and_eval(X: np.ndarray, y: np.ndarray, labels: list, feat_names: list,
                   compute_fi=False, save_fi=False):
    """
    í†µí•© í•™ìŠµ + í‰ê°€ í•¨ìˆ˜.
    compute_fi=True -> feature importance ê³„ì‚°
    save_fi=True -> CSV/plot ì €ì¥
    """
    torch.manual_seed(SEED); np.random.seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
    print(f"[Config] EPOCHS:{EPOCHS}, BATCH_SIZE:{BATCH_SIZE}, SEQ_LEN:{SEQ_LEN}, PRED_LEN:{PRED_LEN}")
    print(f"[Config] PATCH_LEN:{PATCH_LEN}, STRIDE:{STRIDE}, LR:{LR}, Warmup:{WARMUP_EPOCHS}, Patience:{PATIENCE}")

    N = len(y)
    split_tr = int(0.7*N); split_va = int(0.85*N)
    X_tr, y_tr = X[:split_tr], y[:split_tr]
    X_va, y_va = X[split_tr:split_va], y[split_tr:split_va]
    X_te, y_te = X[split_va:], y[split_va:]

    def get_scaler():
        st = SCALER_TYPE.lower()
        if st=="robust": return RobustScaler()
        if st=="minmax": return MinMaxScaler()
        return StandardScaler()

    scaler_y = get_scaler()
    y_tr_sc = scaler_y.fit_transform(y_tr.reshape(-1,1)).ravel()
    y_va_sc = scaler_y.transform(y_va.reshape(-1,1)).ravel()
    y_te_sc = scaler_y.transform(y_te.reshape(-1,1)).ravel()

    scaler_x = get_scaler()
    X_tr_sc = scaler_x.fit_transform(X_tr)
    X_va_sc = scaler_x.transform(X_va)
    X_te_sc = scaler_x.transform(X_te)

    F = X.shape[1]
    print(f"[Shapes] X_tr:{X_tr.shape}, X_va:{X_va.shape}, X_te:{X_te.shape} | F={F}")
    print(f"[Info] Model input feature order -> {feat_names}")

    ds_tr = PatchTSTDataset(X_tr_sc, y_tr_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_va = PatchTSTDataset(X_va_sc, y_va_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)
    ds_te = PatchTSTDataset(X_te_sc, y_te_sc, SEQ_LEN, PRED_LEN, PATCH_LEN, STRIDE)

    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)
    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False)

    model = PatchTSTModel(
        in_features=F, patch_len=PATCH_LEN, d_model=D_MODEL, n_heads=N_HEADS,
        n_layers=ENC_LAYERS, ff_dim=FF_DIM, dropout=DROPOUT,
        pred_len=PRED_LEN, head_hidden=HEAD_HIDDEN
    ).to(DEVICE)

    crit = nn.HuberLoss(delta=1.0)
    opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)

    hist = {"train_loss":[], "val_loss":[], "train_mae":[], "val_mae":[]}

    best_val = float("inf"); best_state=None; noimp=0
    printed_batch_info = False
    for ep in range(1, EPOCHS+1):
        model.train(); tr_loss_sum=0; tr_mae_sum=0; n=0
        for g in opt.param_groups:
            g['lr'] = warmup_lr(ep, LR, WARMUP_EPOCHS)

        for Xb, yb, _ in dl_tr:
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            if not printed_batch_info:
                print(f"[Batch shapes] Xb:{Xb.shape}, yb:{yb.shape}")
                printed_batch_info=True
            opt.zero_grad()
            pred=model(Xb)
            loss=crit(pred,yb)
            loss.backward(); opt.step()

            tr_loss_sum += loss.item()*yb.size(0)
            tr_mae_sum += batch_mae_in_original_units(pred, yb, scaler_y)*yb.size(0)
            n+=yb.size(0)

        tr_loss_avg = tr_loss_sum/max(1,n)
        tr_mae_avg  = tr_mae_sum/max(1,n)

        model.eval(); va_loss_sum=0; va_mae_sum=0; m=0
        with torch.no_grad():
            for Xb,yb,_ in dl_va:
                Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
                pred=model(Xb)
                loss=crit(pred,yb)
                va_loss_sum += loss.item()*yb.size(0)
                va_mae_sum  += batch_mae_in_original_units(pred,yb,scaler_y)*yb.size(0)
                m+=yb.size(0)
        va_loss_avg=va_loss_sum/max(1,m)
        va_mae_avg =va_mae_sum/max(1,m)

        hist["train_loss"].append(tr_loss_avg)
        hist["val_loss"].append(va_loss_avg)
        hist["train_mae"].append(tr_mae_avg)
        hist["val_mae"].append(va_mae_avg)

        if ep<=5 or ep%5==0:
            print(f"Epoch {ep:3d}/{EPOCHS} | TrL:{tr_loss_avg:.6f} TrMAE:{tr_mae_avg:.6f} | VaL:{va_loss_avg:.6f} VaMAE:{va_mae_avg:.6f}")

        if va_mae_avg < best_val:
            best_val = va_mae_avg
            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}
            noimp=0
        else:
            noimp+=1
            if noimp>=PATIENCE:
                print(f"Early stop at epoch {ep} (no improvement for {PATIENCE} epochs)")
                break

        scheduler.step()

    if best_state is not None:
        model.load_state_dict(best_state)
    print(f"Best Val MAE: {best_val:.6f}")

    # Test
    model.eval(); te_mae_sum=0; k=0
    with torch.no_grad():
        for Xb,yb,_ in dl_te:
            Xb=Xb.to(DEVICE); yb=yb.to(DEVICE)
            pred=model(Xb)
            te_mae_sum += batch_mae_in_original_units(pred,yb,scaler_y)*yb.size(0)
            k+=yb.size(0)
    te_mae_avg = te_mae_sum/max(1,k)
    print(f"Test MAE (original units): {te_mae_avg:.6f}")

    # Plot curves
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(hist["train_mae"],label="Train MAE")
    plt.plot(hist["val_mae"],label="Val MAE")
    plt.xlabel("Epoch"); plt.ylabel("MAE (original units)"); plt.legend(); plt.title("MAE curves")
    plt.subplot(1,2,2)
    plt.plot(hist["train_loss"],label="Train Loss")
    plt.plot(hist["val_loss"],label="Val Loss")
    plt.xlabel("Epoch"); plt.ylabel("Huber Loss"); plt.legend(); plt.title("Loss curves")
    plt.tight_layout()
    plt.savefig(PLOT_MA_CURVES, dpi=150)
    print(f"MAE/loss curves saved to {PLOT_MA_CURVES}")
    plt.close()  # ì°½ì„ ë‹«ì•„ ë©”ëª¨ë¦¬ ì ˆì•½

    # Last window
    last_seq_idx = len(y_te_sc) - SEQ_LEN - PRED_LEN
    if last_seq_idx >= 0:
        seq_X = X_te_sc[last_seq_idx:last_seq_idx+SEQ_LEN]  # (SEQ_LEN, F)
        
        # Patchify: Datasetì˜ __getitem__ê³¼ ë™ì¼í•œ ë¡œì§
        patches = []
        pos = 0
        while pos + PATCH_LEN <= SEQ_LEN:
            patches.append(seq_X[pos:pos+PATCH_LEN, :])  # (PATCH_LEN, F)
            pos += STRIDE
        X_patch = np.stack(patches, axis=0)  # (P, PATCH_LEN, F)
        
        # Tensorë¡œ ë³€í™˜í•˜ê³  batch ì°¨ì› ì¶”ê°€
        seq_t = torch.from_numpy(X_patch).unsqueeze(0).float().to(DEVICE)  # (1, P, PATCH_LEN, F)
        
        model.eval()
        with torch.no_grad():
            p = model(seq_t).cpu().numpy().ravel()
        p_orig = scaler_y.inverse_transform(p.reshape(-1,1)).ravel()
        y_true_last = scaler_y.inverse_transform(y_te_sc[last_seq_idx+SEQ_LEN:last_seq_idx+SEQ_LEN+PRED_LEN].reshape(-1,1)).ravel()
        plt.figure(figsize=(8,4))
        plt.plot(range(len(y_true_last)), y_true_last, marker='o', label="True")
        plt.plot(range(len(p_orig)), p_orig, marker='x', label="Pred")
        plt.xlabel("Future step (horizon)"); plt.ylabel("ILI")
        plt.title(f"Last window prediction (SEQ_LEN={SEQ_LEN}, PRED_LEN={PRED_LEN})")
        plt.legend(); plt.grid(True, alpha=0.3)
        plt.savefig(PLOT_LAST_WINDOW, dpi=150)
        print(f"Last window plot saved to {PLOT_LAST_WINDOW}")
        plt.close()  # ì°½ì„ ë‹«ì•„ ë©”ëª¨ë¦¬ ì ˆì•½

    # Test reconstruction
    all_p_te = []
    model.eval()
    with torch.no_grad():
        for Xb,_,_ in dl_te:
            Xb=Xb.to(DEVICE)
            p_b=model(Xb).cpu().numpy()
            all_p_te.append(p_b)
    all_p_te = np.concatenate(all_p_te, axis=0)
    pred_orig = scaler_y.inverse_transform(all_p_te).ravel()
    y_te_orig = scaler_y.inverse_transform(y_te_sc.reshape(-1,1)).ravel()

    plt.figure(figsize=(12,5))
    plt.plot(y_te_orig, label="True", alpha=0.7)
    plt.plot(pred_orig[:len(y_te_orig)], label="Pred", alpha=0.7)
    plt.xlabel("Test set index"); plt.ylabel("ILI")
    plt.title("Test set reconstruction (multi-step predictions)")
    plt.legend(); plt.grid(True, alpha=0.3)
    plt.savefig(PLOT_TEST_RECON, dpi=150)
    print(f"Test reconstruction plot saved to {PLOT_TEST_RECON}")
    plt.close()  # ì°½ì„ ë‹«ì•„ ë©”ëª¨ë¦¬ ì ˆì•½

    # Feature importance
    fi_df = None
    if compute_fi:
        print("\n[Computing Feature Importance...]")
        fi_df = compute_feature_importance(
            model, X_va_sc, y_va_sc, X_te_sc, y_te_sc,
            scaler_y, feat_names, random_state=SEED
        )
        print("\n[Feature Importance (sorted by mean_delta_val)]")
        print(fi_df.to_string(index=False))

        if save_fi:
            plot_feature_importance(
                fi_df,
                out_csv=str(BASE_DIR / "feature_importance.csv"),
                out_png=str(BASE_DIR / "feature_importance.png")
            )

    # ë°˜í™˜: ì™¸ë¶€ ì…€ì—ì„œ ì¬í™œìš© ê°€ëŠ¥í•˜ë„ë¡
    return model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df

# =========================
# ì‹¤í–‰ë¶€ (ê²°ê³¼ ì¶œë ¥)
# =========================
if __name__ == "__main__":
    model, X_va_sc, y_va_sc, X_te_sc, y_te_sc, scaler_y, feat_names, fi_df = train_and_eval(
        X, y, labels, feat_names,
        compute_fi=True,
        save_fi=True
    )

    print("\n=== [ê²°ê³¼ ìš”ì•½] ===")
    print(f"Feature ê°œìˆ˜: {len(feat_names)}")
    if fi_df is not None:
        print("\n[Top 10 Feature Importance]")
        print(fi_df.head(10).to_string(index=False))
    else:
        print("Feature Importance ê³„ì‚°ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")