"""
ClimODE: Climate and Weather Forecasting With Physics-informed Neural ODEs
- ì£¼ê°„(Weekly) ê¸°ìƒ ë°ì´í„° ì˜ˆì¸¡ì„ ìœ„í•œ í†µí•© ëª¨ë“ˆ
- ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸, í•™ìŠµ, í‰ê°€ê°€ í•˜ë‚˜ì˜ íŒŒì¼ì— í†µí•©ë¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import os
from glob import glob
import argparse
import warnings
from torchdiffeq import odeint
from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline

warnings.filterwarnings('ignore')


# ============================================
# 1. ìƒìˆ˜ ì •ì˜
# ============================================

# ì¸í”Œë£¨ì—”ì ë¶„ì„ìš© ê¸°ìƒ ì»¬ëŸ¼ (í•œê¸€ â†’ ì˜ì–´ ë§¤í•‘)
# ğŸ”´ ìµœê³ ê¸°ì˜¨, ìµœì €ê¸°ì˜¨, ìŠµë„ 3ê°œë§Œ ì‚¬ìš©
INFLUENZA_WEATHER_COLS_KR = [
    'ìµœì €ê¸°ì˜¨(â„ƒ)',       # ì•¼ê°„ ì €ì˜¨ â†’ í˜¸í¡ê¸° ê°ì—¼ ì¦ê°€
    'ìµœê³ ê¸°ì˜¨(â„ƒ)',       # ì¼êµì°¨ ê³„ì‚°ìš©
    'í‰ê· ìƒëŒ€ìŠµë„(%)',    # ë‚®ì„ìˆ˜ë¡ ì „íŒŒë ¥ ì¦ê°€
]

# ì˜ì–´ ì»¬ëŸ¼ëª…
INFLUENZA_WEATHER_COLS = [
    'min_temp',           # ìµœì €ê¸°ì˜¨
    'max_temp',           # ìµœê³ ê¸°ì˜¨
    'avg_humidity',       # í‰ê· ìƒëŒ€ìŠµë„
]

# í•œê¸€ â†’ ì˜ì–´ ì»¬ëŸ¼ëª… ë§¤í•‘
COLUMN_NAME_MAPPING = {
    'í‰ê· ê¸°ì˜¨(â„ƒ)': 'avg_temp',
    'ìµœì €ê¸°ì˜¨(â„ƒ)': 'min_temp',
    'ìµœê³ ê¸°ì˜¨(â„ƒ)': 'max_temp',
    'í‰ê· ì§€ë©´ì˜¨ë„(â„ƒ)': 'avg_ground_temp',
    'í‰ê· 5cmì§€ì¤‘ì˜¨ë„(â„ƒ)': 'avg_soil_temp_5cm',
    'í‰ê· 10cmì§€ì¤‘ì˜¨ë„(â„ƒ)': 'avg_soil_temp_10cm',
    'í‰ê· 20cmì§€ì¤‘ì˜¨ë„(â„ƒ)': 'avg_soil_temp_20cm',
    'í‰ê· 30cmì§€ì¤‘ì˜¨ë„(â„ƒ)': 'avg_soil_temp_30cm',
    'í‰ê· ìƒëŒ€ìŠµë„(%)': 'avg_humidity',
    'ìµœì €ìƒëŒ€ìŠµë„(%)': 'min_humidity',
    'í‰ê· ì´ìŠ¬ì ì˜¨ë„(â„ƒ)': 'avg_dew_point',
    'í‰ê· ì¦ê¸°ì••(hPa)': 'avg_vapor_pressure',
    'ì¼êµì°¨(â„ƒ)': 'temp_range',
}

# ODE ì†”ë²„ ì˜µì…˜
SOLVERS = ["dopri8", "dopri5", "bdf", "rk4", "midpoint", 'adams', 
           'explicit_adams', 'fixed_adams', "adaptive_heun", "euler"]


# ============================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬: ì¼ë³„ â†’ ì£¼ê°„ ë°ì´í„° ë³€í™˜
# ============================================

def set_seed(seed: int = 42) -> None:
    """ëœë¤ ì‹œë“œ ì„¤ì •"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ["PYTHONHASHSEED"] = str(seed)
    print(f"Random seed set as {seed}")


def load_weather_data(data_dir: str = None) -> pd.DataFrame:
    """
    data í´ë”ì˜ ëª¨ë“  CSV íŒŒì¼ì„ ì½ì–´ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹¨
    
    Args:
        data_dir: ë°ì´í„° í´ë” ê²½ë¡œ
        
    Returns:
        í•©ì³ì§„ DataFrame
    """
    if data_dir is None:
        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    
    csv_files = sorted(glob(os.path.join(data_dir, "weather_asos_*.csv")))
    
    if not csv_files:
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
    
    dfs = []
    for f in csv_files:
        df = pd.read_csv(f)
        dfs.append(df)
    
    combined_df = pd.concat(dfs, ignore_index=True)
    combined_df['ë‚ ì§œ'] = pd.to_datetime(combined_df['ë‚ ì§œ'])
    combined_df = combined_df.sort_values('ë‚ ì§œ').reset_index(drop=True)
    
    return combined_df


def create_weekly_data(df: pd.DataFrame, numeric_cols: list = None) -> pd.DataFrame:
    """
    ì¼ë³„ ë°ì´í„°ë¥¼ 7ì¼ì”© ë¬¶ì–´ì„œ ì£¼ê°„ ë°ì´í„°ë¡œ ë³€í™˜
    
    Args:
        df: ì¼ë³„ ë°ì´í„° DataFrame
        numeric_cols: ì§‘ê³„í•  ìˆ«ìí˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ì„ íƒ)
        
    Returns:
        ì£¼ê°„ ë°ì´í„° DataFrame
    """
    df = df.copy()
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    df = df.sort_values('ë‚ ì§œ').reset_index(drop=True)
    
    # ì£¼ì°¨ ë²ˆí˜¸ ê³„ì‚° (7ì¼ ë‹¨ìœ„)
    df['ì£¼ì°¨'] = df.index // 7
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ (ìë™)
    if numeric_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        # 'ê´€ì¸¡ì†ŒID', 'ì£¼ì°¨' ë“± ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œì™¸
        exclude_cols = ['ê´€ì¸¡ì†ŒID', 'ì£¼ì°¨']
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    # ì£¼ê°„ ì§‘ê³„ (í‰ê· )
    weekly_df = df.groupby('ì£¼ì°¨').agg({
        'ë‚ ì§œ': 'first',
        **{col: 'mean' for col in numeric_cols if col in df.columns}
    }).reset_index()
    
    # ì£¼ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì¶”ê°€
    weekly_df['ì£¼_ì‹œì‘ì¼'] = df.groupby('ì£¼ì°¨')['ë‚ ì§œ'].first().values
    weekly_df['ì£¼_ì¢…ë£Œì¼'] = df.groupby('ì£¼ì°¨')['ë‚ ì§œ'].last().values
    weekly_df['ì¼ìˆ˜'] = df.groupby('ì£¼ì°¨').size().values
    
    return weekly_df


def create_influenza_weather_data(data_dir: str = None, output_path: str = None) -> pd.DataFrame:
    """
    ì¸í”Œë£¨ì—”ì ë°ì´í„°ì…‹ê³¼ ë³‘í•© ê°€ëŠ¥í•œ ì£¼ê°„ ê¸°ìƒ ë°ì´í„° ìƒì„±
    - ISO ì£¼ì°¨ ê¸°ì¤€ (year, week)
    - ì»¬ëŸ¼ëª…ì€ ì˜ì–´ë¡œ ë³€í™˜ë¨
    
    Args:
        data_dir: ì¼ë³„ ë°ì´í„° í´ë” ê²½ë¡œ
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì£¼ê°„ ê¸°ìƒ ë°ì´í„° DataFrame
    """
    if data_dir is None:
        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    
    # ì¼ë³„ ë°ì´í„° ë¡œë“œ
    df = load_weather_data(data_dir)
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    
    # ISO ì—°ë„, ì£¼ì°¨ ì¶”ì¶œ
    df['year'] = df['ë‚ ì§œ'].dt.isocalendar().year
    df['week'] = df['ë‚ ì§œ'].dt.isocalendar().week
    
    # ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ê¸°ìƒ ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_cols_kr = [col for col in INFLUENZA_WEATHER_COLS_KR if col in df.columns]
    
    # ì£¼ê°„ í‰ê·  ê³„ì‚°
    weekly_df = df.groupby(['year', 'week']).agg({
        **{col: 'mean' for col in available_cols_kr}
    }).reset_index()
    
    # ğŸ”´ ì¼êµì°¨ ê³„ì‚° ì œê±° (3ê°œ ì»¬ëŸ¼ë§Œ ì‚¬ìš©)
    # if 'ìµœê³ ê¸°ì˜¨(â„ƒ)' in weekly_df.columns and 'ìµœì €ê¸°ì˜¨(â„ƒ)' in weekly_df.columns:
    #     weekly_df['ì¼êµì°¨(â„ƒ)'] = weekly_df['ìµœê³ ê¸°ì˜¨(â„ƒ)'] - weekly_df['ìµœì €ê¸°ì˜¨(â„ƒ)']
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    weekly_df = weekly_df.ffill().bfill()
    
    # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
    numeric_cols = weekly_df.select_dtypes(include=[np.number]).columns
    weekly_df[numeric_cols] = weekly_df[numeric_cols].round(2)
    
    # ì»¬ëŸ¼ëª…ì„ ì˜ì–´ë¡œ ë³€í™˜
    weekly_df = weekly_df.rename(columns=COLUMN_NAME_MAPPING)
    
    # ì €ì¥
    if output_path is None:
        output_path = os.path.join(data_dir, "weather_for_influenza.csv")
    
    weekly_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"ì£¼ê°„ ê¸°ìƒ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"ì´ {len(weekly_df)}ì£¼ì˜ ë°ì´í„° ({weekly_df['year'].min()}ë…„ ~ {weekly_df['year'].max()}ë…„)")
    
    return weekly_df


# ============================================
# 3. ClimODE ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
# ============================================

class ResidualBlock(nn.Module):
    """2D Residual Block"""
    
    def __init__(self, in_channels: int, out_channels: int, 
                 activation: str = "gelu", norm: bool = False, n_groups: int = 1):
        super().__init__()
        self.activation = nn.LeakyReLU(0.3)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.drop = nn.Dropout(p=0.1)
        
        if in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))
        else:
            self.shortcut = nn.Identity()
        
        if norm:
            self.norm1 = nn.GroupNorm(n_groups, in_channels)
            self.norm2 = nn.GroupNorm(n_groups, out_channels)
        else:
            self.norm1 = nn.Identity()
            self.norm2 = nn.Identity()

    def forward(self, x: torch.Tensor):
        h = self.activation(self.bn1(self.conv1(self.norm1(x))))
        h = self.activation(self.bn2(self.conv2(self.norm2(h))))
        h = self.drop(h)
        return h + self.shortcut(x)


class ResidualBlock1D(nn.Module):
    """1D Residual Block for sequential data"""
    
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.activation = nn.LeakyReLU(0.3)
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(out_channels)
        self.drop = nn.Dropout(p=0.1)
        
        if in_channels != out_channels:
            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x: torch.Tensor):
        h = self.activation(self.bn1(self.conv1(x)))
        h = self.activation(self.bn2(self.conv2(h)))
        h = self.drop(h)
        return h + self.shortcut(x)


class Climate_ResNet_1D(nn.Module):
    """1D ResNet for weekly weather data"""
    
    def __init__(self, num_channels, layers, hidden_sizes):
        super().__init__()
        layers_cnn = []
        
        for idx in range(len(layers)):
            if idx == 0:
                layers_cnn.append(self._make_layer(num_channels, hidden_sizes[idx], layers[idx]))
            else:
                layers_cnn.append(self._make_layer(hidden_sizes[idx-1], hidden_sizes[idx], layers[idx]))
        
        self.layer_cnn = nn.ModuleList(layers_cnn)

    def _make_layer(self, in_channels, out_channels, reps):
        layers = [ResidualBlock1D(in_channels, out_channels)]
        for _ in range(1, reps):
            layers.append(ResidualBlock1D(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, data):
        dx_final = data.float()
        for layer in self.layer_cnn:
            dx_final = layer(dx_final)
        return dx_final


# ============================================
# 4. ClimODE ì£¼ê°„ ì˜ˆì¸¡ ëª¨ë¸
# ============================================

class Optim_velocity_weekly(nn.Module):
    """ì£¼ê°„ ë°ì´í„°ë¥¼ ìœ„í•œ ì†ë„ ìµœì í™” ëª¨ë“ˆ"""
    
    def __init__(self, num_years, num_features, seq_length):
        super().__init__()
        self.v = nn.Parameter(torch.randn(num_years, num_features, seq_length) * 0.01)
    
    def forward(self, data):
        # ì‹œê°„ì— ë”°ë¥¸ ë¯¸ë¶„ (ë³€í™”ìœ¨)
        grad = torch.gradient(data, dim=2)[0]
        adv = self.v * grad
        return adv, self.v


class ClimODE_Weekly(nn.Module):
    """
    ì£¼ê°„ ê¸°ìƒ ì˜ˆì¸¡ì„ ìœ„í•œ ClimODE ëª¨ë¸
    Physics-informed Neural ODE ê¸°ë°˜
    """
    
    def __init__(self, num_features: int, seq_length: int = 8, 
                 hidden_channels: list = None, method: str = 'euler',
                 use_uncertainty: bool = True):
        """
        Args:
            num_features: ì…ë ¥ íŠ¹ì„± ìˆ˜ (ê¸°ìƒ ë³€ìˆ˜)
            seq_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê³¼ê±° ëª‡ ì£¼)
            hidden_channels: íˆë“  ì±„ë„ ë¦¬ìŠ¤íŠ¸
            method: ODE ì†”ë²„ ë°©ë²•
            use_uncertainty: ë¶ˆí™•ì‹¤ì„± ì¶”ì • ì‚¬ìš© ì—¬ë¶€
        """
        super().__init__()
        
        if hidden_channels is None:
            hidden_channels = [64, 128, 64]
        
        self.num_features = num_features
        self.seq_length = seq_length
        self.method = method
        self.use_uncertainty = use_uncertainty
        
        # ì‹œê°„ ì„ë² ë”© ì°¨ì› í¬í•¨í•œ ì…ë ¥ í¬ê¸°
        # ì…ë ¥: [features, time_emb(4), velocity(features)]
        input_channels = num_features * 2 + 4
        
        # Velocity Field Network
        self.vel_net = Climate_ResNet_1D(
            num_channels=input_channels,
            layers=[3, 2, 2],
            hidden_sizes=[hidden_channels[0], hidden_channels[1], num_features]
        )
        
        # ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë„¤íŠ¸ì›Œí¬
        if use_uncertainty:
            self.uncertainty_net = Climate_ResNet_1D(
                num_channels=num_features + 4,
                layers=[2, 2],
                hidden_sizes=[hidden_channels[0], num_features]
            )
        
        # í•™ìŠµ ê°€ëŠ¥í•œ ì´ˆê¸° ì†ë„
        self.init_velocity = nn.Parameter(torch.zeros(1, num_features, 1))
        
        # ê¸°íƒ€ íŒŒë¼ë¯¸í„°
        self.gamma = nn.Parameter(torch.tensor([0.1]))

    def get_time_embedding(self, t, batch_size, seq_len):
        """
        ì‹œê°„ ì„ë² ë”© ìƒì„± (ê³„ì ˆì„± ë°˜ì˜)
        
        Args:
            t: ì‹œê°„ í…ì„œ (ì£¼ì°¨)
            batch_size: ë°°ì¹˜ í¬ê¸°
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        # ì—°ê°„ ì£¼ê¸° (52ì£¼)
        t_week = t.view(-1, 1, 1).expand(batch_size, 1, seq_len)
        sin_yearly = torch.sin(2 * np.pi * t_week / 52)
        cos_yearly = torch.cos(2 * np.pi * t_week / 52)
        
        # ë°˜ë…„ ì£¼ê¸° (26ì£¼)
        sin_half = torch.sin(2 * np.pi * t_week / 26)
        cos_half = torch.cos(2 * np.pi * t_week / 26)
        
        return torch.cat([sin_yearly, cos_yearly, sin_half, cos_half], dim=1)

    def pde(self, t, state):
        """
        ODE ì‹œìŠ¤í…œì˜ ìš°ë³€ ì •ì˜ (ë¬¼ë¦¬ ê¸°ë°˜ ë¯¸ë¶„ë°©ì •ì‹)
        
        ds/dt = v Â· âˆ‡s + f(s, t)
        
        ì—¬ê¸°ì„œ:
        - s: ìƒíƒœ (ê¸°ìƒ ë³€ìˆ˜ë“¤)
        - v: ì†ë„ì¥ (ì´ë¥˜)
        - f: í•™ìŠµëœ ë¹„ì„ í˜• í•¨ìˆ˜
        """
        batch_size = state.shape[0]
        seq_len = state.shape[2]
        
        # ìƒíƒœì™€ ì†ë„ ë¶„ë¦¬
        s = state[:, :self.num_features, :]  # í˜„ì¬ ìƒíƒœ
        v = state[:, self.num_features:, :]  # ì†ë„ì¥
        
        # ê³µê°„ ë¯¸ë¶„ (ì‹œí€€ìŠ¤ ë°©í–¥)
        ds_dt_spatial = torch.gradient(s, dim=2)[0]
        
        # ì‹œê°„ ì„ë² ë”©
        t_emb = self.get_time_embedding(t, batch_size, seq_len).to(state.device)
        
        # ê²°í•© í‘œí˜„
        combined = torch.cat([s, v, t_emb], dim=1)
        
        # ì†ë„ì¥ ì—…ë°ì´íŠ¸
        dv = self.vel_net(combined)
        
        # ì´ë¥˜ í•­: v Â· âˆ‡s
        advection = v * ds_dt_spatial
        
        # ìƒíƒœ ë³€í™”
        ds = advection + self.gamma * dv
        
        return torch.cat([ds, dv], dim=1)

    def forward(self, x, future_steps: int = 1):
        """
        ìˆœì „íŒŒ
        
        Args:
            x: ì…ë ¥ ì‹œí€€ìŠ¤ (batch, features, seq_length)
            future_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜
            
        Returns:
            mean: ì˜ˆì¸¡ í‰ê·  (batch, features, future_steps)
            std: ì˜ˆì¸¡ í‘œì¤€í¸ì°¨ (batch, features, future_steps) - use_uncertainty=Trueì¼ ë•Œ
        """
        batch_size = x.shape[0]
        device = x.device
        
        # ì´ˆê¸° ì†ë„ ì„¤ì •
        init_v = self.init_velocity.expand(batch_size, -1, x.shape[2]).to(device)
        
        # ì´ˆê¸° ìƒíƒœ: [í˜„ì¬ ìƒíƒœ, ì†ë„]
        state = torch.cat([x, init_v], dim=1)
        
        # ODE ì ë¶„ ì‹œê°„
        t = torch.linspace(0, future_steps, steps=future_steps + 1).to(device)
        
        # ODE ì ë¶„
        pde_rhs = lambda t, state: self.pde(t, state)
        result = odeint(pde_rhs, state, t, method=self.method, atol=0.1, rtol=0.1)
        
        # ë§ˆì§€ë§‰ ìŠ¤í…ë“¤ì˜ ìƒíƒœ ì¶”ì¶œ
        predictions = result[1:, :, :self.num_features, -1]  # (future_steps, batch, features)
        predictions = predictions.permute(1, 2, 0)  # (batch, features, future_steps)
        
        if self.use_uncertainty:
            # ë¶ˆí™•ì‹¤ì„± ì¶”ì •
            t_emb = self.get_time_embedding(
                torch.tensor([future_steps]).float(), 
                batch_size, 
                self.seq_length
            ).to(device)
            
            uncertainty_input = torch.cat([x, t_emb], dim=1)
            log_std = self.uncertainty_net(uncertainty_input)
            std = F.softplus(log_std[:, :, -1:].expand(-1, -1, future_steps))
            
            return predictions, std
        
        return predictions, None


# ============================================
# 5. í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
# ============================================

def prepare_weekly_data(data_path: str = None, seq_length: int = 8):
    """
    ClimODE í•™ìŠµìš© ì£¼ê°„ ë°ì´í„° ì¤€ë¹„
    
    Args:
        data_path: weather_for_influenza.csv íŒŒì¼ ê²½ë¡œ
        seq_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        
    Returns:
        (X, y, feature_cols, scaler_params, df) íŠœí”Œ
    """
    if data_path is None:
        data_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "data", "weather_for_influenza.csv"
        )
    
    df = pd.read_csv(data_path)
    
    # year, week ì œì™¸í•œ ê¸°ìƒ ë³€ìˆ˜
    feature_cols = [col for col in df.columns if col not in ['year', 'week']]
    
    # Min-Max ì •ê·œí™”
    data = df[feature_cols].values
    data_min = data.min(axis=0)
    data_max = data.max(axis=0)
    data_range = data_max - data_min
    data_range[data_range == 0] = 1
    data_normalized = (data - data_min) / data_range
    
    scaler_params = {'min': data_min, 'max': data_max, 'range': data_range}
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    X, y = [], []
    for i in range(len(data_normalized) - seq_length):
        X.append(data_normalized[i:i+seq_length])
        y.append(data_normalized[i+seq_length])
    
    X = np.array(X)
    y = np.array(y)
    
    # PyTorch í…ì„œ ë³€í™˜ (batch, features, seq_length)
    X = torch.FloatTensor(X).permute(0, 2, 1)
    y = torch.FloatTensor(y)
    
    return X, y, feature_cols, scaler_params, df


def nll_loss(mean, std, truth, var_coeff=0.001):
    """
    Negative Log-Likelihood ì†ì‹¤ í•¨ìˆ˜
    
    Args:
        mean: ì˜ˆì¸¡ í‰ê· 
        std: ì˜ˆì¸¡ í‘œì¤€í¸ì°¨
        truth: ì‹¤ì œê°’
        var_coeff: ë¶„ì‚° ì •ê·œí™” ê³„ìˆ˜
    """
    if std is None:
        return F.mse_loss(mean.squeeze(-1), truth)
    
    normal_dist = torch.distributions.Normal(mean.squeeze(-1), std.squeeze(-1) + 1e-6)
    nll = -normal_dist.log_prob(truth)
    loss = nll.mean() + var_coeff * (std ** 2).mean()
    return loss


def train_climode(model, X, y, epochs: int = 100, lr: float = 0.001, 
                  batch_size: int = 32, device: str = 'cpu', verbose: bool = True):
    """
    ClimODE ëª¨ë¸ í•™ìŠµ
    
    Args:
        model: ClimODE_Weekly ì¸ìŠ¤í„´ìŠ¤
        X: ì…ë ¥ ë°ì´í„° (batch, features, seq_length)
        y: íƒ€ê²Ÿ ë°ì´í„° (batch, features)
        epochs: í•™ìŠµ ì—í­ ìˆ˜
        lr: í•™ìŠµë¥ 
        batch_size: ë°°ì¹˜ í¬ê¸°
        device: ì—°ì‚° ì¥ì¹˜
        verbose: í•™ìŠµ ê³¼ì • ì¶œë ¥ ì—¬ë¶€
        
    Returns:
        í•™ìŠµëœ ëª¨ë¸
    """
    model = model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í•  (90% / 10%)
    split_idx = int(len(X) * 0.9)
    X_train, X_val = X[:split_idx].to(device), X[split_idx:].to(device)
    y_train, y_val = y[:split_idx].to(device), y[split_idx:].to(device)
    
    dataset = TensorDataset(X_train, y_train)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    best_val_loss = float('inf')
    best_model_state = None
    
    print("=" * 60)
    print("ClimODE í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        var_coeff = 2 * scheduler.get_last_lr()[0] if epoch > 0 else 0.001
        
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            mean, std = model(batch_X, future_steps=1)
            loss = nll_loss(mean, std, batch_y, var_coeff)
            
            # L2 ì •ê·œí™”
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss = loss + 0.001 * l2_norm
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        with torch.no_grad():
            val_mean, val_std = model(X_val, future_steps=1)
            val_loss = nll_loss(val_mean, val_std, y_val, var_coeff).item()
        
        scheduler.step()
        
        # ìµœì  ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
        
        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1:3d}/{epochs}] | "
                  f"Train Loss: {train_loss/len(dataloader):.6f} | "
                  f"Val Loss: {val_loss:.6f}")
    
    # ìµœì  ëª¨ë¸ ë³µì›
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    print(f"\nìµœì  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
    
    return model


def evaluate_climode(model, X, y, scaler_params, feature_cols, device='cpu'):
    """
    ClimODE ëª¨ë¸ í‰ê°€
    
    Args:
        model: í•™ìŠµëœ ClimODE ëª¨ë¸
        X: í…ŒìŠ¤íŠ¸ ì…ë ¥
        y: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ
        scaler_params: ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
        feature_cols: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        device: ì—°ì‚° ì¥ì¹˜
        
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    model = model.to(device)
    model.eval()
    
    X = X.to(device)
    y = y.to(device)
    
    with torch.no_grad():
        mean, std = model(X, future_steps=1)
        mean = mean.squeeze(-1)
        
        # MSE, MAE ê³„ì‚°
        mse = F.mse_loss(mean, y).item()
        mae = F.l1_loss(mean, y).item()
        
        # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        mean_np = mean.cpu().numpy()
        y_np = y.cpu().numpy()
        
        mean_original = mean_np * scaler_params['range'] + scaler_params['min']
        y_original = y_np * scaler_params['range'] + scaler_params['min']
        
        # RMSE (ì›ë˜ ë‹¨ìœ„)
        rmse_original = np.sqrt(np.mean((mean_original - y_original) ** 2, axis=0))
    
    results = {
        'mse': mse,
        'mae': mae,
        'rmse_per_feature': dict(zip(feature_cols, rmse_original)),
    }
    
    return results


def forecast_future_weeks(model, last_sequence, scaler_params, feature_cols, 
                          num_weeks: int = 4, last_year: int = None, 
                          last_week: int = None, device='cpu'):
    """
    ë¯¸ë˜ ì£¼ê°„ ê¸°ìƒ ì˜ˆì¸¡
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        last_sequence: ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ë°ì´í„° (ì •ê·œí™”ëœ ìƒíƒœ)
        scaler_params: ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
        feature_cols: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        num_weeks: ì˜ˆì¸¡í•  ì£¼ ìˆ˜
        last_year: ë§ˆì§€ë§‰ ë°ì´í„°ì˜ ì—°ë„
        last_week: ë§ˆì§€ë§‰ ë°ì´í„°ì˜ ì£¼ì°¨
        device: ì—°ì‚° ì¥ì¹˜
        
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ DataFrame
    """
    model = model.to(device)
    model.eval()
    
    predictions = []
    uncertainties = []
    current_seq = last_sequence.clone().to(device)
    
    with torch.no_grad():
        for _ in range(num_weeks):
            mean, std = model(current_seq, future_steps=1)
            pred = mean[:, :, 0]  # (1, features)
            predictions.append(pred.cpu().numpy()[0])
            
            if std is not None:
                uncertainties.append(std[:, :, 0].cpu().numpy()[0])
            
            # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
            pred_expanded = pred.unsqueeze(2)
            current_seq = torch.cat([current_seq[:, :, 1:], pred_expanded], dim=2)
    
    # ì—­ì •ê·œí™”
    predictions = np.array(predictions)
    predictions_original = predictions * scaler_params['range'] + scaler_params['min']
    
    # DataFrame ìƒì„±
    result_df = pd.DataFrame(predictions_original, columns=feature_cols)
    
    # ë¶ˆí™•ì‹¤ì„± ì¶”ê°€
    if uncertainties:
        uncertainties = np.array(uncertainties)
        uncertainties_original = uncertainties * scaler_params['range']
        for i, col in enumerate(feature_cols):
            result_df[f'{col}_std'] = uncertainties_original[:, i]
    
    # year, week ê³„ì‚°
    years, weeks = [], []
    current_year, current_week = last_year, last_week
    
    for _ in range(num_weeks):
        current_week += 1
        if current_week > 52:
            current_week = 1
            current_year += 1
        years.append(current_year)
        weeks.append(current_week)
    
    result_df.insert(0, 'year', years)
    result_df.insert(1, 'week', weeks)
    
    # ì†Œìˆ˜ì  ì •ë¦¬
    numeric_cols = result_df.select_dtypes(include=[np.number]).columns
    result_df[numeric_cols] = result_df[numeric_cols].round(2)
    
    return result_df


# ============================================
# 6. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================

def run_climode_forecast(data_dir: str = None, output_path: str = None,
                         num_weeks: int = 4, epochs: int = 100,
                         seq_length: int = 8, solver: str = 'euler'):
    """
    ì „ì²´ ClimODE ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_path: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        num_weeks: ì˜ˆì¸¡í•  ì£¼ ìˆ˜
        epochs: í•™ìŠµ ì—í­ ìˆ˜
        seq_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        solver: ODE ì†”ë²„
        
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ DataFrame
    """
    set_seed(42)
    
    if data_dir is None:
        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    
    if output_path is None:
        output_path = os.path.join(data_dir, "climode_forecast.csv")
    
    data_path = os.path.join(data_dir, "weather_for_influenza.csv")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print("=" * 60)
    print("ClimODE ê¸°ë°˜ ì£¼ê°„ ê¸°ìƒ ì˜ˆì¸¡")
    print(f"Device: {device}")
    print("=" * 60)
    
    # 0. ë°ì´í„° ì „ì²˜ë¦¬ (ì£¼ê°„ ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    if not os.path.exists(data_path):
        print("\n[0] ì£¼ê°„ ê¸°ìƒ ë°ì´í„° ìƒì„± (ì¼ë³„ â†’ ì£¼ê°„ ë³€í™˜)...")
        try:
            create_influenza_weather_data(data_dir, data_path)
        except FileNotFoundError as e:
            print(f"\nâŒ ì˜¤ë¥˜: ì›ë³¸ ì¼ë³„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   data í´ë”ì— 'weather_asos_*.csv' íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            print(f"   ê²½ë¡œ: {data_dir}")
            raise e
    else:
        print(f"\n[0] ê¸°ì¡´ ì£¼ê°„ ë°ì´í„° ì‚¬ìš©: {data_path}")
    
    # 1. ë°ì´í„° ì¤€ë¹„
    print("\n[1] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...")
    X, y, feature_cols, scaler_params, df = prepare_weekly_data(data_path, seq_length)
    print(f"  - ì´ {len(df)}ì£¼ì˜ ë°ì´í„°")
    print(f"  - íŠ¹ì„± ìˆ˜: {len(feature_cols)}")
    print(f"  - í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(X)}")
    
    last_year = int(df['year'].iloc[-1])
    last_week = int(df['week'].iloc[-1])
    print(f"  - ë§ˆì§€ë§‰ ë°ì´í„°: {last_year}ë…„ {last_week}ì£¼ì°¨")
    
    # 2. ëª¨ë¸ ìƒì„±
    print(f"\n[2] ClimODE ëª¨ë¸ ìƒì„± (ODE Solver: {solver})...")
    model = ClimODE_Weekly(
        num_features=len(feature_cols),
        seq_length=seq_length,
        hidden_channels=[64, 128, 64],
        method=solver,
        use_uncertainty=True
    )
    
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  - í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    
    # 3. ëª¨ë¸ í•™ìŠµ
    print(f"\n[3] ëª¨ë¸ í•™ìŠµ (ì—í­: {epochs})...")
    model = train_climode(model, X, y, epochs=epochs, lr=0.001, 
                          batch_size=32, device=device)
    
    # 4. í‰ê°€
    print("\n[4] ëª¨ë¸ í‰ê°€...")
    split_idx = int(len(X) * 0.9)
    X_test, y_test = X[split_idx:], y[split_idx:]
    results = evaluate_climode(model, X_test, y_test, scaler_params, feature_cols, device)
    print(f"  - Test MSE: {results['mse']:.6f}")
    print(f"  - Test MAE: {results['mae']:.6f}")
    print("  - Feature-wise RMSE:")
    for feat, rmse in results['rmse_per_feature'].items():
        print(f"      {feat}: {rmse:.4f}")
    
    # 5. ë¯¸ë˜ ì˜ˆì¸¡
    print(f"\n[5] ë¯¸ë˜ {num_weeks}ì£¼ ì˜ˆì¸¡...")
    last_data = df[feature_cols].iloc[-seq_length:].values
    last_data_normalized = (last_data - scaler_params['min']) / scaler_params['range']
    last_sequence = torch.FloatTensor(last_data_normalized).T.unsqueeze(0)
    
    forecast_df = forecast_future_weeks(
        model, last_sequence, scaler_params, feature_cols,
        num_weeks=num_weeks, last_year=last_year, last_week=last_week,
        device=device
    )
    
    # 6. ê²°ê³¼ ì €ì¥
    forecast_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\n[6] ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # 7. ëª¨ë¸ ì €ì¥
    model_path = os.path.join(data_dir, "climode_weekly_model.pt")
    torch.save(model.state_dict(), model_path)
    print(f"    ëª¨ë¸ ì €ì¥: {model_path}")
    
    print("\n" + "=" * 60)
    print("ì˜ˆì¸¡ ê²°ê³¼")
    print("=" * 60)
    # ë¶ˆí™•ì‹¤ì„± ì»¬ëŸ¼ ì œì™¸í•˜ê³  ì¶œë ¥
    display_cols = ['year', 'week'] + feature_cols
    print(forecast_df[display_cols].to_string(index=False))
    
    return forecast_df, model


# ============================================
# 7. ë©”ì¸ ì‹¤í–‰
# ============================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='ClimODE ê¸°ë°˜ ì£¼ê°„ ê¸°ìƒ ì˜ˆì¸¡\n'
                    'ê¸°ë³¸ ì‹¤í–‰: python ClimODE.py (ì „ì²˜ë¦¬ â†’ í•™ìŠµ â†’ ì˜ˆì¸¡ ìë™ ì‹¤í–‰)',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--weeks', type=int, default=4,
                        help='ì˜ˆì¸¡í•  ì£¼ ìˆ˜ (ê¸°ë³¸: 4)')
    parser.add_argument('--epochs', type=int, default=100,
                        help='í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--seq_length', type=int, default=8,
                        help='ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: 8ì£¼)')
    parser.add_argument('--solver', type=str, default='euler',
                        choices=SOLVERS, help='ODE ì†”ë²„')
    parser.add_argument('--data_dir', type=str, default=None,
                        help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ==========================================
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
    # (ì „ì²˜ë¦¬ â†’ í•™ìŠµ â†’ í‰ê°€ â†’ ì˜ˆì¸¡)
    # ==========================================
    
    print("\n" + "=" * 60)
    print("ğŸŒ¤ï¸  ClimODE ì£¼ê°„ ê¸°ìƒ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("""
    [ìë™ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸]
    1. ë°ì´í„° ì „ì²˜ë¦¬ (ì¼ë³„ â†’ ì£¼ê°„ ë³€í™˜)
    2. ClimODE ëª¨ë¸ í•™ìŠµ
    3. ëª¨ë¸ í‰ê°€
    4. ë¯¸ë˜ ê¸°ìƒ ì˜ˆì¸¡
    """)
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì „ì²˜ë¦¬ëŠ” run_climode_forecast ë‚´ë¶€ì—ì„œ ìë™ ì²˜ë¦¬)
    forecast_df, model = run_climode_forecast(
        data_dir=args.data_dir,
        num_weeks=args.weeks,
        epochs=args.epochs,
        seq_length=args.seq_length,
        solver=args.solver
    )
    
    # ==========================================
    # ì›ë³¸ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„° ë³‘í•©
    # ==========================================
    print("\n[7] ì›ë³¸ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„° ë³‘í•©...")
    
    data_dir = args.data_dir or os.path.join(
        os.path.dirname(os.path.abspath(__file__)), "data"
    )
    
    # ì›ë³¸ ì£¼ê°„ ê¸°ìƒ ë°ì´í„° ë¡œë“œ
    original_data_path = os.path.join(data_dir, "weather_for_influenza.csv")
    original_df = pd.read_csv(original_data_path)
    
    # ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ë¶ˆí™•ì‹¤ì„± ì»¬ëŸ¼ ì œê±° (ì›ë³¸ê³¼ ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì¡°ë¡œ)
    forecast_cols = [col for col in forecast_df.columns if not col.endswith('_std')]
    forecast_clean = forecast_df[forecast_cols].copy()
    
    # ì›ë³¸ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„° ë³‘í•© (ì„¸ë¡œë¡œ ì—°ê²°)
    merged_df = pd.concat([original_df, forecast_clean], ignore_index=True)
    
    # ì •ë ¬ (year, week ê¸°ì¤€)
    merged_df = merged_df.sort_values(['year', 'week']).reset_index(drop=True)
    
    # ë³‘í•©ëœ ë°ì´í„° ì €ì¥
    merged_output_path = os.path.join(data_dir, "weather_forecast_data.csv")
    merged_df.to_csv(merged_output_path, index=False, encoding='utf-8-sig')
    
    print(f"  - ì›ë³¸ ë°ì´í„°: {len(original_df)}ì£¼")
    print(f"  - ì˜ˆì¸¡ ë°ì´í„°: {len(forecast_clean)}ì£¼")
    print(f"  - ë³‘í•©ëœ ë°ì´í„°: {len(merged_df)}ì£¼")
    print(f"  - ì €ì¥ ì™„ë£Œ: {merged_output_path}")
    
    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ!")
    print("=" * 60)
    print(f"""
ğŸ“ ìƒì„±ëœ íŒŒì¼:
    - data/weather_for_influenza.csv  (ì›ë³¸ ì£¼ê°„ ê¸°ìƒ ë°ì´í„°)
    - data/climode_forecast.csv       (ì˜ˆì¸¡ ê²°ê³¼)
    - data/weather_forecast_data.csv  (ì›ë³¸ + ì˜ˆì¸¡ ë³‘í•©) â­ NEW
    - data/climode_weekly_model.pt    (í•™ìŠµëœ ëª¨ë¸)

ğŸ”§ ì˜µì…˜ ë³€ê²½:
    python ClimODE.py --weeks 8        # 8ì£¼ ì˜ˆì¸¡
    python ClimODE.py --epochs 200     # 200 ì—í­ í•™ìŠµ
    python ClimODE.py --solver dopri5  # dopri5 ì†”ë²„ ì‚¬ìš©

ğŸ“Š íŠ¹ì„± ì»¬ëŸ¼ (3ê°œ):
    min_temp (ìµœì €ê¸°ì˜¨), max_temp (ìµœê³ ê¸°ì˜¨), avg_humidity (í‰ê· ìŠµë„)
    """)
